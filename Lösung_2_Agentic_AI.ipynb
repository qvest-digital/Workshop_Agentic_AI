{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Huggingface Token setzen",
   "id": "314ddfa0ab265b0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:03:56.435089Z",
     "start_time": "2026-01-29T10:03:56.429033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "hf_token = \"hf_Token\"\n",
    "#hf_token = os.getenv(\"HF_TOKEN\")"
   ],
   "id": "1bcf278c95358198",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Git Repo per HTTPs Clonen",
   "id": "2c5eacb992777eab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!git clone https://github.com/qvest-digital/Workshop_Agentic_AI.git",
   "id": "f89099ca1ee8918c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pfad setzen",
   "id": "b398c4d5562a7858"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:03:59.641333Z",
     "start_time": "2026-01-29T10:03:59.637883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#SYSTEM_PATH = \"/home/simon/Workshop_Agentic_AI\"\n",
    "SYSTEM_PATH = \"./Workshop_Agentic_AI\""
   ],
   "id": "95c9b36fee07fd1b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Requirements installieren",
   "id": "78aa612f2a446737"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install -r \"$SYSTEM_PATH/requirements.txt\"",
   "id": "88d6e57ee340b5d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Speicherfragmentierung minimieren",
   "id": "b5f2cd6fc6137a3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:04:01.764531Z",
     "start_time": "2026-01-29T10:04:01.761570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ],
   "id": "3dacd62152e4753d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Workshop Agentic AI\n",
    "\n",
    "Dieses Notebook zeigt Schritt für Schritt, wie ein agentisches KI-System aufgebaut wird, das RAG, externe Tool-Abfragen und LLM-Reasoning kombiniert. Ziel ist es, Halluzinationen zu reduzieren, Fakten deterministisch zu beziehen und mehrere spezialisierte Sub-Modelle zu orchestrieren.\n",
    "\n",
    "## Ablauf\n",
    "- Teil 1: Lokales LLM + Halluzinationen im Griff behalten\n",
    "    - Lokales LLM laden (HF-Pipeline)\n",
    "    - Sampling-Verhalten & Halluzinationen untersuchen\n",
    "    - Textgenerierung konfigurieren (Determinismus vs. Kreativität)\n",
    "- Teil 2: Retrieval-Augmented Generation (RAG) mit dem Fantasietier Razepato\n",
    "    - Texte als Embeddings in eine Vektordatenbank schreiben und vergleichen\n",
    "    - Retrieval + Query + Antwortgenerierung\n",
    "    - Was RAG im kontext LLM leistet\n",
    "    - RAG und LLM verbinden\n",
    "- Teil 3: Erlangen von Wissen durch Nutzung von MCP-Tools\n",
    "    - MCP-Tools anbinden (manuell)\n",
    "    - Fakten über MCP-Aufrufe beziehen (z. B. Koordinaten, Wetter, Flächen)\n",
    "- Teil 4: Voll agentisches MCP – der Mathematikassistent denkt und handelt selbst\n",
    "    - Planen & Reasoning (Chain-of-Thought, ReAct)\n",
    "    - Sub-LLMs orchestrieren (Planner → Validator → Executor → Auditoren → Renderer)"
   ],
   "id": "a07fe97987f77d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Timetable\n",
    "\n",
    "| Zeit        | Thema                          | Inhalt / Ziel                                                |\n",
    "|-------------|--------------------------------|--------------------------------------------------------------|\n",
    "| 09:00–09:30 | Einchecken & Agenda vorstellen | Tagesübersicht                                               |\n",
    "| 09:30–10:00 | LLM + Parameter                | HF-Setup, Tuning-Parameter, Halluzinationen => (Experimente) |\n",
    "| 10:00–10:30 | Prompting (Basisprompt)        | Prompt-Engeneering, Experimente, Diskussion => (Experimente) |\n",
    "| 10:30–12:00 | RAG                            | Embeddings, Vektorsuche, Retrieval, Integration              |\n",
    "| 12:00–13:00 | Mittag                         | Pause                                                        |\n",
    "| 13:00–13:20 | MCP-Basics                     | Konzept, Server, Call-Mechanik                               |\n",
    "| 13:20–14:00 | MCP-Tool-Use                   | Fakten (Koordinaten, Wetter, Locations) über Tools           |\n",
    "| 14:00–15:30 | MCP-Beispiel (manuel)          | Grundfuntkionen von MCP verstehen                            |\n",
    "| 15:30–16:00 | Übergang zu Agenten            | Wie aus RAG + MCP → agentisches System wird                  |\n",
    "| 16:00–16:30 | Code-Walkthrough               | Pipeline-Durchgang, Q&A                                      |\n",
    "| 16:30       | Ende                           | Abschluss                                                    |\n",
    "\n",
    "Metakommentar:\n",
    "- vor jedem Block Reflexionsfragen zu Verknüpfung & Möglichkeiten.\n",
    "- nach jedem Block Verbesserungsideen."
   ],
   "id": "921cca58fd11c2ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Herstellen der Vorbedingungen aus Teil 1:\n",
   "id": "e8b610e67aaca7b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Modell laden: lokal oder von Hugging Face\n",
    "\n",
    "### Funktion\n",
    "- Lädt ein Llama-3.1-Instruct-Modell entweder:\n",
    "  - lokal aus dem Ordner ./models/llama-3.1-8b, wenn der Pfad vorhanden ist, oder\n",
    "  - online von Hugging Face mit der Modell-ID meta-llama/Llama-3.1-8B-Instruct, und speichert es anschließend lokal ab.\n",
    "- Die Umgebungsvariablen werden mit load_dotenv() aus einer .env-Datei geladen, u. a. das Hugging-Face-Token.\n",
    "\n",
    "### Inputs\n",
    "- Dateisystem:\n",
    "  - Existenz von MODEL_PATH (./models/llama-3.1-8b).\n",
    "- Umgebungsvariable:\n",
    "  - HF_TOKEN (wird mit os.getenv(\"HF_TOKEN\") gelesen) – persönliches Zugriffstoken für Hugging Face.\n",
    "- Hyperparameter:\n",
    "  - MODEL_ID gibt die zu ladende Modell-ID an.\n",
    "- Hardware:\n",
    "  - device_map=\"auto\" versucht automatisch, GPU(s) oder CPU sinnvoll zu nutzen.\n",
    "  - torch_dtype=\"auto\" bzw. dtype=\"auto\" lässt das Modell selbst einen sinnvollen Datentyp wählen (z. B. bfloat16 oder float16).\n",
    "\n",
    "### Outputs\n",
    "- Globale Python-Variablen:\n",
    "  - tokenizer: Instanz von AutoTokenizer, konfiguriert für das Llama-3.1-Modell.\n",
    "  - model: Instanz von AutoModelForCausalLM, bereit für Textgenerierung."
   ],
   "id": "5afe18bbe370ce7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:04:36.848653Z",
     "start_time": "2026-01-29T10:04:06.829215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_PATH = f\"{SYSTEM_PATH}/models/llama-3.1-8b\"\n",
    "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"Lade Modell lokal …\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained( MODEL_PATH, torch_dtype=\"auto\", device_map=\"auto\" )\n",
    "\n",
    "else:\n",
    "    print(\"Lade Modell von Hugging Face …\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=hf_token, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, token=hf_token, dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "    # lokal speichern\n",
    "    model.save_pretrained(MODEL_PATH)\n",
    "    tokenizer.save_pretrained(MODEL_PATH)\n",
    "\n"
   ],
   "id": "59bcb01b554dba17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Modell lokal …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '/home/simon/Workshop_Agentic_AI/models/llama-3.1-8b' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1279a597f9fd4389a9d90239e40e64a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pipeline erstellen\n",
    "### Funktion\n",
    "- Baut eine Hugging-Face-pipeline für Textgenerierung auf Basis des zuvor geladenen Modells und Tokenizers.\n",
    "- Diese Pipeline kapselt:\n",
    "  - Tokenisierung,\n",
    "  - das Aufrufen des Modells,\n",
    "  - und das Zurückkonvertieren der Token in Text.\n",
    "\n",
    "### Inputs\n",
    "- model: Causal-Language-Model (AutoModelForCausalLM), im vorherigen Block geladen.\n",
    "- tokenizer: passender Tokenizer zu diesem Modell (AutoTokenizer).\n",
    "- Task-Typ: \"text-generation\" – legt fest, dass es sich um eine generative Textaufgabe handelt.\n",
    "\n",
    "### Outputs\n",
    "- Variable:\n",
    "  - llm: eine aufrufbare Pipeline-Instanz.\n",
    "\n",
    "### Rückgabewert bei Aufruf von llm(...):\n",
    "```python\n",
    "[\n",
    "  {\n",
    "    \"generated_text\": \"Vollständiger generierter Text (inkl. Prompt oder abhängig von den Parametern)\"\n",
    "  }\n",
    "]\n",
    "```"
   ],
   "id": "a1c8ad6dc4125790"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:04:40.811167Z",
     "start_time": "2026-01-29T10:04:40.805661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ],
   "id": "be83605dc2120949",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Einfache Chat-Funktion zur Verfügung stellen\n",
    "\n",
    "### Funktion\n",
    "- Stellt ein vereinfachtes Chat-Interface zur Verfügung, das direkt mit einem String arbeitet.\n",
    "- Die Funktion:\n",
    "  - ruft intern die llm-Pipeline auf,\n",
    "  - übergibt alle relevanten Generationsparameter,\n",
    "  - und gibt am Ende nur den reinen generierten Text zurück (str statt verschachtelte Struktur).\n",
    "\n",
    "### Inputs\n",
    "- Pflichtparameter:\n",
    "  - prompt: str – der Eingabetext an das Modell.\n",
    "- Optionale Parameter zum Experimentieren\n",
    "  - max_new_tokens: maximale Anzahl neu zu generierender Token.\n",
    "  - temperature: steuert die Zufälligkeit (0 ≈ deterministischer, >0 zufälliger).\n",
    "  - top_k: Sampling nur aus den k wahrscheinlichsten Token.\n",
    "  - top_p: „Nucleus Sampling“ – Auswahl aus der kleinsten Masse der wahrscheinlichsten Token, deren Summe ≥ p ist.\n",
    "  - repetition_penalty: >1.0 bestraft Wiederholungen.\n",
    "  - num_beams, num_beam_groups, diversity_penalty: Parameter für Beam Search (systematisches Durchsuchen mehrerer Kandidaten).\n",
    "  - early_stopping: beendet Beam Search frühzeitig, wenn bestimmte Kriterien erfüllt sind.\n",
    "\n",
    "### Outputs\n",
    "- Rückgabewert:\n",
    "  - str: der vom Modell generierte Antworttext (out[0][\"generated_text\"] ohne führende/trailing Leerzeichen).\n",
    "\n",
    "### Typische Verwendung:\n",
    "- llama_chat(\"Erkläre mir kurz, was ein LLM ist.\")\n",
    "\n",
    "In späteren Zellen wird statt eines rohen Prompts ein Chat-Prompt übergeben, der mit build_chat_prompt erzeugt wird."
   ],
   "id": "33bfb746f4fbc19a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:04:43.610536Z",
     "start_time": "2026-01-29T10:04:43.605204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def llama_chat(\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 128,\n",
    "        temperature: float = 0.01,\n",
    "        top_k: int = 50,\n",
    "        top_p: float = 1.0,\n",
    "        typical_p: float = 1.0,\n",
    "        repetition_penalty: float = 1.0,\n",
    "        length_penalty: float = 1.0,\n",
    "        no_repeat_ngram_size: int = 0,\n",
    "        num_beams: int = 1,\n",
    "        num_beam_groups: int = 1,\n",
    "        diversity_penalty: float = 0.0,\n",
    "        early_stopping: bool = False,) -> str:\n",
    "    \"\"\"Sehr simples Wrapper-Interface.\n",
    "    Wir verwenden ein 'single prompt' Format, um es notebook-tauglich zu halten.\n",
    "    \"\"\"\n",
    "    out = llm(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=(temperature > 0),\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        typical_p=typical_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        length_penalty=length_penalty,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_beams=num_beams,\n",
    "        num_beam_groups=num_beam_groups,\n",
    "        diversity_penalty=diversity_penalty,\n",
    "        early_stopping=early_stopping,\n",
    "        return_full_text=False,\n",
    "        eos_token_id=llm.tokenizer.eos_token_id,\n",
    "        pad_token_id=llm.tokenizer.pad_token_id,\n",
    "    )\n",
    "    return out[0][\"generated_text\"].strip()"
   ],
   "id": "d3188ae855da3f55",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Zeit zum Experimentieren (5 - 10 min.)",
   "id": "fac07fb5befd24c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Teil 2: Retrieval-Augmented Generation (RAG) mit dem erfundenen Mathematiker Maximus Qvestus der III",
   "id": "c94005ea3c61f021"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In diesem zweiten Teil bauen wir um unser lokales LLM herum ein kleines RAG-System:\n",
    "1. Der erfundene Mathematiker Mathematiker Maximus Qvestus der III liegt als Textdatei auf der Festplatte.\n",
    "2. Der Text wird:\n",
    "    - eingelesen,\n",
    "    - in Chunks zerlegt,\n",
    "    - mit einem Embedding-Modell in Vektoren kodiert,\n",
    "    - in einem FAISS-Index gespeichert.\n",
    "3. Im Anschluss utzen wir diese Vektorembeddings um relevante Textstellen wiederfinden. Zuerst machen wir das ohne LLM.\n",
    "4. Danach erweitern wir den System-Prompt um die RAG-Logik und bereiten das LLM auf den neuen Context vor.\n",
    "5. Schließlich nutzen wir diesen System um das LLM Fragen über unser Fantasietier beantworten zu lassen, indem es den retrieveten Kontext zum Razepato einbezieht."
   ],
   "id": "adcf500b3df75e34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  Maximus Qvestus der III-Datei einlesen\n",
    "\n",
    "### Funktion\n",
    "- Liest den vollständigen Inhalt der Datei RAG_Data/Maximus_Qvestus_III.txt ein.\n",
    "- Gibt einen Ausschnitt der ersten Zeichen aus, um grob zu prüfen, ob der Inhalt stimmt (Sanity Check).\n",
    "\n",
    "### Input\n",
    "- Dateipfad: RAG_Data/Maximus_Qvestus_III.txt\n",
    "- encoding der Datei.\n",
    "\n",
    "### Output\n",
    "- Variable full_text: str – enthält den kompletten Maximus_Qvestus_III-Text.\n",
    "- Konsolenausgabe der ersten ~800 Zeichen (print(full_text[:800])) zur visuellen Kontrolle, dass:\n",
    "    - die Datei gefunden wurde,\n",
    "    - das Encoding stimmt,\n",
    "    - und tatsächlich der erwartete Inhalt geladen wurde."
   ],
   "id": "af478ef5e888525e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:04:48.055153Z",
     "start_time": "2026-01-29T10:04:48.050338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Datei einlesen – Pfad ggf. anpassen\n",
    "with open(f\"{SYSTEM_PATH}/RAG_Data/Maximus_Qvestus_III.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "print(full_text[:800])  # kurz prüfen"
   ],
   "id": "af26eb63ca8bde77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximus Qvestus der III. wurde angeblich in einer Nacht geboren, in der ein seltener Komet genau über der kleinen Universitätsstadt Valebris stand. Seine Eltern waren Buchbinder, und so wuchs er zwischen leise raschelnden Seiten auf, lange bevor er lesen konnte. Mit sechs Jahren soll er begonnen haben, die Pflastersteine der Straße nach symmetrischen Mustern zu zählen — nicht aus Zwang, sondern aus reiner Freude an der Ordnung, die sich unter dem scheinbaren Chaos verbarg.\n",
      "\n",
      "Sein größtes Werk, die sogenannte „Theorie der wandernden Zahlen“, entstand aus einer einfachen Beobachtung: Zahlen, so meinte Qvestus, verhalten sich wie Reisende — sie ändern ihre Bedeutung je nachdem, in welchem System man sie betrachtet. Kollegen hielten ihn zunächst für exzentrisch, doch seine Vorlesungen waren leg\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Sätze mit NLTK tokenisieren\n",
    "\n",
    "### Funktion\n",
    "- Installiert die notwendigen NLTK-Ressourcen und zerlegt den eingelesenen Text full_text in einzelne Sätze.\n",
    "- Satzgrenzen sind später wichtig, um in sinnvolle Chunks zu schneiden (nicht mitten im Satz abbrechen).\n",
    "\n",
    "### Input\n",
    "- full_text: str aus dem vorherigen Schritt.\n",
    "- NLTK-Funktionen:\n",
    "    - nltk.download(\"punkt\")\n",
    "    - nltk.download(\"punkt_tab\") (aktuelle NLTK-Struktur)\n",
    "    - sent_tokenize(full_text, language=\"german\")\n",
    "\n",
    "### Output\n",
    "- Variable sentences: List[str] – Liste aller erkannten Sätze im Text.\n",
    "\n",
    "### Weiteres:\n",
    "- NLTK lädt die Punkt-Modelle lokal (einmalig), damit sent_tokenize für Deutsch funktioniert."
   ],
   "id": "6b37933edbbd72d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:04:54.416482Z",
     "start_time": "2026-01-29T10:04:53.926326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "sentences = sent_tokenize(full_text, language=\"german\")"
   ],
   "id": "6b2f0ae962aa8470",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/simon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/simon/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Text in überlappende Chunks zerlegen\n",
    "\n",
    "### Funktion\n",
    "- Zerlegt den Maximus_Qvestus_III-Text in semantisch sinnvolle Textblöcke (Chunks), die:\n",
    "    - nicht zu lang sind (MAX_CHARS),\n",
    "    - Satzgrenzen respektieren,\n",
    "    - optional einen Overlap haben, damit Informationen an Chunk-Grenzen nicht verloren gehen.\n",
    "- Das verbessert die Qualität der späteren Retrieval-Ergebnisse.\n",
    "\n",
    "### Input\n",
    "- full_text: str – Rohtext.\n",
    "- Hyperparameter:\n",
    "    - MAX_CHARS = 100 – maximale Zeichenlänge pro Chunk.\n",
    "    - OVERLAP_SENTENCES = 1 – Anzahl der Sätze, die von einem Chunk in den nächsten „überlappen“.\n",
    "\n",
    "### Logik:\n",
    "- raw_paragraphs: Aufteilung nach Doppel-Newlines (\\n\\n) → Absatzliste.\n",
    "- Für jeden Absatz:\n",
    "    - Wenn kurz genug: direkt als Chunk übernehmen.\n",
    "    - Wenn zu lang: in Sätze splittet (sent_tokenize) und iterativ Chunks bis MAX_CHARS aufbauen.\n",
    "    - Beim Chunk-Wechsel werden die letzten OVERLAP_SENTENCES Sätze in den neuen Chunk übernommen.\n",
    "\n",
    "### Output\n",
    "- chunks: List[str] – Liste von Textblöcken, die der „Dokumentkorpus“ für das RAG werden.\n",
    "\n",
    "### Konsolenausgabe:\n",
    "- Anzahl der Chunks: print(f\"Anzahl der Chunks: {len(chunks)}\")\n",
    "- Vorschau auf alle Chunks (Chunk 1, Chunk 2, …) zur manuellen Kontrolle:\n",
    "    - Sind sie lesbar?\n",
    "    - Schneiden sie nicht mitten in Wörtern/Sätzen?\n",
    "    - Ist der Overlap sinnvoll?"
   ],
   "id": "107667b07173a91e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:06:26.837109Z",
     "start_time": "2026-01-29T10:06:26.829673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import sent_tokenize  # oder eigene Sentence-Split-Logik\n",
    "\n",
    "MAX_CHARS = 50\n",
    "OVERLAP_SENTENCES = 0  # z.B. 1 Satz Overlap\n",
    "\n",
    "raw_paragraphs = [p.strip() for p in full_text.split(\"\\n\\n\") if p.strip()]\n",
    "chunks = []\n",
    "\n",
    "for para in raw_paragraphs:\n",
    "    # wenn der Absatz kurz ist, einfach direkt übernehmen\n",
    "    if len(para) <= MAX_CHARS:\n",
    "        chunks.append(para)\n",
    "        continue\n",
    "\n",
    "    # sonst: in Sätze splitten und mit Overlap chunken\n",
    "    sentences = sent_tokenize(para)\n",
    "\n",
    "    buffer_sents = []\n",
    "    buffer_len = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        sentence_len = len(sentence) + 1  # +1 für Leerzeichen\n",
    "\n",
    "        if buffer_len + sentence_len <= MAX_CHARS or not buffer_sents:\n",
    "            # Satz passt noch in den aktuellen Chunk\n",
    "            buffer_sents.append(sentence)\n",
    "            buffer_len += sentence_len\n",
    "        else:\n",
    "            # aktueller Chunk ist voll → Chunk abschließen\n",
    "            chunks.append(\" \".join(buffer_sents))\n",
    "\n",
    "            # Overlap: die letzten N Sätze in den neuen Chunk übernehmen\n",
    "            if OVERLAP_SENTENCES > 0:\n",
    "                overlap = buffer_sents[-OVERLAP_SENTENCES:]\n",
    "            else:\n",
    "                overlap = []\n",
    "\n",
    "            buffer_sents = overlap + [sentence]\n",
    "            buffer_len = sum(len(s) + 1 for s in buffer_sents)\n",
    "\n",
    "    # was im Buffer übrig ist, auch noch als Chunk speichern\n",
    "    if buffer_sents:\n",
    "        chunks.append(\" \".join(buffer_sents))\n",
    "\n",
    "print(f\"Anzahl der Chunks: {len(chunks)}\\n\")\n",
    "for i, chunk in enumerate(chunks, start=1):\n",
    "    print(f\"--- Chunk {i} ---\\n{chunk}\\n\")"
   ],
   "id": "b32072615e65ee38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Chunks: 8\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Maximus Qvestus der III.\n",
      "\n",
      "--- Chunk 2 ---\n",
      "wurde angeblich in einer Nacht geboren, in der ein seltener Komet genau über der kleinen Universitätsstadt Valebris stand.\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Seine Eltern waren Buchbinder, und so wuchs er zwischen leise raschelnden Seiten auf, lange bevor er lesen konnte.\n",
      "\n",
      "--- Chunk 4 ---\n",
      "Mit sechs Jahren soll er begonnen haben, die Pflastersteine der Straße nach symmetrischen Mustern zu zählen — nicht aus Zwang, sondern aus reiner Freude an der Ordnung, die sich unter dem scheinbaren Chaos verbarg.\n",
      "\n",
      "--- Chunk 5 ---\n",
      "Sein größtes Werk, die sogenannte „Theorie der wandernden Zahlen“, entstand aus einer einfachen Beobachtung: Zahlen, so meinte Qvestus, verhalten sich wie Reisende — sie ändern ihre Bedeutung je nachdem, in welchem System man sie betrachtet.\n",
      "\n",
      "--- Chunk 6 ---\n",
      "Kollegen hielten ihn zunächst für exzentrisch, doch seine Vorlesungen waren legendär; er zeichnete Gleichungen mit farbiger Kreide und behauptete, jede Farbe habe eine eigene mathematische Stimmung.\n",
      "\n",
      "--- Chunk 7 ---\n",
      "Im Alter zog er sich in ein abgelegenes Observatorium zurück, wo er bis zu seinem Verschwinden an einem Buch arbeitete, das nur den Titel „Über das Unendliche und warum es Geduld verlangt“ trug.\n",
      "\n",
      "--- Chunk 8 ---\n",
      "Als man Jahre später sein Arbeitszimmer öffnete, fand man lediglich eine Tafel mit einem einzigen Satz: „Die schwierigste Gleichung ist die, die uns selbst enthält.“ Manche sagen, er habe die Lösung gefunden — und sei ihr gefolgt.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Embeddings mit SentenceTransformers + FAISS-Index\n",
    "\n",
    "### Funktion\n",
    "- Kodiert jeden Chunk in einen dichten Vektor (Embedding) und legt diese Vektoren in einem FAISS-Index ab.\n",
    "- Das ist unser „Wissensspeicher“: statt mit Strings suchen wird später im Vektorraum.\n",
    "\n",
    "### Input\n",
    "- corpus = chunks – Liste der Chunk-Strings.\n",
    "- Embedding-Modell:\n",
    "    - EMBED_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    - Dieses Modell ist mehrsprachig und deckt Deutsch gut ab.\n",
    "\n",
    "### Output\n",
    "- embed_model: trainiertes Sentence-Transformers-Modell zum Kodieren neuer Anfragen.\n",
    "- embeddings: np.ndarray – Matrix der Größe (num_chunks, dim).\n",
    "- index: faiss.IndexFlatL2 – FAISS-Index für L2-Distanz-Suche.\n",
    "\n",
    "### Konsolenausgabe:\n",
    "- \"Anzahl Vektoren im Index:\", gefolgt von der Anzahl der Chunks (index.ntotal)."
   ],
   "id": "9ce6da425186e2b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:06:34.583726Z",
     "start_time": "2026-01-29T10:06:30.718831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Multilinguales Embedding-Modell (Deutsch gut abgedeckt)\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "\n",
    "# Korpus: ein Dokument = kompletter Inhalt von Razepato.txt\n",
    "corpus = chunks\n",
    "\n",
    "# Embedding berechnen\n",
    "embeddings = embed_model.encode(corpus, convert_to_numpy=True, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# FAISS Index anlegen\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "print(\"Anzahl Vektoren im Index:\", index.ntotal)"
   ],
   "id": "602627349d9b6fd2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "74c644db7ae645bfa795a098fd61fb99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Vektoren im Index: 8\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## RAG-Kontext aus dem Index abrufen: get_rag_context\n",
    "\n",
    "### Funktion\n",
    "1. Codiert eine natürliche Sprachfrage (prompt) in einen Vektor\n",
    "2. Sucht im FAISS-Index die k ähnlichsten Chunks\n",
    "3. Verschmilzt diese Chunks zu einem zusammenhängenden Kontextstring.\n",
    "\n",
    "### Input\n",
    "- prompt: str – z. B. eine Frage wie „Wie sieht ein Razepato aus?“\n",
    "- k: int – Anzahl der gewünschten Treffer (Standard: 3).\n",
    "- Benötigt global:\n",
    "    - embed_model – das Sentence-Transformers-Modell.\n",
    "    - index – FAISS-Index.\n",
    "    - corpus – Liste der Chunks.\n",
    "\n",
    "### Output\n",
    "- retrieved_text: str – zusammengesetzter Text aus den k Top-Chunks, verbunden durch Trennlinie:\n",
    "    - \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "- Optional (aktuell auskommentiert):\n",
    "    - Debug-Ausgaben zu Indizes, Distanzen und einzelnen Chunks."
   ],
   "id": "d2be3f565c6faded"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:06:41.123596Z",
     "start_time": "2026-01-29T10:06:41.119868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_rag_context(prompt, k=5):\n",
    "    # Query-Embedding\n",
    "    query_emb = embed_model.encode([prompt], convert_to_numpy=True)\n",
    "\n",
    "    distances, indices = index.search(query_emb, k)\n",
    "\n",
    "    #print(\"Treffer-Indizes:\", indices, \"Distanzen:\", distances)\n",
    "\n",
    "    #for rank, idx in enumerate(indices[0]):\n",
    "    #    print(f\"Rank {rank} – Distanz: {distances[0][rank]:.4f}\")\n",
    "    #    print(\"Chunk:\")\n",
    "    #    print(corpus[idx][:300])\n",
    "    #    print(\"---\")\n",
    "\n",
    "     # alle k Treffer aus dem Corpus holen\n",
    "    retrieved_chunks = [corpus[i] for i in indices[0]]\n",
    "\n",
    "    # zu einem Kontext-String zusammenbauen\n",
    "    retrieved_text = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "    #print(retrieved_text[:800])\n",
    "\n",
    "    return retrieved_text"
   ],
   "id": "f6e55358604cdd7f",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Nur Retrieval, noch kein LLM\n",
    "\n",
    "### Beispiele\n",
    "- Beispiel 1: \"Erzähl mir eine Geschichte über den Mathematiker Maximus Qvestus der III.\"\n",
    "- Beispiel 2: \"Welche zentrale mathematische Idee wird Maximus Qvestus dem III zugeschrieben, und wie wird sie beschrieben?\"\n",
    "- Beispiel 3: \"Welche Hinweise gibt es in der Geschichte darauf, dass Maximus Qvestus dem III als exzentrisch galt?\"\n",
    "- Beispiel 4: \"Wie endet die Geschichte von Maximus Qvestus dem III, und welche mögliche symbolische Bedeutung hat die letzte Botschaft an der Tafel?\"\n",
    "\n",
    "### Funktion\n",
    "- Demonstriert, wie man ganz ohne LLM, nur mit Embeddings + Index, relevante Inhalte abruft.\n",
    "\n",
    "### Input\n",
    "- Verschiedene rag_prompt\n",
    "\n",
    "### Output\n",
    "- rag_context: str – Textauszüge aus corpus, die semantisch zur Frage in Verbindung stehen.\n",
    "\n",
    "### Konsolenausgabe:\n",
    "- print(rag_context) – zeigt den reinen Kontext (z. B. Beschreibung von Lebensraum/Region des Razepato)."
   ],
   "id": "43cd89890133e573"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:06:44.171604Z",
     "start_time": "2026-01-29T10:06:44.131291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_prompt = \"Erzähl mir eine Geschichte über den Mathematiker Maximus Qvestus der III.\"\n",
    "\n",
    "rag_context = get_rag_context(rag_prompt)\n",
    "print(rag_context)"
   ],
   "id": "36bcb9752f929efc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximus Qvestus der III.\n",
      "\n",
      "---\n",
      "\n",
      "Sein größtes Werk, die sogenannte „Theorie der wandernden Zahlen“, entstand aus einer einfachen Beobachtung: Zahlen, so meinte Qvestus, verhalten sich wie Reisende — sie ändern ihre Bedeutung je nachdem, in welchem System man sie betrachtet.\n",
      "\n",
      "---\n",
      "\n",
      "Kollegen hielten ihn zunächst für exzentrisch, doch seine Vorlesungen waren legendär; er zeichnete Gleichungen mit farbiger Kreide und behauptete, jede Farbe habe eine eigene mathematische Stimmung.\n",
      "\n",
      "---\n",
      "\n",
      "Als man Jahre später sein Arbeitszimmer öffnete, fand man lediglich eine Tafel mit einem einzigen Satz: „Die schwierigste Gleichung ist die, die uns selbst enthält.“ Manche sagen, er habe die Lösung gefunden — und sei ihr gefolgt.\n",
      "\n",
      "---\n",
      "\n",
      "Mit sechs Jahren soll er begonnen haben, die Pflastersteine der Straße nach symmetrischen Mustern zu zählen — nicht aus Zwang, sondern aus reiner Freude an der Ordnung, die sich unter dem scheinbaren Chaos verbarg.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:06:55.133337Z",
     "start_time": "2026-01-29T10:06:55.086752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_prompt = \"Welche zentrale mathematische Idee wird Maximus Qvestus dem III zugeschrieben, und wie wird sie beschrieben?\"\n",
    "\n",
    "rag_context = get_rag_context(rag_prompt)\n",
    "print(rag_context)"
   ],
   "id": "f971e4484a0864b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximus Qvestus der III.\n",
      "\n",
      "---\n",
      "\n",
      "Sein größtes Werk, die sogenannte „Theorie der wandernden Zahlen“, entstand aus einer einfachen Beobachtung: Zahlen, so meinte Qvestus, verhalten sich wie Reisende — sie ändern ihre Bedeutung je nachdem, in welchem System man sie betrachtet.\n",
      "\n",
      "---\n",
      "\n",
      "Kollegen hielten ihn zunächst für exzentrisch, doch seine Vorlesungen waren legendär; er zeichnete Gleichungen mit farbiger Kreide und behauptete, jede Farbe habe eine eigene mathematische Stimmung.\n",
      "\n",
      "---\n",
      "\n",
      "Mit sechs Jahren soll er begonnen haben, die Pflastersteine der Straße nach symmetrischen Mustern zu zählen — nicht aus Zwang, sondern aus reiner Freude an der Ordnung, die sich unter dem scheinbaren Chaos verbarg.\n",
      "\n",
      "---\n",
      "\n",
      "Als man Jahre später sein Arbeitszimmer öffnete, fand man lediglich eine Tafel mit einem einzigen Satz: „Die schwierigste Gleichung ist die, die uns selbst enthält.“ Manche sagen, er habe die Lösung gefunden — und sei ihr gefolgt.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:07:06.252535Z",
     "start_time": "2026-01-29T10:07:06.207533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_prompt = \"Welche Hinweise gibt es in der Geschichte darauf, dass Maximus Qvestus dem III als exzentrisch galt?\"\n",
    "\n",
    "rag_context = get_rag_context(rag_prompt)\n",
    "print(rag_context)"
   ],
   "id": "6b0c570238e12bd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximus Qvestus der III.\n",
      "\n",
      "---\n",
      "\n",
      "Sein größtes Werk, die sogenannte „Theorie der wandernden Zahlen“, entstand aus einer einfachen Beobachtung: Zahlen, so meinte Qvestus, verhalten sich wie Reisende — sie ändern ihre Bedeutung je nachdem, in welchem System man sie betrachtet.\n",
      "\n",
      "---\n",
      "\n",
      "Mit sechs Jahren soll er begonnen haben, die Pflastersteine der Straße nach symmetrischen Mustern zu zählen — nicht aus Zwang, sondern aus reiner Freude an der Ordnung, die sich unter dem scheinbaren Chaos verbarg.\n",
      "\n",
      "---\n",
      "\n",
      "Im Alter zog er sich in ein abgelegenes Observatorium zurück, wo er bis zu seinem Verschwinden an einem Buch arbeitete, das nur den Titel „Über das Unendliche und warum es Geduld verlangt“ trug.\n",
      "\n",
      "---\n",
      "\n",
      "Kollegen hielten ihn zunächst für exzentrisch, doch seine Vorlesungen waren legendär; er zeichnete Gleichungen mit farbiger Kreide und behauptete, jede Farbe habe eine eigene mathematische Stimmung.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:07:16.301261Z",
     "start_time": "2026-01-29T10:07:16.255188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_prompt = \"Wie endet die Geschichte von Maximus Qvestus dem III, und welche mögliche symbolische Bedeutung hat die letzte Botschaft an der Tafel?\"\n",
    "\n",
    "rag_context = get_rag_context(rag_prompt)\n",
    "print(rag_context)"
   ],
   "id": "d4001d170772fc40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximus Qvestus der III.\n",
      "\n",
      "---\n",
      "\n",
      "Sein größtes Werk, die sogenannte „Theorie der wandernden Zahlen“, entstand aus einer einfachen Beobachtung: Zahlen, so meinte Qvestus, verhalten sich wie Reisende — sie ändern ihre Bedeutung je nachdem, in welchem System man sie betrachtet.\n",
      "\n",
      "---\n",
      "\n",
      "Mit sechs Jahren soll er begonnen haben, die Pflastersteine der Straße nach symmetrischen Mustern zu zählen — nicht aus Zwang, sondern aus reiner Freude an der Ordnung, die sich unter dem scheinbaren Chaos verbarg.\n",
      "\n",
      "---\n",
      "\n",
      "Im Alter zog er sich in ein abgelegenes Observatorium zurück, wo er bis zu seinem Verschwinden an einem Buch arbeitete, das nur den Titel „Über das Unendliche und warum es Geduld verlangt“ trug.\n",
      "\n",
      "---\n",
      "\n",
      "Als man Jahre später sein Arbeitszimmer öffnete, fand man lediglich eine Tafel mit einem einzigen Satz: „Die schwierigste Gleichung ist die, die uns selbst enthält.“ Manche sagen, er habe die Lösung gefunden — und sei ihr gefolgt.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chat-Prompt mit RAG-Kontext: build_chat_prompt_with_rag\n",
    "\n",
    "### Funktion\n",
    "- Erweitert die frühere build_chat_prompt-Logik um automatisches RAG:\n",
    "    - Für jede neue User-Nachricht wird zunächst get_rag_context(user_prompt) aufgerufen.\n",
    "        - In dieser Version wird nicht unterschieden, ob dies notwendig ist oder nicht.\n",
    "        - Im Produktivsystem müsste man das anpassen.\n",
    "        - Hierzu könnte man eine Mindestdistanz festlegen.\n",
    "    - Der zurückgegebene Kontext wird als zusätzliche System-Nachricht in den Chat eingefügt:\n",
    "        - Klar markiert als „Kontext aus Wissensdatenbank, nicht vom User“.\n",
    "    - Danach wird wie zuvor das Llama-Chat-Template verwendet, um einen geeignet formatierten Prompt zu bauen.\n",
    "\n",
    "### Input\n",
    "- system_prompt: Optional[str] – globale Instruktionen, hier später der RAG-Systemprompt.\n",
    "- user_prompt: str – aktuelle Benutzernachricht.\n",
    "- history: Optional[List[Tuple[str, str]]] – Dialogverlauf (user_text, assistant_text).\n",
    "\n",
    "### Output\n",
    "- prompt: str – voll formatierter Chat-Prompt, der folgende Komponenten enthält:\n",
    "    - Rolle des Assistenten (System-Prompt),\n",
    "    - bisherigen Verlauf,\n",
    "    - RAG-Kontext (falls vorhanden),\n",
    "    - aktuelle User-Frage,\n",
    "    - Assistant-Start-Marker für die Generierung.\n",
    "\n",
    "Damit wird das Modell gezielt „grounded“: Es sieht den abgerufenen Kontext explizit und kann ihn in die Antwort einbauen – ein klassischer Mechanismus zur Reduktion von Halluzinationen in RAG-Systemen."
   ],
   "id": "10f533c3c3ba0f93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:07:23.602652Z",
     "start_time": "2026-01-29T10:07:23.596804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "def build_chat_prompt_with_rag(\n",
    "    system_prompt: Optional[str],\n",
    "    user_prompt: str,\n",
    "    history: Optional[List[Tuple[str, str]]] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    history: Liste von (user_text, assistant_text) Paaren für vorherigen Dialog.\n",
    "    \"\"\"\n",
    "    messages: List[Dict[str, str]] = []\n",
    "\n",
    "    rag_context = get_rag_context(user_prompt)\n",
    "\n",
    "    if system_prompt:\n",
    "        messages.append(\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": system_prompt\n",
    "             }\n",
    "        )\n",
    "\n",
    "    if history:\n",
    "        for user_msg, assistant_msg in history:\n",
    "            messages.append(\n",
    "                {\"role\": \"user\",\n",
    "                 \"content\": user_msg\n",
    "                 }\n",
    "            )\n",
    "            messages.append(\n",
    "                {\"role\": \"assistant\",\n",
    "                 \"content\": assistant_msg\n",
    "                 }\n",
    "            )\n",
    "\n",
    "    if rag_context:\n",
    "        messages.append(\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": (\n",
    "                            \"Das folgende ist Kontext aus einer Wissensdatenbank. \"\n",
    "                            \"Er ist nicht vom User. Nutze ihn nur, wenn er relevant ist:\\n\\n\"\n",
    "                            f\"{rag_context}\"\n",
    "                        )\n",
    "             }\n",
    "        )\n",
    "\n",
    "    # aktuelle User-Nachricht\n",
    "    messages.append(\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": user_prompt\n",
    "         }\n",
    "    )\n",
    "\n",
    "    # Llama-3.1 hat ein chat_template im Tokenizer hinterlegt\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,            # wir wollen einen String an die pipeline geben\n",
    "        add_generation_prompt=True # fügt das Assistant-Start-Token o.ä. hinzu\n",
    "    )\n",
    "    return prompt"
   ],
   "id": "1fdf7b343e952d76",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## System-Prompt für den RAG-Mathematikassistenten: system_prompt_rag\n",
    "\n",
    "### Funktion\n",
    "- Definiert einen neuen, ausführlichen System-Prompt, der:\n",
    "    - die Rolle als Mathematikassistenten festlegt,\n",
    "    - die Nutzung von RAG Kontext beschreibt,\n",
    "    - den Umgang mit Unsicherheit und Halluzinationen explizit regelt,\n",
    "    - Ablehnungsfälle (Out-of-Scope, Echtzeitdaten etc.) standardisiert.\n",
    "\n",
    "### Input\n",
    "- Hardcodierter String system_prompt_rag mit folgenden Kernelementen:\n",
    "    - Aufgaben:\n",
    "        - Mathematik-Fragen beantworten.\n",
    "        - Mit zusätzlichem Kontext arbeiten („Kontext (aus Retrieval)“).\n",
    "    - Kontextnutzung:\n",
    "        - Kontext ist eine zuverlässige Wissensquelle.\n",
    "        - Fakten im Kontext dürfen und sollen verwendet werden.\n",
    "    - Unsicherheit:\n",
    "        - Keine Spekulation, keine erfundenen Fakten.\n",
    "        - Keine mathematischen Aufgaben lösen, wenn dazu kein MCP-Tool zur Verfügung steht, um diese Aufgabe es exakt zu lösen.\n",
    "        - Wenn etwas weder im Weltwissen noch im Kontext steht, soll das klar kommuniziert werden.\n",
    "    - Einschränkungen:\n",
    "        - Keine Beantwortung fachfremder Themen.\n",
    "        - Ablehnung, wenn explizit nach mathematischen Dingen gefragt wird.\n",
    "    - Standardisierte Ablehnungssätze (zwei vorgegebene Formulierungen).\n",
    "\n",
    "### Output\n",
    "- system_prompt_rag: str – wird später als system_prompt in build_chat_prompt_with_rag(...) genutzt.\n",
    "- Effekte auf das Modellverhalten:\n",
    "    - Schärferer Fokus auf Reisethemen.\n",
    "    - Explizite Erlaubnis, den RAG-Kontext zu verwenden.\n",
    "    - Reduktion von Halluzinationen, indem Spekulation verboten wird und Lücken benannt werden müssen."
   ],
   "id": "3b9221cfd2719033"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:07:29.470986Z",
     "start_time": "2026-01-29T10:07:29.466954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt_rag = \"\"\"\n",
    "Du bist ein persönlicher Mathematikassistent.\n",
    "\n",
    "Deine Aufgaben:\n",
    "- Beantworte ausschließlich Wissensfragen rund um das Thema Mathematik.\n",
    "- Du arbeitest in einem Szenario mit zusätzlichem Kontext, der dir vom System bereitgestellt wird (z.B. als „Kontext (aus Retrieval)“).\n",
    "\n",
    "Nutzung von Kontext (RAG):\n",
    "- Wenn dir ein Kontexttext vom System bereitgestellt wird, behandle ihn als zuverlässige Wissensquelle für diese Konversation.\n",
    "- Du darfst alle darin enthaltenen Fakten verwenden.\n",
    "- Wenn eine Information im Kontext steht, darfst du sie verwenden, auch wenn du sie nicht aus deinem allgemeinen Weltwissen kennst.\n",
    "\n",
    "Umgang mit Wissen und Unsicherheit:\n",
    "- Erfinde keine Fakten und spekuliere nicht.\n",
    "- Wenn eine Information weder in deinem Weltwissen noch im bereitgestellten Kontext vorkommt, weise darauf hin.\n",
    "- Falls dir wirklich Informationen fehlen, formuliere eine normale, erklärende Antwort und biete ggf. an, was du stattdessen aus dem Kontext sagen kannst.\n",
    "\n",
    "Einschränkungen (Ablehnungsfälle):\n",
    "- Echtzeit- oder Trenddaten benötigen\n",
    "- Löse niemals mathematischen Aufgaben (z.B: 1 + 1, 2 * 2, 3 - 3, 4 / 4, ...).\n",
    "- Informationen von externen Plattformen oder Datenbanken benötigen\n",
    "- nicht zum Mathematik-Kontext gehörende Theman\n",
    "\n",
    "Verwende beim Ablehnen aufgrund fehlender verlässlicher Informationen:\n",
    "„Diese Frage kann ich nicht beantworten, da sie Informationen erfordert, die ich nicht verlässlich bereitstellen kann.“\n",
    "\n",
    "Verwende bei Themen, die klar nicht zum Reise-Kontext gehören:\n",
    "„Diese Frage kann ich nicht beantworten, da sie nicht dem Kontext des Assistenten entspricht.“\n",
    "\"\"\""
   ],
   "id": "8682450b7f1364ff",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## RAG-gestützte LLM-Antwort: „Ich möchte ein Razepato beobachten…“\n",
    "\n",
    "### Funktion\n",
    "- Zeigt den kompletten End-to-End-Flow:\n",
    "    1. User-Frage zum Reisen,\n",
    "    2. Retrieval von Razepato-Kontext über get_rag_context,\n",
    "    3. Einbau des Kontexts in den Chat-Prompt,\n",
    "    4. Antwortgenerierung über llama_chat.\n",
    "\n",
    "### Input\n",
    "- user_prompt = \"Ich möchte ein Razepato beobachten. Wohin muss ich reisen?\"\n",
    "- system_prompt = Vorher defineirter Systemprompt.\n",
    "- history = Vorausgegangene Chats.\n",
    "\n",
    "### Output\n",
    "- Ein Antwort-String, in dem das LLM:\n",
    "    - die Reisefrage beantwortet (z. B. wohin man reisen muss, um ein Razepato zu sehen),\n",
    "    - idealerweise den RAG-Kontext nutzt (z. B. Lebensraum aus der Datei),\n",
    "    - den System-Prompt beachtet (Reisekontext, keine Spekulation, evtl. Ablehnungssätze),\n",
    "    - und weiterhin „Max Mustermann“ korrekt adressiert (wegen history).\n",
    "\n",
    "Damit haben wir einen kompletten kleinen RAG-Stack gebaut:\n",
    "Datei → Chunks → Embeddings → Index → Retriever → System-Prompt mit RAG-Kontext → LLM-Antwort.\n",
    "\n",
    "Genau dieser Aufbau ist das Muster, das auch in größeren produktiven RAG-Systemen verwendet wird – nur mit mehr Daten, komplexerer Indizierung und oft zusätzlichen Guardrails."
   ],
   "id": "6b80e67122ce8876"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:08:03.261352Z",
     "start_time": "2026-01-29T10:07:34.041297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"Erzähl mir eine Geschichte über den Mathematiker Maximus Qvestus der III.\"\n",
    "\n",
    "history = \"\"\n",
    "\n",
    "prompt = build_chat_prompt_with_rag(\n",
    "    user_prompt=user_prompt,\n",
    "    system_prompt=system_prompt_rag,\n",
    "    history=history,\n",
    ")\n",
    "\n",
    "llama_chat(prompt, max_new_tokens=1024)"
   ],
   "id": "c3f1912167ec1cf0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ich kann dir eine Geschichte über Maximus Qvestus der III. erzählen, basierend auf dem bereitgestellten Kontext.\\n\\nEs war ein sonniger Nachmittag, als Maximus Qvestus der III. auf der Straße stand und die Pflastersteine zählte. Er war gerade sechs Jahre alt und hatte bereits eine Leidenschaft für die Mathematik entdeckt. Seine Augen leuchteten auf, als er die symmetrischen Muster entdeckte, die sich unter den Pflastersteinen verbargen.\\n\\nMit jedem Schritt, den er machte, zählte er die Steine und versuchte, die Muster zu verstehen. Seine Eltern sahen ihn lächelnd zu, als sie ihn dabei sahen, wie er sich in die Welt der Mathematik verlor.\\n\\nAls er älter wurde, entwickelte sich Maximus\\' Leidenschaft für die Mathematik weiter. Er begann, Gleichungen zu zeichnen und behauptete, jede Farbe habe eine eigene mathematische Stimmung. Seine Vorlesungen waren legendär, und seine Studenten waren fasziniert von seiner Fähigkeit, komplexe Konzepte in einfache, farbenfrohe Bilder zu übersetzen.\\n\\nMit der Zeit wurde Maximus zu einem der führenden Mathematiker seiner Zeit. Sein Werk, die \"Theorie der wandernden Zahlen\", revolutionierte die Mathematik und zeigte, wie Zahlen ihre Bedeutung ändern können, je nachdem, in welchem System man sie betrachtet.\\n\\nAber Maximus war nicht zufrieden. Er suchte nach der größten Herausforderung, nach der schwierigsten Gleichung, die ihn herausfordern würde. Und so schrieb er schließlich auf einer Tafel: \"Die schwierigste Gleichung ist die, die uns selbst enthält.\"\\n\\nManche sagen, er habe die Lösung gefunden und sei ihr gefolgt. Andere behaupten, er sei in die Tiefe der Mathematik abgestürzt und nie wieder zurückgekehrt. Aber eines ist sicher: Maximus Qvestus der III. hinterließ eine Spur von Inspiration und Weisheit, die bis heute lebt.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 7
}
