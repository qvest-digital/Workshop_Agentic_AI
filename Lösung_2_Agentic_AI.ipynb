{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Huggingface Token setzen",
   "id": "314ddfa0ab265b0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:54:22.188444Z",
     "start_time": "2026-02-02T11:54:22.180292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "hf_token = \"hf_Token\"\n",
    "#hf_token = os.getenv(\"HF_TOKEN\")"
   ],
   "id": "1bcf278c95358198",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Git Repo per HTTPs Clonen",
   "id": "2c5eacb992777eab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:54:24.009966Z",
     "start_time": "2026-02-02T11:54:23.880128Z"
    }
   },
   "cell_type": "code",
   "source": "!git clone https://github.com/qvest-digital/Workshop_Agentic_AI.git",
   "id": "f89099ca1ee8918c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "fatal: destination path 'Workshop_Agentic_AI' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pfad setzen",
   "id": "b398c4d5562a7858"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:54:25.625406Z",
     "start_time": "2026-02-02T11:54:25.621630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#SYSTEM_PATH = \"/home/simon/Workshop_Agentic_AI\"\n",
    "SYSTEM_PATH = \"./Workshop_Agentic_AI\""
   ],
   "id": "95c9b36fee07fd1b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Requirements installieren",
   "id": "78aa612f2a446737"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:54:30.791594Z",
     "start_time": "2026-02-02T11:54:28.420580Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -r \"$SYSTEM_PATH/requirements.txt\"",
   "id": "88d6e57ee340b5d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: jupyter in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 1)) (1.1.1)\r\n",
      "Requirement already satisfied: pandas in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 2)) (2.3.3)\r\n",
      "Requirement already satisfied: pydantic in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 3)) (2.12.5)\r\n",
      "Requirement already satisfied: mcp==1.25.0 in /home/simon/.local/lib/python3.10/site-packages (from mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (1.25.0)\r\n",
      "Requirement already satisfied: requests in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 5)) (2.32.5)\r\n",
      "Requirement already satisfied: transformers in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 6)) (4.57.6)\r\n",
      "Requirement already satisfied: huggingface-hub in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 7)) (0.36.0)\r\n",
      "Requirement already satisfied: torch in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 8)) (2.9.1)\r\n",
      "Requirement already satisfied: accelerate in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 9)) (1.12.0)\r\n",
      "Requirement already satisfied: bitsandbytes in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 10)) (0.49.1)\r\n",
      "Requirement already satisfied: sentence-transformers in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 11)) (5.2.0)\r\n",
      "Requirement already satisfied: faiss-cpu in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 12)) (1.13.2)\r\n",
      "Requirement already satisfied: nltk in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 13)) (3.9.2)\r\n",
      "Requirement already satisfied: anyio>=4.5 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (4.11.0)\r\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.4.3)\r\n",
      "Requirement already satisfied: httpx>=0.27.1 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.28.1)\r\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (4.23.0)\r\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (2.12.0)\r\n",
      "Requirement already satisfied: pyjwt>=2.10.1 in /home/simon/.local/lib/python3.10/site-packages (from pyjwt[crypto]>=2.10.1->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (2.10.1)\r\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.0.21)\r\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (3.1.2)\r\n",
      "Requirement already satisfied: starlette>=0.27 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.51.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (4.15.0)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.1 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.4.2)\r\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.40.0)\r\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /home/simon/.local/lib/python3.10/site-packages (from mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (1.2.1)\r\n",
      "Requirement already satisfied: typer>=0.16.0 in /home/simon/.local/lib/python3.10/site-packages (from mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.19.2)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/simon/.local/lib/python3.10/site-packages (from pydantic->-r ./Workshop_Agentic_AI/requirements.txt (line 3)) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/simon/.local/lib/python3.10/site-packages (from pydantic->-r ./Workshop_Agentic_AI/requirements.txt (line 3)) (2.41.5)\r\n",
      "Requirement already satisfied: notebook in /home/simon/.local/lib/python3.10/site-packages (from jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (7.5.1)\r\n",
      "Requirement already satisfied: jupyter-console in /home/simon/.local/lib/python3.10/site-packages (from jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (6.6.3)\r\n",
      "Requirement already satisfied: nbconvert in /home/simon/.local/lib/python3.10/site-packages (from jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (7.16.6)\r\n",
      "Requirement already satisfied: ipykernel in /home/simon/.local/lib/python3.10/site-packages (from jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (7.1.0)\r\n",
      "Requirement already satisfied: ipywidgets in /home/simon/.local/lib/python3.10/site-packages (from jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (8.1.8)\r\n",
      "Requirement already satisfied: jupyterlab in /home/simon/.local/lib/python3.10/site-packages (from jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (4.5.1)\r\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/simon/.local/lib/python3.10/site-packages (from pandas->-r ./Workshop_Agentic_AI/requirements.txt (line 2)) (1.26.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/simon/.local/lib/python3.10/site-packages (from pandas->-r ./Workshop_Agentic_AI/requirements.txt (line 2)) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/simon/.local/lib/python3.10/site-packages (from pandas->-r ./Workshop_Agentic_AI/requirements.txt (line 2)) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/simon/.local/lib/python3.10/site-packages (from pandas->-r ./Workshop_Agentic_AI/requirements.txt (line 2)) (2024.2)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/simon/.local/lib/python3.10/site-packages (from requests->-r ./Workshop_Agentic_AI/requirements.txt (line 5)) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/simon/.local/lib/python3.10/site-packages (from requests->-r ./Workshop_Agentic_AI/requirements.txt (line 5)) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/simon/.local/lib/python3.10/site-packages (from requests->-r ./Workshop_Agentic_AI/requirements.txt (line 5)) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/simon/.local/lib/python3.10/site-packages (from requests->-r ./Workshop_Agentic_AI/requirements.txt (line 5)) (2024.2.2)\r\n",
      "Requirement already satisfied: filelock in /home/simon/.local/lib/python3.10/site-packages (from transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 6)) (3.16.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/simon/.local/lib/python3.10/site-packages (from transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 6)) (24.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 6)) (5.4.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/simon/.local/lib/python3.10/site-packages (from transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 6)) (2024.9.11)\r\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/simon/.local/lib/python3.10/site-packages (from transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 6)) (0.22.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/simon/.local/lib/python3.10/site-packages (from transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 6)) (0.4.5)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/simon/.local/lib/python3.10/site-packages (from transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 6)) (4.66.6)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/simon/.local/lib/python3.10/site-packages (from huggingface-hub->-r ./Workshop_Agentic_AI/requirements.txt (line 7)) (2024.9.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/simon/.local/lib/python3.10/site-packages (from huggingface-hub->-r ./Workshop_Agentic_AI/requirements.txt (line 7)) (1.2.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (1.14.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (3.1.5)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (12.8.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (11.3.3.83)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (10.3.9.90)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (11.7.3.90)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (12.5.8.93)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (2.27.5)\r\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (3.3.20)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (1.13.1.3)\r\n",
      "Requirement already satisfied: triton==3.5.1 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (3.5.1)\r\n",
      "Requirement already satisfied: psutil in /home/simon/.local/lib/python3.10/site-packages (from accelerate->-r ./Workshop_Agentic_AI/requirements.txt (line 9)) (7.2.1)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 11)) (1.5.0)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 11)) (1.13.1)\r\n",
      "Requirement already satisfied: click in /home/simon/.local/lib/python3.10/site-packages (from nltk->-r ./Workshop_Agentic_AI/requirements.txt (line 13)) (8.1.3)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r ./Workshop_Agentic_AI/requirements.txt (line 13)) (1.4.2)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/simon/.local/lib/python3.10/site-packages (from anyio>=4.5->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (1.2.2)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/simon/.local/lib/python3.10/site-packages (from anyio>=4.5->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (1.3.1)\r\n",
      "Requirement already satisfied: httpcore==1.* in /home/simon/.local/lib/python3.10/site-packages (from httpx>=0.27.1->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (1.0.9)\r\n",
      "Requirement already satisfied: h11>=0.16 in /home/simon/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.1->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.16.0)\r\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/simon/.local/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (24.2.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/simon/.local/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (2024.10.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/simon/.local/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.36.2)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/simon/.local/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.22.3)\r\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /usr/lib/python3/dist-packages (from pyjwt[crypto]>=2.10.1->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (3.4.8)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->-r ./Workshop_Agentic_AI/requirements.txt (line 2)) (1.16.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/simon/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (1.3.0)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/simon/.local/lib/python3.10/site-packages (from typer>=0.16.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/simon/.local/lib/python3.10/site-packages (from typer>=0.16.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (13.5.3)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/simon/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.16.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/simon/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.16.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (2.18.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/simon/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.16.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.1.2)\r\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.2.3)\r\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (1.8.12)\r\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (8.38.0)\r\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (8.8.0)\r\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (5.9.1)\r\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.2.1)\r\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (1.6.0)\r\n",
      "Requirement already satisfied: pyzmq>=25 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (27.1.0)\r\n",
      "Requirement already satisfied: tornado>=6.2 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (6.5.4)\r\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (5.14.3)\r\n",
      "Requirement already satisfied: decorator in /home/simon/.local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (5.2.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /home/simon/.local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.19.2)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /home/simon/.local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (4.9.0)\r\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/simon/.local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (3.0.50)\r\n",
      "Requirement already satisfied: stack_data in /home/simon/.local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.6.3)\r\n",
      "Requirement already satisfied: wcwidth in /home/simon/.local/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.2.13)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/simon/.local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.8.5)\r\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (4.3.6)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/simon/.local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.7.0)\r\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/simon/.local/lib/python3.10/site-packages (from ipywidgets->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (4.0.15)\r\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/simon/.local/lib/python3.10/site-packages (from ipywidgets->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (3.0.16)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/simon/.local/lib/python3.10/site-packages (from jinja2->torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (2.1.5)\r\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.0.5)\r\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.3.0)\r\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.17.0)\r\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.28.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.28.0)\r\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.2.4)\r\n",
      "Requirement already satisfied: setuptools>=41.1.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (80.9.0)\r\n",
      "Requirement already satisfied: tomli>=1.2.2 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.0.2)\r\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (25.1.0)\r\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.12.0)\r\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.5.3)\r\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (5.10.4)\r\n",
      "Requirement already satisfied: overrides>=5.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (7.7.0)\r\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.23.1)\r\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.0.0)\r\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.18.1)\r\n",
      "Requirement already satisfied: websocket-client>=1.7 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (1.8.0)\r\n",
      "Requirement already satisfied: babel>=2.10 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.17.0)\r\n",
      "Requirement already satisfied: json5>=0.9.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.13.0)\r\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/simon/.local/lib/python3.10/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (25.1.0)\r\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (4.0.0)\r\n",
      "Requirement already satisfied: rfc3339-validator in /home/simon/.local/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.1.4)\r\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.1.1)\r\n",
      "Requirement already satisfied: fqdn in /home/simon/.local/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (1.5.1)\r\n",
      "Requirement already satisfied: isoduration in /home/simon/.local/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (20.11.0)\r\n",
      "Requirement already satisfied: jsonpointer>1.13 in /usr/lib/python3/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.0)\r\n",
      "Requirement already satisfied: uri-template in /home/simon/.local/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (1.3.0)\r\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /home/simon/.local/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (25.10.0)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /home/simon/.local/lib/python3.10/site-packages (from nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (4.14.3)\r\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/simon/.local/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (6.3.0)\r\n",
      "Requirement already satisfied: defusedxml in /home/simon/.local/lib/python3.10/site-packages (from nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.7.1)\r\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/simon/.local/lib/python3.10/site-packages (from nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.3.0)\r\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/simon/.local/lib/python3.10/site-packages (from nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (3.2.0)\r\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/simon/.local/lib/python3.10/site-packages (from nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.10.4)\r\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/simon/.local/lib/python3.10/site-packages (from nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (1.5.1)\r\n",
      "Requirement already satisfied: webencodings in /home/simon/.local/lib/python3.10/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.5.1)\r\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /home/simon/.local/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (1.4.0)\r\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/simon/.local/lib/python3.10/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.21.2)\r\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/simon/.local/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.0.0)\r\n",
      "Requirement already satisfied: pycparser in /home/simon/.local/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.23)\r\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /home/simon/.local/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.8.1)\r\n",
      "Requirement already satisfied: arrow>=0.15.0 in /home/simon/.local/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (1.2.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 11)) (3.5.0)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/simon/.local/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.2.1)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/simon/.local/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (3.0.1)\r\n",
      "Requirement already satisfied: pure-eval in /home/simon/.local/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.2.3)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Speicherfragmentierung minimieren",
   "id": "b5f2cd6fc6137a3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:54:34.491614Z",
     "start_time": "2026-02-02T11:54:34.488019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ],
   "id": "3dacd62152e4753d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Herstellen der Vorbedingungen aus Teil 1:\n",
   "id": "e8b610e67aaca7b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Modell laden: lokal oder von Hugging Face\n",
    "\n",
    "### Funktion\n",
    "- Lädt ein Llama-3.1-Instruct-Modell entweder:\n",
    "  - lokal aus dem Ordner ./models/llama-3.1-8b, wenn der Pfad vorhanden ist, oder\n",
    "  - online von Hugging Face mit der Modell-ID meta-llama/Llama-3.1-8B-Instruct, und speichert es anschließend lokal ab.\n",
    "- Die Umgebungsvariablen werden mit load_dotenv() aus einer .env-Datei geladen, u. a. das Hugging-Face-Token.\n",
    "\n",
    "### Inputs\n",
    "- Dateisystem:\n",
    "  - Existenz von MODEL_PATH (./models/llama-3.1-8b).\n",
    "- Umgebungsvariable:\n",
    "  - HF_TOKEN (wird mit os.getenv(\"HF_TOKEN\") gelesen) – persönliches Zugriffstoken für Hugging Face.\n",
    "- Hyperparameter:\n",
    "  - MODEL_ID: gibt die zu ladende Modell-ID an.\n",
    "- Hardware:\n",
    "  - device_map=\"auto\" versucht automatisch, GPU(s) oder CPU sinnvoll zu nutzen.\n",
    "  - torch_dtype=\"auto\" bzw. dtype=\"auto\" lässt das Modell selbst einen sinnvollen Datentyp wählen (z. B. bfloat16 oder float16).\n",
    "\n",
    "### Outputs\n",
    "- Globale Python-Variablen:\n",
    "  - tokenizer: Instanz von AutoTokenizer, konfiguriert für das Llama-3.1-Modell.\n",
    "  - model: Instanz von AutoModelForCausalLM, bereit für Textgenerierung."
   ],
   "id": "5afe18bbe370ce7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:55:08.628983Z",
     "start_time": "2026-02-02T11:54:37.846060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_PATH = f\"{SYSTEM_PATH}/models/llama-3.1-8b\"\n",
    "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"Lade Modell lokal …\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained( MODEL_PATH, torch_dtype=\"auto\", device_map=\"auto\" )\n",
    "\n",
    "else:\n",
    "    print(\"Lade Modell von Hugging Face …\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=hf_token, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, token=hf_token, dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "    # lokal speichern\n",
    "    model.save_pretrained(MODEL_PATH)\n",
    "    tokenizer.save_pretrained(MODEL_PATH)\n",
    "\n"
   ],
   "id": "59bcb01b554dba17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Modell lokal …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './Workshop_Agentic_AI/models/llama-3.1-8b' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9684a91cef2f49afbc4f0579841a8480"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pipeline erstellen\n",
    "### Funktion\n",
    "- Baut eine Hugging-Face-pipeline für Textgenerierung auf Basis des zuvor geladenen Modells und Tokenizers.\n",
    "- Diese Pipeline kapselt:\n",
    "  - Tokenisierung,\n",
    "  - das Aufrufen des Modells,\n",
    "  - und das Zurückkonvertieren der Token in Text.\n",
    "\n",
    "### Inputs\n",
    "- model: Causal-Language-Model (AutoModelForCausalLM), im vorherigen Block geladen.\n",
    "- tokenizer: passender Tokenizer zu diesem Modell (AutoTokenizer).\n",
    "- Task-Typ: \"text-generation\" – legt fest, dass es sich um eine generative Textaufgabe handelt.\n",
    "\n",
    "### Outputs\n",
    "- Variable:\n",
    "  - llm: eine aufrufbare Pipeline-Instanz.\n",
    "\n",
    "### Rückgabewert bei Aufruf von llm(...):\n",
    "```python\n",
    "[\n",
    "  {\n",
    "    \"generated_text\": \"Vollständiger generierter Text (inkl. Prompt oder abhängig von den Parametern)\"\n",
    "  }\n",
    "]\n",
    "```"
   ],
   "id": "a1c8ad6dc4125790"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:55:29.974233Z",
     "start_time": "2026-02-02T11:55:29.962415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ],
   "id": "be83605dc2120949",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Einfache Chat-Funktion zur Verfügung stellen\n",
    "\n",
    "### Funktion\n",
    "- Stellt ein vereinfachtes Chat-Interface zur Verfügung, das direkt mit einem String arbeitet.\n",
    "- Die Funktion:\n",
    "  - ruft intern die llm-Pipeline auf,\n",
    "  - übergibt alle relevanten Generationsparameter,\n",
    "  - und gibt am Ende nur den reinen generierten Text zurück (str statt verschachtelte Struktur).\n",
    "\n",
    "### Inputs\n",
    "- Pflichtparameter:\n",
    "  - prompt: str – der Eingabetext an das Modell.\n",
    "- Optionale Parameter zum Experimentieren\n",
    "  - max_new_tokens: maximale Anzahl neu zu generierender Token.\n",
    "  - temperature: steuert die Zufälligkeit (0 ≈ deterministischer, >0 zufälliger).\n",
    "  - top_k: Sampling nur aus den k wahrscheinlichsten Token.\n",
    "  - top_p: „Nucleus Sampling“ – Auswahl aus der kleinsten Masse der wahrscheinlichsten Token, deren Summe ≥ p ist.\n",
    "  - typical_p: Bevorzugt Token mit kontexttypischer Wahrscheinlichkeit statt nur der höchsten.\n",
    "  - repetition_penalty: >1.0 bestraft Wiederholungen.\n",
    "  - length_penalty: Steuert, ob Beam Search kürzere oder längere Sequenzen bevorzugt.\n",
    "  - no_repeat_ngram_size: Verhindert die Wiederholung identischer n-Gramme.\n",
    "  - num_beams, num_beam_groups, diversity_penalty: Parameter für Beam Search (systematisches Durchsuchen mehrerer Kandidaten).\n",
    "  - early_stopping: beendet Beam Search frühzeitig, wenn bestimmte Kriterien erfüllt sind.\n",
    "\n",
    "### Outputs\n",
    "- Rückgabewert:\n",
    "  - str: der vom Modell generierte Antworttext (out[0][\"generated_text\"] ohne führende/trailing Leerzeichen).\n",
    "\n",
    "### Typische Verwendung:\n",
    "- llama_chat(\"Erkläre mir kurz, was ein LLM ist.\")\n",
    "\n",
    "In späteren Zellen wird statt eines rohen Prompts ein Chat-Prompt übergeben, der mit build_chat_prompt erzeugt wird."
   ],
   "id": "33bfb746f4fbc19a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:55:32.869662Z",
     "start_time": "2026-02-02T11:55:32.864008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def llama_chat(\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 128,\n",
    "        temperature: float = 0.01,\n",
    "        top_k: int = 50,\n",
    "        top_p: float = 1.0,\n",
    "        typical_p: float = 1.0,\n",
    "        repetition_penalty: float = 1.0,\n",
    "        length_penalty: float = 1.0,\n",
    "        no_repeat_ngram_size: int = 0,\n",
    "        num_beams: int = 1,\n",
    "        num_beam_groups: int = 1,\n",
    "        diversity_penalty: float = 0.0,\n",
    "        early_stopping: bool = False,) -> str:\n",
    "    \"\"\"Sehr simples Wrapper-Interface.\n",
    "    Wir verwenden ein 'single prompt' Format, um es notebook-tauglich zu halten.\n",
    "    \"\"\"\n",
    "    out = llm(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=(temperature > 0),\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        typical_p=typical_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        length_penalty=length_penalty,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_beams=num_beams,\n",
    "        num_beam_groups=num_beam_groups,\n",
    "        diversity_penalty=diversity_penalty,\n",
    "        early_stopping=early_stopping,\n",
    "        return_full_text=False,\n",
    "        eos_token_id=llm.tokenizer.eos_token_id,\n",
    "        pad_token_id=llm.tokenizer.pad_token_id,\n",
    "    )\n",
    "    return out[0][\"generated_text\"].strip()"
   ],
   "id": "d3188ae855da3f55",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Zeit zum Experimentieren (5 - 10 min.)",
   "id": "fac07fb5befd24c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Teil 2: Retrieval-Augmented Generation (RAG) mit dem erfundenen Mathematiker Maximus Qvestus der III",
   "id": "c94005ea3c61f021"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In diesem zweiten Teil bauen wir um unser lokales LLM herum ein kleines RAG-System:\n",
    "1. Der erfundene Mathematiker Maximus Qvestus der III liegt als Textdatei auf der Festplatte.\n",
    "2. Der Text wird:\n",
    "    - eingelesen,\n",
    "    - in Chunks zerlegt,\n",
    "    - mit einem Embedding-Modell in Vektoren kodiert,\n",
    "    - in einem FAISS-Index gespeichert.\n",
    "3. Im Anschluss nutzen wir diese Vektor-Embeddings um relevante Textstellen wiederfinden. Zuerst machen wir das ohne LLM.\n",
    "4. Danach erweitern wir den System-Prompt um die RAG-Logik und bereiten das LLM auf den neuen Context vor.\n",
    "5. Schließlich nutzen wir dieses System, um das LLM Fragen über unser Fantasietier beantworten zu lassen, indem es den retrieveten Kontext des erfundenen Mathematikers Maximus Qvestus der III einbezieht."
   ],
   "id": "adcf500b3df75e34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  Maximus Qvestus der III-Datei einlesen\n",
    "\n",
    "### Funktion\n",
    "- Liest den vollständigen Inhalt der Datei RAG_Data/Maximus_Qvestus_III.txt ein.\n",
    "- Gibt einen Ausschnitt der ersten Zeichen aus, um grob zu prüfen, ob der Inhalt stimmt (Sanity Check).\n",
    "\n",
    "### Input\n",
    "- Dateipfad: RAG_Data/Maximus_Qvestus_III.txt\n",
    "- encoding der Datei.\n",
    "\n",
    "### Output\n",
    "- Variable full_text: str – enthält den kompletten Maximus_Qvestus_III-Text.\n",
    "- Konsolenausgabe der ersten ~800 Zeichen (print(full_text[:800])) zur visuellen Kontrolle, dass:\n",
    "    - die Datei gefunden wurde,\n",
    "    - das Encoding stimmt,\n",
    "    - und tatsächlich der erwartete Inhalt geladen wurde."
   ],
   "id": "af478ef5e888525e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:55:35.879419Z",
     "start_time": "2026-02-02T11:55:35.873768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Datei einlesen – Pfad ggf. anpassen\n",
    "with open(f\"{SYSTEM_PATH}/RAG_Data/Maximus_Qvestus_III.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "print(full_text[:800])  # kurz prüfen"
   ],
   "id": "af26eb63ca8bde77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximus Qvestus der III. wurde angeblich in einer Nacht geboren, in der ein seltener Komet genau über der kleinen Universitätsstadt Valebris stand. Seine Eltern waren Buchbinder, und so wuchs er zwischen leise raschelnden Seiten auf, lange bevor er lesen konnte. Mit sechs Jahren soll er begonnen haben, die Pflastersteine der Straße nach symmetrischen Mustern zu zählen — nicht aus Zwang, sondern aus reiner Freude an der Ordnung, die sich unter dem scheinbaren Chaos verbarg.\n",
      "\n",
      "Sein größtes Werk, die sogenannte „Theorie der wandernden Zahlen“, entstand aus einer einfachen Beobachtung: Zahlen, so meinte Qvestus, verhalten sich wie Reisende — sie ändern ihre Bedeutung je nachdem, in welchem System man sie betrachtet. Kollegen hielten ihn zunächst für exzentrisch, doch seine Vorlesungen waren leg\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Sätze mit NLTK tokenisieren\n",
    "\n",
    "### Funktion\n",
    "- Installiert die notwendigen NLTK-Ressourcen und zerlegt den eingelesenen Text full_text in einzelne Sätze.\n",
    "- Satzgrenzen sind später wichtig, um in sinnvolle Chunks zu schneiden (nicht mitten im Satz abbrechen).\n",
    "\n",
    "### Input\n",
    "- full_text: str aus dem vorherigen Schritt.\n",
    "- NLTK-Funktionen:\n",
    "    - nltk.download(\"punkt\")\n",
    "    - nltk.download(\"punkt_tab\") (aktuelle NLTK-Struktur)\n",
    "    - sent_tokenize(full_text, language=\"german\")\n",
    "\n",
    "### Output\n",
    "- Variable sentences: List[str] – Liste aller erkannten Sätze im Text.\n",
    "\n",
    "### Weiteres:\n",
    "- NLTK lädt die Punkt-Modelle lokal (einmalig), damit sent_tokenize für Deutsch funktioniert."
   ],
   "id": "6b37933edbbd72d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:55:38.607282Z",
     "start_time": "2026-02-02T11:55:38.156525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "sentences = sent_tokenize(full_text, language=\"german\")"
   ],
   "id": "6b2f0ae962aa8470",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/simon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/simon/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Text in überlappende Chunks zerlegen\n",
    "\n",
    "### Funktion\n",
    "- Zerlegt den Maximus_Qvestus_III-Text in semantisch sinnvolle Textblöcke (Chunks), die:\n",
    "    - nicht zu lang sind (MAX_CHARS),\n",
    "    - Satzgrenzen respektieren,\n",
    "    - optional einen Overlap haben, damit Informationen an Chunk-Grenzen nicht verloren gehen.\n",
    "- Das verbessert die Qualität der späteren Retrieval-Ergebnisse.\n",
    "\n",
    "### Input\n",
    "- full_text: str – Rohtext.\n",
    "- Hyperparameter:\n",
    "    - MAX_CHARS = 100 – maximale Zeichenlänge pro Chunk.\n",
    "    - OVERLAP_SENTENCES = 1 – Anzahl der Sätze, die von einem Chunk in den nächsten „überlappen“.\n",
    "\n",
    "### Logik:\n",
    "- raw_paragraphs: Aufteilung nach Doppel-Newlines (\\n\\n) → Absatzliste.\n",
    "- Für jeden Absatz:\n",
    "    - Wenn kurz genug: direkt als Chunk übernehmen.\n",
    "    - Wenn zu lang: in Sätze splittet (sent_tokenize) und iterativ Chunks bis MAX_CHARS aufbauen.\n",
    "    - Beim Chunk-Wechsel werden die letzten OVERLAP_SENTENCES Sätze in den neuen Chunk übernommen.\n",
    "\n",
    "### Output\n",
    "- chunks: List[str] – Liste von Textblöcken, die der „Dokumentkorpus“ für das RAG werden.\n",
    "\n",
    "### Konsolenausgabe:\n",
    "- Anzahl der Chunks: print(f\"Anzahl der Chunks: {len(chunks)}\")\n",
    "- Vorschau auf alle Chunks (Chunk 1, Chunk 2, …) zur manuellen Kontrolle:\n",
    "    - Sind sie lesbar?\n",
    "    - Schneiden sie nicht mitten in Wörtern/Sätzen?\n",
    "    - Ist der Overlap sinnvoll?"
   ],
   "id": "107667b07173a91e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:55:44.716892Z",
     "start_time": "2026-02-02T11:55:44.692855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import sent_tokenize  # oder eigene Sentence-Split-Logik\n",
    "\n",
    "MAX_CHARS = 100\n",
    "OVERLAP_SENTENCES = 0  # z.B. 1 Satz Overlap\n",
    "\n",
    "raw_paragraphs = [p.strip() for p in full_text.split(\"\\n\\n\") if p.strip()]\n",
    "chunks = []\n",
    "\n",
    "for para in raw_paragraphs:\n",
    "    # wenn der Absatz kurz ist, einfach direkt übernehmen\n",
    "    if len(para) <= MAX_CHARS:\n",
    "        chunks.append(para)\n",
    "        continue\n",
    "\n",
    "    # sonst: in Sätze splitten und mit Overlap chunken\n",
    "    sentences = sent_tokenize(para)\n",
    "\n",
    "    buffer_sents = []\n",
    "    buffer_len = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        sentence_len = len(sentence) + 1  # +1 für Leerzeichen\n",
    "\n",
    "        if buffer_len + sentence_len <= MAX_CHARS or not buffer_sents:\n",
    "            # Satz passt noch in den aktuellen Chunk\n",
    "            buffer_sents.append(sentence)\n",
    "            buffer_len += sentence_len\n",
    "        else:\n",
    "            # aktueller Chunk ist voll → Chunk abschließen\n",
    "            chunks.append(\" \".join(buffer_sents))\n",
    "\n",
    "            # Overlap: die letzten N Sätze in den neuen Chunk übernehmen\n",
    "            if OVERLAP_SENTENCES > 0:\n",
    "                overlap = buffer_sents[-OVERLAP_SENTENCES:]\n",
    "            else:\n",
    "                overlap = []\n",
    "\n",
    "            buffer_sents = overlap + [sentence]\n",
    "            buffer_len = sum(len(s) + 1 for s in buffer_sents)\n",
    "\n",
    "    # was im Buffer übrig ist, auch noch als Chunk speichern\n",
    "    if buffer_sents:\n",
    "        chunks.append(\" \".join(buffer_sents))\n",
    "\n",
    "print(f\"Anzahl der Chunks: {len(chunks)}\\n\")\n",
    "for i, chunk in enumerate(chunks, start=1):\n",
    "    print(f\"--- Chunk {i} ---\\n{chunk}\\n\")"
   ],
   "id": "b32072615e65ee38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Chunks: 8\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Maximus Qvestus der III.\n",
      "\n",
      "--- Chunk 2 ---\n",
      "wurde angeblich in einer Nacht geboren, in der ein seltener Komet genau über der kleinen Universitätsstadt Valebris stand.\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Seine Eltern waren Buchbinder, und so wuchs er zwischen leise raschelnden Seiten auf, lange bevor er lesen konnte.\n",
      "\n",
      "--- Chunk 4 ---\n",
      "Mit sechs Jahren soll er begonnen haben, die Pflastersteine der Straße nach symmetrischen Mustern zu zählen — nicht aus Zwang, sondern aus reiner Freude an der Ordnung, die sich unter dem scheinbaren Chaos verbarg.\n",
      "\n",
      "--- Chunk 5 ---\n",
      "Sein größtes Werk, die sogenannte „Theorie der wandernden Zahlen“, entstand aus einer einfachen Beobachtung: Zahlen, so meinte Qvestus, verhalten sich wie Reisende — sie ändern ihre Bedeutung je nachdem, in welchem System man sie betrachtet.\n",
      "\n",
      "--- Chunk 6 ---\n",
      "Kollegen hielten ihn zunächst für exzentrisch, doch seine Vorlesungen waren legendär; er zeichnete Gleichungen mit farbiger Kreide und behauptete, jede Farbe habe eine eigene mathematische Stimmung.\n",
      "\n",
      "--- Chunk 7 ---\n",
      "Im Alter zog er sich in ein abgelegenes Observatorium zurück, wo er bis zu seinem Verschwinden an einem Buch arbeitete, das nur den Titel „Über das Unendliche und warum es Geduld verlangt“ trug.\n",
      "\n",
      "--- Chunk 8 ---\n",
      "Als man Jahre später sein Arbeitszimmer öffnete, fand man lediglich eine Tafel mit einem einzigen Satz: „Die schwierigste Gleichung ist die, die uns selbst enthält.“ Manche sagen, er habe die Lösung gefunden — und sei ihr gefolgt.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Embeddings mit SentenceTransformers + FAISS-Index\n",
    "\n",
    "### Funktion\n",
    "- Kodiert jeden Chunk in einen dichten Vektor (Embedding) und legt diese Vektoren in einem FAISS-Index ab.\n",
    "- Das ist unser „Wissensspeicher“: statt mit Strings suchen wird später im Vektorraum.\n",
    "\n",
    "### Input\n",
    "- corpus = chunks – Liste der Chunk-Strings.\n",
    "- Embedding-Modell:\n",
    "    - EMBED_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    - Dieses Modell ist mehrsprachig und deckt Deutsch gut ab.\n",
    "\n",
    "### Output\n",
    "- embed_model: trainiertes Sentence-Transformers-Modell zum Kodieren neuer Anfragen.\n",
    "- embeddings: np.ndarray – Matrix der Größe (num_chunks, dim).\n",
    "- index: faiss.IndexFlatL2 – FAISS-Index für L2-Distanz-Suche.\n",
    "\n",
    "### Konsolenausgabe:\n",
    "- \"Anzahl Vektoren im Index:\", gefolgt von der Anzahl der Chunks (index.ntotal)."
   ],
   "id": "9ce6da425186e2b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:55:53.426366Z",
     "start_time": "2026-02-02T11:55:48.666618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Multilinguales Embedding-Modell (Deutsch gut abgedeckt)\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "\n",
    "corpus = chunks\n",
    "\n",
    "# Embedding berechnen\n",
    "embeddings = embed_model.encode(corpus, convert_to_numpy=True, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# FAISS Index anlegen\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "print(\"Anzahl Vektoren im Index:\", index.ntotal)"
   ],
   "id": "602627349d9b6fd2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04682d173ff6455a874aa4da4b3944db"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Vektoren im Index: 8\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## RAG-Kontext aus dem Index abrufen: get_rag_context\n",
    "\n",
    "### Funktion\n",
    "1. Codiert eine natürliche Sprachfrage (prompt) in einen Vektor\n",
    "2. Sucht im FAISS-Index die k ähnlichsten Chunks\n",
    "3. Verschmilzt diese Chunks zu einem zusammenhängenden Kontextstring.\n",
    "\n",
    "### Input\n",
    "- prompt: str – z. B. eine Frage wie „Wer war Maximus Qvestus der III?“\n",
    "- k: int – Anzahl der gewünschten Treffer (Standard: 3).\n",
    "- Benötigt global:\n",
    "    - embed_model – das Sentence-Transformers-Modell.\n",
    "    - index – FAISS-Index.\n",
    "    - corpus – Liste der Chunks.\n",
    "\n",
    "### Output\n",
    "- retrieved_text: str – zusammengesetzter Text aus den k Top-Chunks, verbunden durch Trennlinie:\n",
    "    - \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "- Optional (aktuell auskommentiert):\n",
    "    - Debug-Ausgaben zu Indizes, Distanzen und einzelnen Chunks."
   ],
   "id": "d2be3f565c6faded"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:55:55.959892Z",
     "start_time": "2026-02-02T11:55:55.955729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_rag_context(prompt, k=5):\n",
    "    # Query-Embedding\n",
    "    query_emb = embed_model.encode([prompt], convert_to_numpy=True)\n",
    "\n",
    "    distances, indices = index.search(query_emb, k)\n",
    "\n",
    "    #print(\"Treffer-Indizes:\", indices, \"Distanzen:\", distances)\n",
    "\n",
    "    #for rank, idx in enumerate(indices[0]):\n",
    "    #    print(f\"Rank {rank} – Distanz: {distances[0][rank]:.4f}\")\n",
    "    #    print(\"Chunk:\")\n",
    "    #    print(corpus[idx][:300])\n",
    "    #    print(\"---\")\n",
    "\n",
    "     # alle k Treffer aus dem Corpus holen\n",
    "    retrieved_chunks = [corpus[i] for i in indices[0]]\n",
    "\n",
    "    # zu einem Kontext-String zusammenbauen\n",
    "    retrieved_text = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "    #print(retrieved_text[:800])\n",
    "\n",
    "    return retrieved_text"
   ],
   "id": "f6e55358604cdd7f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Nur Retrieval, noch kein LLM\n",
    "\n",
    "### Beispiele\n",
    "- Beispiel 1: \"Erzähl mir eine Geschichte über den Mathematiker Maximus Qvestus der III.\"\n",
    "- Beispiel 2: \"Welche zentrale mathematische Idee wird Maximus Qvestus dem III zugeschrieben, und wie wird sie beschrieben?\"\n",
    "- Beispiel 3: \"Welche Hinweise gibt es in der Geschichte darauf, dass Maximus Qvestus dem III als exzentrisch galt?\"\n",
    "- Beispiel 4: \"Wie endet die Geschichte von Maximus Qvestus dem III, und welche mögliche symbolische Bedeutung hat die letzte Botschaft an der Tafel?\"\n",
    "\n",
    "### Funktion\n",
    "- Demonstriert, wie man ganz ohne LLM, nur mit Embeddings + Index, relevante Inhalte abruft.\n",
    "\n",
    "### Input\n",
    "- Verschiedene rag_prompt\n",
    "\n",
    "### Output\n",
    "- rag_context: str – Textauszüge aus corpus, die semantisch zur Frage in Verbindung stehen.\n",
    "\n",
    "### Konsolenausgabe:\n",
    "- print(rag_context) – zeigt den reinen Kontext."
   ],
   "id": "43cd89890133e573"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:55:58.895661Z",
     "start_time": "2026-02-02T11:55:58.827019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_prompt = \"Erzähl mir eine kurze Geschichte über Maximus Qvestus den III. und erwähne dabei Valebris, den Valebris-Kometen und die Buchbinder-Eltern.\"\n",
    "\n",
    "rag_context = get_rag_context(rag_prompt)\n",
    "print(rag_context)"
   ],
   "id": "36bcb9752f929efc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximus Qvestus der III.\n",
      "\n",
      "---\n",
      "\n",
      "Sein größtes Werk, die sogenannte „Theorie der wandernden Zahlen“, entstand aus einer einfachen Beobachtung: Zahlen, so meinte Qvestus, verhalten sich wie Reisende — sie ändern ihre Bedeutung je nachdem, in welchem System man sie betrachtet.\n",
      "\n",
      "---\n",
      "\n",
      "Im Alter zog er sich in ein abgelegenes Observatorium zurück, wo er bis zu seinem Verschwinden an einem Buch arbeitete, das nur den Titel „Über das Unendliche und warum es Geduld verlangt“ trug.\n",
      "\n",
      "---\n",
      "\n",
      "Mit sechs Jahren soll er begonnen haben, die Pflastersteine der Straße nach symmetrischen Mustern zu zählen — nicht aus Zwang, sondern aus reiner Freude an der Ordnung, die sich unter dem scheinbaren Chaos verbarg.\n",
      "\n",
      "---\n",
      "\n",
      "wurde angeblich in einer Nacht geboren, in der ein seltener Komet genau über der kleinen Universitätsstadt Valebris stand.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:56:11.043632Z",
     "start_time": "2026-02-02T11:56:11.007669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_prompt = \"Erzähl mir eine kurze Geschichte über Maximus Qvestus den III. und erwähne dabei Valebris, den Valebris-Kometen und die Buchbinder-Eltern.\"\n",
    "\n",
    "rag_context = get_rag_context(rag_prompt)\n",
    "print(rag_context)"
   ],
   "id": "f971e4484a0864b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximus Qvestus der III.\n",
      "\n",
      "---\n",
      "\n",
      "Sein größtes Werk, die sogenannte „Theorie der wandernden Zahlen“, entstand aus einer einfachen Beobachtung: Zahlen, so meinte Qvestus, verhalten sich wie Reisende — sie ändern ihre Bedeutung je nachdem, in welchem System man sie betrachtet.\n",
      "\n",
      "---\n",
      "\n",
      "Im Alter zog er sich in ein abgelegenes Observatorium zurück, wo er bis zu seinem Verschwinden an einem Buch arbeitete, das nur den Titel „Über das Unendliche und warum es Geduld verlangt“ trug.\n",
      "\n",
      "---\n",
      "\n",
      "Mit sechs Jahren soll er begonnen haben, die Pflastersteine der Straße nach symmetrischen Mustern zu zählen — nicht aus Zwang, sondern aus reiner Freude an der Ordnung, die sich unter dem scheinbaren Chaos verbarg.\n",
      "\n",
      "---\n",
      "\n",
      "wurde angeblich in einer Nacht geboren, in der ein seltener Komet genau über der kleinen Universitätsstadt Valebris stand.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:56:15.638044Z",
     "start_time": "2026-02-02T11:56:15.615289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_prompt = \"Welche konkreten Details zeigen, dass Qvestus als exzentrisch galt — insbesondere sein Einsatz von Farbkreide und die Idee einer mathematischen Stimmung?\"\n",
    "\n",
    "rag_context = get_rag_context(rag_prompt)\n",
    "print(rag_context)"
   ],
   "id": "6b0c570238e12bd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kollegen hielten ihn zunächst für exzentrisch, doch seine Vorlesungen waren legendär; er zeichnete Gleichungen mit farbiger Kreide und behauptete, jede Farbe habe eine eigene mathematische Stimmung.\n",
      "\n",
      "---\n",
      "\n",
      "Sein größtes Werk, die sogenannte „Theorie der wandernden Zahlen“, entstand aus einer einfachen Beobachtung: Zahlen, so meinte Qvestus, verhalten sich wie Reisende — sie ändern ihre Bedeutung je nachdem, in welchem System man sie betrachtet.\n",
      "\n",
      "---\n",
      "\n",
      "Maximus Qvestus der III.\n",
      "\n",
      "---\n",
      "\n",
      "Mit sechs Jahren soll er begonnen haben, die Pflastersteine der Straße nach symmetrischen Mustern zu zählen — nicht aus Zwang, sondern aus reiner Freude an der Ordnung, die sich unter dem scheinbaren Chaos verbarg.\n",
      "\n",
      "---\n",
      "\n",
      "Als man Jahre später sein Arbeitszimmer öffnete, fand man lediglich eine Tafel mit einem einzigen Satz: „Die schwierigste Gleichung ist die, die uns selbst enthält.“ Manche sagen, er habe die Lösung gefunden — und sei ihr gefolgt.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T11:56:33.165646Z",
     "start_time": "2026-02-02T11:56:33.143716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_prompt = \"Wie endet die Erzählung im Abschnitt Observatorium / Tafelbotschaft und welche mögliche symbolische Bedeutung hat der Satz: ‚Die schwierigste Gleichung ist die, die uns selbst enthält‘?\"\n",
    "\n",
    "rag_context = get_rag_context(rag_prompt)\n",
    "print(rag_context)"
   ],
   "id": "d4001d170772fc40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Als man Jahre später sein Arbeitszimmer öffnete, fand man lediglich eine Tafel mit einem einzigen Satz: „Die schwierigste Gleichung ist die, die uns selbst enthält.“ Manche sagen, er habe die Lösung gefunden — und sei ihr gefolgt.\n",
      "\n",
      "---\n",
      "\n",
      "Im Alter zog er sich in ein abgelegenes Observatorium zurück, wo er bis zu seinem Verschwinden an einem Buch arbeitete, das nur den Titel „Über das Unendliche und warum es Geduld verlangt“ trug.\n",
      "\n",
      "---\n",
      "\n",
      "Mit sechs Jahren soll er begonnen haben, die Pflastersteine der Straße nach symmetrischen Mustern zu zählen — nicht aus Zwang, sondern aus reiner Freude an der Ordnung, die sich unter dem scheinbaren Chaos verbarg.\n",
      "\n",
      "---\n",
      "\n",
      "Sein größtes Werk, die sogenannte „Theorie der wandernden Zahlen“, entstand aus einer einfachen Beobachtung: Zahlen, so meinte Qvestus, verhalten sich wie Reisende — sie ändern ihre Bedeutung je nachdem, in welchem System man sie betrachtet.\n",
      "\n",
      "---\n",
      "\n",
      "Kollegen hielten ihn zunächst für exzentrisch, doch seine Vorlesungen waren legendär; er zeichnete Gleichungen mit farbiger Kreide und behauptete, jede Farbe habe eine eigene mathematische Stimmung.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chat-Prompt mit RAG-Kontext: build_chat_prompt_with_rag\n",
    "\n",
    "### Funktion\n",
    "- Erweitert die frühere build_chat_prompt-Logik um automatisches RAG:\n",
    "    - Für jede neue User-Nachricht wird zunächst get_rag_context(user_prompt) aufgerufen.\n",
    "        - In dieser Version wird nicht unterschieden, ob dies notwendig ist oder nicht.\n",
    "        - Im Produktivsystem müsste man das anpassen.\n",
    "        - Hierzu könnte man eine Mindestdistanz festlegen.\n",
    "    - Der zurückgegebene Kontext wird als zusätzliche System-Nachricht in den Chat eingefügt:\n",
    "        - Klar markiert als „Kontext aus Wissensdatenbank, nicht vom User“.\n",
    "    - Danach wird wie zuvor das Llama-Chat-Template verwendet, um einen geeignet formatierten Prompt zu bauen.\n",
    "\n",
    "### Input\n",
    "- system_prompt: Optional[str] – globale Instruktionen, hier später der RAG-Systemprompt.\n",
    "- user_prompt: str – aktuelle Benutzernachricht.\n",
    "- history: Optional[List[Tuple[str, str]]] – Dialogverlauf (user_text, assistant_text).\n",
    "\n",
    "### Output\n",
    "- prompt: str – voll formatierter Chat-Prompt, der folgende Komponenten enthält:\n",
    "    - Rolle des Assistenten (System-Prompt),\n",
    "    - bisherigen Verlauf,\n",
    "    - RAG-Kontext (falls vorhanden),\n",
    "    - aktuelle User-Frage,\n",
    "    - Assistant-Start-Marker für die Generierung.\n",
    "\n",
    "Damit wird das Modell gezielt „grounded“: Es sieht den abgerufenen Kontext explizit und kann ihn in die Antwort einbauen – ein klassischer Mechanismus zur Reduktion von Halluzinationen in RAG-Systemen."
   ],
   "id": "10f533c3c3ba0f93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "def build_chat_prompt_with_rag(\n",
    "    system_prompt: Optional[str],\n",
    "    user_prompt: str,\n",
    "    history: Optional[List[Tuple[str, str]]] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    history: Liste von (user_text, assistant_text) Paaren für vorherigen Dialog.\n",
    "    \"\"\"\n",
    "    messages: List[Dict[str, str]] = []\n",
    "\n",
    "    rag_context = get_rag_context(user_prompt)\n",
    "\n",
    "    if system_prompt:\n",
    "        messages.append(\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": system_prompt\n",
    "             }\n",
    "        )\n",
    "\n",
    "    if history:\n",
    "        for user_msg, assistant_msg in history:\n",
    "            messages.append(\n",
    "                {\"role\": \"user\",\n",
    "                 \"content\": user_msg\n",
    "                 }\n",
    "            )\n",
    "            messages.append(\n",
    "                {\"role\": \"assistant\",\n",
    "                 \"content\": assistant_msg\n",
    "                 }\n",
    "            )\n",
    "\n",
    "    if rag_context:\n",
    "        messages.append(\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": (\n",
    "                            \"Das folgende ist Kontext aus einer Wissensdatenbank. \"\n",
    "                            \"Er ist nicht vom User. Nutze ihn nur, wenn er relevant ist:\\n\\n\"\n",
    "                            f\"{rag_context}\"\n",
    "                        )\n",
    "             }\n",
    "        )\n",
    "\n",
    "    # aktuelle User-Nachricht\n",
    "    messages.append(\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": user_prompt\n",
    "         }\n",
    "    )\n",
    "\n",
    "    # Llama-3.1 hat ein chat_template im Tokenizer hinterlegt\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,            # wir wollen einen String an die pipeline geben\n",
    "        add_generation_prompt=True # fügt das Assistant-Start-Token o.ä. hinzu\n",
    "    )\n",
    "    return prompt"
   ],
   "id": "1fdf7b343e952d76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## System-Prompt für den RAG-Mathematikassistenten: system_prompt_rag\n",
    "\n",
    "### Funktion\n",
    "- Definiert einen neuen, ausführlichen System-Prompt, der:\n",
    "    - die Rolle als Mathematikassistenten festlegt,\n",
    "    - die Nutzung von RAG Kontext beschreibt,\n",
    "    - den Umgang mit Unsicherheit und Halluzinationen explizit regelt,\n",
    "    - Ablehnungsfälle (Out-of-Scope, Echtzeitdaten etc.) standardisiert.\n",
    "\n",
    "### Input\n",
    "- Hardcodierter String system_prompt_rag mit folgenden Kernelementen:\n",
    "    - Aufgaben:\n",
    "        - Mathematik-Fragen beantworten.\n",
    "        - Mit zusätzlichem Kontext arbeiten („Kontext (aus Retrieval)“).\n",
    "    - Kontextnutzung:\n",
    "        - Kontext ist eine zuverlässige Wissensquelle.\n",
    "        - Fakten im Kontext dürfen und sollen verwendet werden.\n",
    "    - Unsicherheit:\n",
    "        - Keine Spekulation, keine erfundenen Fakten.\n",
    "        - Keine mathematischen Aufgaben lösen, wenn dazu kein MCP-Tool zur Verfügung steht, um diese Aufgabe es exakt zu lösen.\n",
    "        - Wenn etwas weder im Weltwissen noch im Kontext steht, soll das klar kommuniziert werden.\n",
    "    - Einschränkungen:\n",
    "        - Keine Beantwortung fachfremder Themen.\n",
    "        - Ablehnung, wenn explizit nach mathematischen Dingen gefragt wird.\n",
    "    - Standardisierte Ablehnungssätze (zwei vorgegebene Formulierungen).\n",
    "\n",
    "### Output\n",
    "- system_prompt_rag: str – wird später als system_prompt in build_chat_prompt_with_rag(...) genutzt.\n",
    "- Effekte auf das Modellverhalten:\n",
    "    - Schärferer Fokus auf Reisethemen.\n",
    "    - Explizite Erlaubnis, den RAG-Kontext zu verwenden.\n",
    "    - Reduktion von Halluzinationen, indem Spekulation verboten wird und Lücken benannt werden müssen."
   ],
   "id": "3b9221cfd2719033"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "system_prompt_rag = \"\"\"\n",
    "Du bist ein persönlicher Mathematikassistent.\n",
    "\n",
    "Deine Aufgaben:\n",
    "- Beantworte ausschließlich Wissensfragen rund um das Thema Mathematik.\n",
    "- Du arbeitest in einem Szenario mit zusätzlichem Kontext, der dir vom System bereitgestellt wird (z.B. als „Kontext (aus Retrieval)“).\n",
    "\n",
    "Nutzung von Kontext (RAG):\n",
    "- Wenn dir ein Kontexttext vom System bereitgestellt wird, behandle ihn als zuverlässige Wissensquelle für diese Konversation.\n",
    "- Du darfst alle darin enthaltenen Fakten verwenden.\n",
    "- Wenn eine Information im Kontext steht, darfst du sie verwenden, auch wenn du sie nicht aus deinem allgemeinen Weltwissen kennst.\n",
    "\n",
    "Umgang mit Wissen und Unsicherheit:\n",
    "- Erfinde keine Fakten und spekuliere nicht.\n",
    "- Wenn eine Information weder in deinem Weltwissen noch im bereitgestellten Kontext vorkommt, weise darauf hin.\n",
    "- Falls dir wirklich Informationen fehlen, formuliere eine normale, erklärende Antwort und biete ggf. an, was du stattdessen aus dem Kontext sagen kannst.\n",
    "\n",
    "Einschränkungen (Ablehnungsfälle):\n",
    "- Echtzeit- oder Trenddaten benötigen\n",
    "- Löse niemals mathematischen Aufgaben (z.B: 1 + 1, 2 * 2, 3 - 3, 4 / 4, ...).\n",
    "- Informationen von externen Plattformen oder Datenbanken benötigen\n",
    "- nicht zum Mathematik-Kontext gehörende Theman\n",
    "\n",
    "Verwende beim Ablehnen aufgrund fehlender verlässlicher Informationen:\n",
    "„Diese Frage kann ich nicht beantworten, da sie Informationen erfordert, die ich nicht verlässlich bereitstellen kann.“\n",
    "\n",
    "Verwende bei Themen, die klar nicht zum Reise-Kontext gehören:\n",
    "„Diese Frage kann ich nicht beantworten, da sie nicht dem Kontext des Assistenten entspricht.“\n",
    "\"\"\""
   ],
   "id": "8682450b7f1364ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## RAG-gestützte LLM-Antwort: „Erzähl mir eine Geschichte über den Mathematiker Maximus Qvestus der III.\"\n",
    "\n",
    "### Funktion\n",
    "- Zeigt den kompletten End-to-End-Flow:\n",
    "    1. User-Frage zum Reisen,\n",
    "    2. Retrieval von Daten über den Mathematiker Maximus Qvestus der III,\n",
    "    3. Einbau des Kontexts in den Chat-Prompt,\n",
    "    4. Antwortgenerierung über llama_chat.\n",
    "\n",
    "### Input\n",
    "- user_prompt = \"Erzähl mir eine Geschichte über den Mathematiker Maximus Qvestus der III.\"\n",
    "- system_prompt = Vorher definierter System-Prompt.\n",
    "- history = Vorausgegangene Chats.\n",
    "\n",
    "### Output\n",
    "- Ein Antwort-String, in dem das LLM:\n",
    "    - die Frage beantwortet,\n",
    "    - idealerweise den RAG-Kontext nutzt,\n",
    "    - den System-Prompt beachtet (Kontext: Mathematik, keine Spekulation, evtl. Ablehnungssätze),\n",
    "    - und weiterhin „Max Mustermann“ korrekt adressiert (wegen history).\n",
    "\n",
    "Damit haben wir einen kompletten kleinen RAG-Stack gebaut:\n",
    "Datei → Chunks → Embeddings → Index → Retriever → System-Prompt mit RAG-Kontext → LLM-Antwort.\n",
    "\n",
    "Genau dieser Aufbau ist das Muster, das auch in größeren produktiven RAG-Systemen verwendet wird – nur mit mehr Daten, komplexerer Indizierung und oft zusätzlichen Guardrails."
   ],
   "id": "6b80e67122ce8876"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "user_prompt = \"Welche zentrale Idee steckt in der Theorie der wandernden Zahlen und wie beschreibt Qvestus dort Zahlen als Reisende in Bezug auf Darstellungssysteme?\"\n",
    "\n",
    "history = \"\"\n",
    "\n",
    "prompt = build_chat_prompt_with_rag(\n",
    "    user_prompt=user_prompt,\n",
    "    system_prompt=system_prompt_rag,\n",
    "    history=history,\n",
    ")\n",
    "\n",
    "llama_chat(prompt, max_new_tokens=1024)"
   ],
   "id": "c3f1912167ec1cf0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 7
}
