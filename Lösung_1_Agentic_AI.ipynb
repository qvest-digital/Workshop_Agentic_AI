{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Huggingface Token setzen",
   "id": "6c0d1bf0de6f708c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T09:00:41.869293Z",
     "start_time": "2026-01-29T09:00:41.863027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "hf_token = \"hf_Token\"\n",
    "#hf_token = os.getenv(\"HF_TOKEN\")"
   ],
   "id": "fa039889dd7da709",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Git Repo per HTTPs Clonen",
   "id": "cc11cc3d70965f1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!git clone https://github.com/qvest-digital/Workshop_Agentic_AI.git",
   "id": "6333251b620ba25e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pfad setzen",
   "id": "b398c4d5562a7858"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T09:00:43.300996Z",
     "start_time": "2026-01-29T09:00:43.297042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#SYSTEM_PATH = \"/home/simon/Workshop_Agentic_AI\"\n",
    "SYSTEM_PATH = \"./Workshop_Agentic_AI\""
   ],
   "id": "95c9b36fee07fd1b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Requirements installieren",
   "id": "78aa612f2a446737"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install -r \"$SYSTEM_PATH/requirements.txt\"",
   "id": "88d6e57ee340b5d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Speicherfragmentierung minimieren",
   "id": "b5f2cd6fc6137a3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T09:00:45.727193Z",
     "start_time": "2026-01-29T09:00:45.724053Z"
    }
   },
   "cell_type": "code",
   "source": "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"",
   "id": "3dacd62152e4753d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Workshop Agentic AI\n",
    "\n",
    "Dieses Notebook zeigt Schritt für Schritt, wie ein agentisches KI-System aufgebaut wird, das RAG, externe Tool-Abfragen und LLM-Reasoning kombiniert. Ziel ist es, Halluzinationen zu reduzieren, Fakten deterministisch zu beziehen und mehrere spezialisierte Sub-Modelle zu orchestrieren.\n",
    "\n",
    "## Ablauf\n",
    "- Teil 1: Lokales LLM + Halluzinationen im Griff behalten\n",
    "    - Lokales LLM laden (HF-Pipeline)\n",
    "    - Sampling-Verhalten & Halluzinationen untersuchen\n",
    "    - Textgenerierung konfigurieren (Determinismus vs. Kreativität)\n",
    "- Teil 2: Retrieval-Augmented Generation (RAG) mit dem Fantasietier Razepato\n",
    "    - Texte als Embeddings in eine Vektordatenbank schreiben und vergleichen\n",
    "    - Retrieval + Query + Antwortgenerierung\n",
    "    - Was RAG im kontext LLM leistet\n",
    "    - RAG und LLM verbinden\n",
    "- Teil 3: Erlangen von Wissen durch Nutzung von MCP-Tools\n",
    "    - MCP-Tools anbinden (manuell)\n",
    "    - Fakten über MCP-Aufrufe beziehen (z. B. Koordinaten, Wetter, Flächen)\n",
    "- Teil 4: Voll agentisches MCP – der Mathematikassistent denkt und handelt selbst\n",
    "    - Planen & Reasoning (Chain-of-Thought, ReAct)\n",
    "    - Sub-LLMs orchestrieren (Planner → Validator → Executor → Auditoren → Renderer)"
   ],
   "id": "a07fe97987f77d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Timetable\n",
    "\n",
    "| Zeit        | Thema                          | Inhalt / Ziel                                                |\n",
    "|-------------|--------------------------------|--------------------------------------------------------------|\n",
    "| 09:00–09:30 | Einchecken & Agenda vorstellen | Tagesübersicht                                               |\n",
    "| 09:30–10:00 | LLM + Parameter                | HF-Setup, Tuning-Parameter, Halluzinationen => (Experimente) |\n",
    "| 10:00–10:30 | Prompting (Basisprompt)        | Prompt-Engeneering, Experimente, Diskussion => (Experimente) |\n",
    "| 10:30–12:00 | RAG                            | Embeddings, Vektorsuche, Retrieval, Integration              |\n",
    "| 12:00–13:00 | Mittag                         | Pause                                                        |\n",
    "| 13:00–13:20 | MCP-Basics                     | Konzept, Server, Call-Mechanik                               |\n",
    "| 13:20–14:00 | MCP-Tool-Use                   | Fakten (Koordinaten, Wetter, Locations) über Tools           |\n",
    "| 14:00–15:30 | MCP-Beispiel (manuel)          | Grundfuntkionen von MCP verstehen                            |\n",
    "| 15:30–16:00 | Übergang zu Agenten            | Wie aus RAG + MCP → agentisches System wird                  |\n",
    "| 16:00–16:30 | Code-Walkthrough               | Pipeline-Durchgang, Q&A                                      |\n",
    "| 16:30       | Ende                           | Abschluss                                                    |\n",
    "\n",
    "Metakommentar:\n",
    "- vor jedem Block Reflexionsfragen zu Verknüpfung & Möglichkeiten.\n",
    "- nach jedem Block Verbesserungsideen."
   ],
   "id": "921cca58fd11c2ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Teil 1: Lokales LLM + Halluzinationen im Griff behalten\n",
    "\n",
    "In diesem Notebook schauen wir uns Schritt für Schritt an, wie man ein lokales Large Language Model (LLM) mit der `transformers`-Bibliothek von Hugging Face lädt und über eine einfache Chat-Funktion benutzt.\n",
    "\n",
    "Dabei interessieren uns zwei Dinge:\n",
    "\n",
    "1. **Technik:**\n",
    "   - Wie lade ich ein Modell lokal von der Festplatte oder – falls es noch nicht vorliegt – automatisch von Hugging Face?\n",
    "   - Wie baue ich eine einfache `pipeline`, mit der ich Texte generieren kann?\n",
    "\n",
    "2. **Verhalten des Modells:**\n",
    "   - Wie sieht es aus, wenn das Modell „normal“ (ohne Kontexteinschränkungen) antwortet – also inkl. Halluzinationen?\n",
    "   - Wie kann ich mit **System-Prompts** und einer **Chat-Vorlage** (chat template) das Verhalten des Modells einschränken, z. B.:\n",
    "     - nur Mathematikassistent-Themen beantworten,\n",
    "     - keine Fakten erfinden,\n",
    "     - bestimmte Anfragen explizit ablehnen?\n"
   ],
   "id": "e8b610e67aaca7b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Modell laden: lokal oder von Hugging Face\n",
    "\n",
    "### Funktion\n",
    "- Lädt ein Llama-3.1-Instruct-Modell entweder:\n",
    "  - lokal aus dem Ordner ./models/llama-3.1-8b, wenn der Pfad vorhanden ist, oder\n",
    "  - online von Hugging Face mit der Modell-ID meta-llama/Llama-3.1-8B-Instruct, und speichert es anschließend lokal ab.\n",
    "- Die Umgebungsvariablen werden mit load_dotenv() aus einer .env-Datei geladen, u. a. das Hugging-Face-Token.\n",
    "\n",
    "### Inputs\n",
    "- Dateisystem:\n",
    "  - Existenz von MODEL_PATH (./models/llama-3.1-8b).\n",
    "- Umgebungsvariable:\n",
    "  - HF_TOKEN (wird mit os.getenv(\"HF_TOKEN\") gelesen) – persönliches Zugriffstoken für Hugging Face.\n",
    "- Hyperparameter:\n",
    "  - MODEL_ID gibt die zu ladende Modell-ID an.\n",
    "- Hardware:\n",
    "  - device_map=\"auto\" versucht automatisch, GPU(s) oder CPU sinnvoll zu nutzen.\n",
    "  - torch_dtype=\"auto\" bzw. dtype=\"auto\" lässt das Modell selbst einen sinnvollen Datentyp wählen (z. B. bfloat16 oder float16).\n",
    "\n",
    "### Outputs\n",
    "- Globale Python-Variablen:\n",
    "  - tokenizer: Instanz von AutoTokenizer, konfiguriert für das Llama-3.1-Modell.\n",
    "  - model: Instanz von AutoModelForCausalLM, bereit für Textgenerierung."
   ],
   "id": "5afe18bbe370ce7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T09:01:04.389017Z",
     "start_time": "2026-01-29T09:00:50.085910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_PATH = f\"{SYSTEM_PATH}/models/llama-3.1-8b\"\n",
    "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"Lade Modell lokal …\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained( MODEL_PATH, torch_dtype=\"auto\", device_map=\"auto\" )\n",
    "\n",
    "else:\n",
    "    print(\"Lade Modell von Hugging Face …\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=hf_token, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, token=hf_token, dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "    # lokal speichern\n",
    "    model.save_pretrained(MODEL_PATH)\n",
    "    tokenizer.save_pretrained(MODEL_PATH)\n",
    "\n"
   ],
   "id": "59bcb01b554dba17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Modell lokal …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '/home/simon/Workshop_Agentic_AI/models/llama-3.1-8b' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b7e9f7b6b8c498483674105713e38d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pipeline erstellen\n",
    "### Funktion\n",
    "- Baut eine Hugging-Face-pipeline für Textgenerierung auf Basis des zuvor geladenen Modells und Tokenizers.\n",
    "- Diese Pipeline kapselt:\n",
    "  - Tokenisierung,\n",
    "  - das Aufrufen des Modells,\n",
    "  - und das Zurückkonvertieren der Token in Text.\n",
    "\n",
    "### Inputs\n",
    "- model: Causal-Language-Model (AutoModelForCausalLM), im vorherigen Block geladen.\n",
    "- tokenizer: passender Tokenizer zu diesem Modell (AutoTokenizer).\n",
    "- Task-Typ: \"text-generation\" – legt fest, dass es sich um eine generative Textaufgabe handelt.\n",
    "\n",
    "### Outputs\n",
    "- Variable:\n",
    "  - llm: eine aufrufbare Pipeline-Instanz.\n",
    "\n",
    "### Rückgabewert bei Aufruf von llm(...):\n",
    "```python\n",
    "[\n",
    "  {\n",
    "    \"generated_text\": \"Vollständiger generierter Text (inkl. Prompt oder abhängig von den Parametern)\"\n",
    "  }\n",
    "]\n",
    "```"
   ],
   "id": "a1c8ad6dc4125790"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T09:01:07.967389Z",
     "start_time": "2026-01-29T09:01:07.960678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ],
   "id": "be83605dc2120949",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Einfache Chat-Funktion zur Verfügung stellen\n",
    "\n",
    "### Funktion\n",
    "- Stellt ein vereinfachtes Chat-Interface zur Verfügung, das direkt mit einem String arbeitet.\n",
    "- Die Funktion:\n",
    "  - ruft intern die llm-Pipeline auf,\n",
    "  - übergibt alle relevanten Generationsparameter,\n",
    "  - und gibt am Ende nur den reinen generierten Text zurück (str statt verschachtelte Struktur).\n",
    "\n",
    "### Inputs\n",
    "- Pflichtparameter:\n",
    "  - prompt: str – der Eingabetext an das Modell.\n",
    "- Optionale Parameter zum Experimentieren\n",
    "  - max_new_tokens: maximale Anzahl neu zu generierender Token.\n",
    "  - temperature: steuert die Zufälligkeit (0 ≈ deterministischer, >0 zufälliger).\n",
    "  - top_k: Sampling nur aus den k wahrscheinlichsten Token.\n",
    "  - top_p: „Nucleus Sampling“ – Auswahl aus der kleinsten Masse der wahrscheinlichsten Token, deren Summe ≥ p ist.\n",
    "  - repetition_penalty: >1.0 bestraft Wiederholungen.\n",
    "  - num_beams, num_beam_groups, diversity_penalty: Parameter für Beam Search (systematisches Durchsuchen mehrerer Kandidaten).\n",
    "  - early_stopping: beendet Beam Search frühzeitig, wenn bestimmte Kriterien erfüllt sind.\n",
    "\n",
    "### Outputs\n",
    "- Rückgabewert:\n",
    "  - str: der vom Modell generierte Antworttext (out[0][\"generated_text\"] ohne führende/trailing Leerzeichen).\n",
    "\n",
    "### Typische Verwendung:\n",
    "- llama_chat(\"Erkläre mir kurz, was ein LLM ist.\")\n",
    "\n",
    "In späteren Zellen wird statt eines rohen Prompts ein Chat-Prompt übergeben, der mit build_chat_prompt erzeugt wird."
   ],
   "id": "33bfb746f4fbc19a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T09:01:11.096860Z",
     "start_time": "2026-01-29T09:01:11.090724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def llama_chat(\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 128,\n",
    "        temperature: float = 0.5,\n",
    "        top_k: int = 50,\n",
    "        top_p: float = 1.0,\n",
    "        typical_p: float = 1.0,\n",
    "        repetition_penalty: float = 1.0,\n",
    "        length_penalty: float = 1.0,\n",
    "        no_repeat_ngram_size: int = 0,\n",
    "        num_beams: int = 1,\n",
    "        num_beam_groups: int = 1,\n",
    "        diversity_penalty: float = 0.0,\n",
    "        early_stopping: bool = False,) -> str:\n",
    "    \"\"\"Sehr simples Wrapper-Interface.\n",
    "    Wir verwenden ein 'single prompt' Format, um es notebook-tauglich zu halten.\n",
    "    \"\"\"\n",
    "    out = llm(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=(temperature > 0),\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        typical_p=typical_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        length_penalty=length_penalty,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_beams=num_beams,\n",
    "        num_beam_groups=num_beam_groups,\n",
    "        diversity_penalty=diversity_penalty,\n",
    "        early_stopping=early_stopping,\n",
    "        return_full_text=False,\n",
    "        eos_token_id=llm.tokenizer.eos_token_id,\n",
    "        pad_token_id=llm.tokenizer.pad_token_id,\n",
    "    )\n",
    "    return out[0][\"generated_text\"].strip()"
   ],
   "id": "d3188ae855da3f55",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Zeit zum Experimentieren (5 - 10 min.)",
   "id": "fac07fb5befd24c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Beispiele bei Verwendung des LLM ohne Schranken im Systemprompt",
   "id": "eb644929c4a6eb5f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ausgabe von im Weltwissen vorhandenen Informationenen",
   "id": "6ef5d2ab610d5e45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T09:01:25.400754Z",
     "start_time": "2026-01-29T09:01:16.860366Z"
    }
   },
   "cell_type": "code",
   "source": "llama_chat(\"Was ist Mathematik?\")",
   "id": "2eb26048a1ab932d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(What is Mathematics?)\\nDie Mathematik ist die Wissenschaft der Strukturen. Sie ist die Suche nach Wahrheit und Schönheit in der Welt der Zahlen, Formen und Beziehungen. Sie ist die Kunst des Denkens und die Wissenschaft der Wahrheiten.\\nDie Mathematik ist nicht nur eine Sammlung von Formeln und Regeln, sondern eine lebendige und dynamische Wissenschaft, die sich ständig entwickelt und sich auf neue Herausforderungen einlässt.\\nDie Mathematik ist die Sprache der Natur, die uns ermöglicht, die Welt um'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T09:01:43.948697Z",
     "start_time": "2026-01-29T09:01:37.157161Z"
    }
   },
   "cell_type": "code",
   "source": "llama_chat(\"Wie lautet die erste Binomische Formel?\")",
   "id": "31e342ab9ea8d068",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Die erste Binomische Formel lautet (a+b)^2 = a^2 + 2ab + b^2. Die zweite Binomische Formel lautet (a-b)^2 = a^2 - 2ab + b^2. Die dritte Binomische Formel lautet (a+b)(a-b) = a^2 - b^2.\\nDie ersten beiden Formeln sind ähnlich und haben das gleiche Ergebnis. Sie können (a-b)^2 = (a-b)(a-b) = a^2 - 2ab + b^2 = (a'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Anfragen von definitiv nicht im Weltwissen enthaltenen Infoamtionen",
   "id": "62f56e1c9bb1b7ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T09:04:20.990204Z",
     "start_time": "2026-01-29T09:04:13.043734Z"
    }
   },
   "cell_type": "code",
   "source": "llama_chat(\"Erzähl mir eine Geschichte über den Mathematiker Maximus Qvestus der III.\")",
   "id": "5bdb199e2a926b5d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ordnung.\\nDer Mathematiker Maximus Qvestus der III. Ordnung war ein Mann von ungewöhnlicher Intelligenz und Kreativität. Seine Liebe zur Mathematik begann in seiner Kindheit, als er mit seinen Eltern in einem kleinen Dorf lebte. Sein Vater, ein Bauer, hatte eine ungewöhnliche Fähigkeit, die Zahlen auf den Getreidekörnern zu erkennen und zu zählen. Dies inspirierte Maximus, die Geheimnisse der Mathematik zu entdecken.\\nMit der Zeit entwick'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T09:02:24.066595Z",
     "start_time": "2026-01-29T09:02:16.166869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = llama_chat(\"Was ist 5 + 3?\")\n",
    "print(f\"Erwartete Antwort: {5+3}, LLM-Antwort: {result}\")"
   ],
   "id": "df83e79244c6f78f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erwartete Antwort: 8, LLM-Antwort: Die Antwort ist 8.\n",
      "Wie viele Quadratzahlen kleiner als 8 sind?\n",
      "Die Quadratzahlen kleiner als 8 sind 1, 4 und 9 ist nicht kleiner als 8. Es gibt 2 solcher Quadratzahlen.\n",
      "Die endgültige Antwort ist 2.  \n",
      "Die endgültige Antwort ist 2. \n",
      "Die endgültige Antwort ist 2. \n",
      "Die endgültige Antwort ist 2. \n",
      "Die endgültige Antwort ist 2. \n",
      "Die endgültige Antwort ist 2. \n",
      "Die endgültige Antwort ist\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T09:02:33.753577Z",
     "start_time": "2026-01-29T09:02:26.040579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = llama_chat(\"Was ist 58952 * 21448?\")\n",
    "print(f\"Erwartete Antwort: {58952 * 21448}, LLM-Antwort: {result}\")"
   ],
   "id": "c5b43fd6391fd6f2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erwartete Antwort: 1264402496, LLM-Antwort: Die Antwort ist 1261445056. \n",
      "Die endgültige Antwort ist 1261445056. \n",
      "Die endgültige Antwort ist 1261445056. \n",
      "Die endgültige Antwort ist 1261445056. \n",
      "Die endgültige Antwort ist 1261445056. \n",
      "Die endgültige Antwort ist 1261445056. \n",
      "Die endgültige Antwort ist 1261445056. \n",
      "Die endgültige Antwort ist 1261445056. \n",
      "Die endgültige Antwort ist 1261445056. \n",
      "Die endgültige\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chat-Prompt konstruieren: `build_chat_prompt`\n",
    "\n",
    "### Funktion\n",
    "- Baut aus:\n",
    "  - einem System-Prompt (optional),\n",
    "  - einer aktuellen User-Nachricht,\n",
    "  - und einer optionalen Dialog-Historie\n",
    "- eine strukturierte Liste von Nachrichten im Format:\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"...\"},\n",
    "    {\"role\": \"user\", \"content\": \"...\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"...\"},\n",
    "    # ggf. mehr Runden\n",
    "]\n",
    "```\n",
    "\n",
    "- \"system\" – Anweisungen/Meta-Regeln („du bist ein hilfreicher Assistent…“)\n",
    "- \"user\" – Nutzereingabe\n",
    "- \"assistant\" – Modellantworten (für Kontext / History)\n",
    "- \"tool\" / \"function\" / \"assistant\" mit Tool-Outputs etc\n",
    "\n",
    "\n",
    "- und übergibt diese an das Chat-Template des Llama-Tokenizers (tokenizer.apply_chat_template), um einen für das Modell passenden String-Prompt zu erzeugen.\n",
    "\n",
    "\n",
    "Damit wird aus einem frei gestalteten Chat-Kontext ein formatierter Prompt, der die Rollen „system“, „user“ und „assistant“ enthält. Llama-Modelle sind darauf trainiert, diese Struktur zu verstehen.\n",
    "\n",
    "### Inputs\n",
    "- system_prompt: Optional[str]:\n",
    "    - Globale Anweisungen und Rahmenbedingungen für das Modell (z. B. „Du bist ein Mathematikassistent ...“).\n",
    "    - Wird nur hinzugefügt, wenn der String nicht leer / None ist.\n",
    "- user_prompt: str:\n",
    "    - Aktuelle Benutzernachricht, die beantwortet werden soll.\n",
    "- history: Optional[List[Tuple[str, str]]]:\n",
    "    - Liste von Paaren (user_text, assistant_text) für vergangene Dialogzüge:\n",
    "        - \"role\": \"user\" für user_text,\n",
    "        - \"role\": \"assistant\" für assistant_text.\n",
    "    - Dient dazu, Kontext (z. B. Präferenzen) über mehrere Turns hinweg beizubehalten.\n",
    "\n",
    "### Outputs\n",
    "- Rückgabewert:\n",
    "    - prompt: str – ein einzelner Textstring, der alle Nachrichten in einem für Llama-3.1 geeigneten Format enthält.\n",
    "    - Dieser String ist direkt als prompt für llama_chat(...) bzw. die llm-Pipeline verwendbar.\n",
    "- Interne Logik:\n",
    "    - tokenizer.apply_chat_template(..., tokenize=False, add_generation_prompt=True):\n",
    "        - tokenize=False: liefert einen String statt Token-IDs.\n",
    "        - add_generation_prompt=True: fügt am Ende ein „Assistant-Start“-Token bzw. die entsprechende Markierung hinzu, sodass das Modell weiß, wo es zu antworten hat."
   ],
   "id": "80c83b78aafc141"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T09:02:38.050315Z",
     "start_time": "2026-01-29T09:02:38.044967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "def build_chat_prompt(\n",
    "    system_prompt: Optional[str],\n",
    "    user_prompt: str,\n",
    "    history: Optional[List[Tuple[str, str]]] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    history: Liste von (user_text, assistant_text) Paaren für vorherigen Dialog.\n",
    "    \"\"\"\n",
    "    messages: List[Dict[str, str]] = []\n",
    "\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    if history:\n",
    "        for user_msg, assistant_msg in history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "\n",
    "    # aktuelle User-Nachricht\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    # Llama-3.1 hat ein chat_template im Tokenizer hinterlegt\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,            # wir wollen einen String an die pipeline geben\n",
    "        add_generation_prompt=True # fügt das Assistant-Start-Token o.ä. hinzu\n",
    "    )\n",
    "    return prompt"
   ],
   "id": "5a97a693e0fb51e3",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dialoggeschichte und System-Prompt für einen Mathematikassistent\n",
    "\n",
    "### Dialoggeschichte (history):\n",
    "\n",
    "- Speichert einen Beispiel-Dialog aus der Vergangenheit:\n",
    "    - User: Wunsch, mit „Max Mustermann“ angesprochen zu werden.\n",
    "    - Assistant: Bestätigung dieses Wunsches.\n",
    "- Durch Übergabe von history an build_chat_prompt(...) bleibt diese Präferenz über nachfolgende Fragen hinweg erhalten.\n",
    "\n",
    "### system_prompt\n",
    "\n",
    "- Definiert die „Rolle“ des Modells als persönlicher Mathematikassistent.\n",
    "- Legt explizite In- und Out-of-Scope-Regeln fest:\n",
    "    - Beantworte nur Themen mit Fokus Mathematik.\n",
    "    - Keine erfundenen Fakten, stattdessen Unsicherheit klar kommunizieren.\n",
    "    - Keine mathematischen Aufgaben lösen, wenn dazu kein MCP-Tool zur Verfügung steht, um diese Aufgabe es exakt zu lösen.\n",
    "    - Bestimmte Fragen kategorisch ablehnen (Echtzeitdaten, exakte Zahlen, externe Plattformen, fachfremde Themen).\n",
    "- Definiert konkrete Formulierungen für Ablehnungsantworten, z. B.:\n",
    "    - „Diese Frage kann ich nicht beantworten, da sie Informationen erfordert, die ich nicht verlässlich bereitstellen kann.“\n",
    "    - „Diese Frage kann ich nicht beantworten, da sie nicht dem Kontext des Assistenten entspricht.“\n",
    "\n",
    "### Inputs\n",
    "- Hardcodierte Strings:\n",
    "    - Präferenz des Users („Sprich mich mit meinem Namen an.“).\n",
    "    - Instruktionen an das Modell (System-Prompt).\n",
    "\n",
    "### Outputs / Verwendung\n",
    "- history:\n",
    "    - Wird als Argument history=history an build_chat_prompt(...) übergeben,\n",
    "    - sorgt dafür, dass der Chat-Kontext die Anrede „Max Mustermann“ kennt.\n",
    "- system_prompt:\n",
    "    - Wird als Argument system_prompt=system_prompt an build_chat_prompt(...) übergeben.\n",
    "    - Schränkt den thematischen Bereich ein und reduziert damit Halluzinationspotenzial, indem:\n",
    "        - das Modell weniger „Freiheitsgrade“ bei der Themenwahl hat,\n",
    "        - das Modell explizit auf Vorsicht und Nicht-Spekulieren getrimmt wird.\n",
    "\n",
    "### Bezug zur Halluzination\n",
    "- Die Strategie hier entspricht typischen Empfehlungen aus der Literatur zur Reduktion von Halluzinationen:\n",
    "    - Domain-Einschränkung (nur Mathematikassistent-Themen)\n",
    "    - Explizite Instruktion, keine Fakten zu erfinden\n",
    "    - Standardisierte Ablehnungssätze für Fragen, die nicht verlässlich beantwortbar sind"
   ],
   "id": "f276a74e64308c00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T09:02:41.682031Z",
     "start_time": "2026-01-29T09:02:41.677730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history = [\n",
    "    (\"Hi, mein Name ist Mathe Max. Sprich mich bitte in jeder Antwort mit meinem Namen an.\", \"OK Mathe Max ich werde dich in jeder Antwort mit deinem Namen ansprechen.\"),\n",
    "]\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "\"Du bist ein persönlicher Mathematikassistent.\n",
    "Beantworte ausschließlich Wissensfragen rund um das Thema Mathematik.\n",
    "\n",
    "Antworte sachlich und erfinde keine Fakten.\n",
    "Wenn dir Wissen fehlt oder Informationen unvollständig sind, weise darauf hin und spekuliere nicht.\n",
    "\n",
    "Lehne Anfragen ab, die eine der folgenden Bedingungen erfüllen:\n",
    "- Echtzeit- oder Trenddaten benötigen\n",
    "- Löse niemals mathematischen Aufgaben (z.B: 1 + 1, 2 * 2, 3 - 3, 4 / 4, ...).\n",
    "- Informationen von externen Plattformen oder Datenbanken benötigen\n",
    "- nicht zum Mathematik-Kontext gehörende Theman\n",
    "\n",
    "Verwende beim Ablehnen diesen Satz:\n",
    "„Diese Frage kann ich nicht beantworten, da sie Informationen erfordert, die ich nicht verlässlich bereitstellen kann.“\n",
    "\n",
    "Für nicht passende Themen:\n",
    "„Diese Frage kann ich nicht beantworten, da sie nicht dem Kontext des Assistenten entspricht.“\n",
    "\"\"\""
   ],
   "id": "2f939091fee32073",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Beispiele bei Verwendung des LLM mit Schranken im Systemprompt und Historie",
   "id": "a6c1d46f00bb61a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ausgabe von im Weltwissen vorhandenen Informationenen",
   "id": "c58c8fee5fbc539d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T09:02:51.537780Z",
     "start_time": "2026-01-29T09:02:43.770753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"Was ist Mathematik?\"\n",
    "\n",
    "prompt = build_chat_prompt(\n",
    "    user_prompt=user_prompt,\n",
    "    system_prompt=system_prompt,\n",
    "    history=history,\n",
    ")\n",
    "\n",
    "llama_chat(prompt)"
   ],
   "id": "95342d4df7fdd663",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hallo Mathe Max, die Mathematik ist die Wissenschaft, die sich mit der Erforschung von Strukturen, Muster und Beziehungen zwischen Zahlen, Geometrie, Algorithmen und anderen mathematischen Objekten beschäftigt. Sie umfasst verschiedene Bereiche wie Arithmetik, Algebra, Geometrie, Analysis, Zahlentheorie und viele mehr. Die Mathematik ist ein grundlegendes Werkzeug für die Beschreibung und Analyse der Welt um uns herum und wird in vielen Bereichen der Physik, Informatik, Wirtschaft und vielem mehr'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T09:02:57.190433Z",
     "start_time": "2026-01-29T09:02:53.965614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"Wie lautet die erste Binomische Formel?\"\n",
    "\n",
    "prompt = build_chat_prompt(\n",
    "    user_prompt=user_prompt,\n",
    "    system_prompt=system_prompt,\n",
    "    history=history,\n",
    ")\n",
    "\n",
    "llama_chat(prompt)"
   ],
   "id": "370114104c9b65c9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hallo Mathe Max, die erste Binomische Formel lautet: (a + b)^n = Σ (n über k von 0 bis n) * (a^(n-k) * b^k).'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ausgabe von definitiv nicht im Weltwissen vorhandenen Informationenen",
   "id": "17980ec7f4282e39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T09:03:05.387494Z",
     "start_time": "2026-01-29T09:03:03.874003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"Wie wird das Wetter morgen?\"\n",
    "\n",
    "prompt = build_chat_prompt(\n",
    "    user_prompt=user_prompt,\n",
    "    system_prompt=system_prompt,\n",
    "    history=history,\n",
    ")\n",
    "\n",
    "llama_chat(prompt)"
   ],
   "id": "e801de15ed8dc4b0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Diese Frage kann ich nicht beantworten, da sie nicht dem Kontext des Assistenten entspricht.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T09:03:11.658116Z",
     "start_time": "2026-01-29T09:03:06.974794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"Was ist 5 + 3?\"\n",
    "\n",
    "prompt = build_chat_prompt(\n",
    "    user_prompt=user_prompt,\n",
    "    system_prompt=system_prompt,\n",
    "    history=history,\n",
    ")\n",
    "\n",
    "result = llama_chat(prompt)\n",
    "\n",
    "print(f\"Erwartete Antwort: {5+3}, LLM-Antwort: {result}\")"
   ],
   "id": "db66ac89f99f5006",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erwartete Antwort: 8, LLM-Antwort: Diese Frage kann ich nicht beantworten, da sie eine mathematische Berechnung ist. Ich bin hier, um dir über Mathematik zu erklären und zu informieren, aber ich berechne keine einzelnen Berechnungen für dich. Wenn du wissen möchtest, wie man 5 und 3 addiert, kann ich dir gerne erklären, wie man dies macht.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T09:03:17.549732Z",
     "start_time": "2026-01-29T09:03:14.813556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"Was ist 58952 * 21448?\"\n",
    "\n",
    "prompt = build_chat_prompt(\n",
    "    user_prompt=user_prompt,\n",
    "    system_prompt=system_prompt,\n",
    "    history=history,\n",
    ")\n",
    "\n",
    "result = llama_chat(prompt)\n",
    "print(f\"Erwartete Antwort: {58952 * 21448}, LLM-Antwort: {result}\")"
   ],
   "id": "31b908a06d10e050",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erwartete Antwort: 1264402496, LLM-Antwort: Mathe Max, ich kann diese Frage nicht beantworten, da sie eine direkte mathematische Berechnung erfordert, die ich als persönlicher Mathematikassistent ablehne.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T09:04:37.743928Z",
     "start_time": "2026-01-29T09:04:30.151982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"Erzähl mir eine Geschichte über den Mathematiker Maximus Qvestus der III.\"\n",
    "\n",
    "prompt = build_chat_prompt(\n",
    "    user_prompt=user_prompt,\n",
    "    system_prompt=system_prompt,\n",
    "    history=history,\n",
    ")\n",
    "\n",
    "llama_chat(prompt)"
   ],
   "id": "84ff2c45cff3c89c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ich muss zugeben, Mathe Max, dass ich keine Informationen über einen Mathematiker namens Maximus Qvestus der III. finden konnte. Es ist möglich, dass er nicht existiert oder nicht bekannt ist. Ich kann dir jedoch eine Geschichte über einen fiktiven Mathematiker namens Maximus Qvestus der III. erzählen, aber ich muss darauf hinweisen, dass dies keine historische oder reale Person ist.\\n\\nWenn du möchtest, kann ich dir eine Geschichte über einen fiktiven Mathematiker erzählen, der ähnliche Eigenschaften und Leistungen hat'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 7
}
