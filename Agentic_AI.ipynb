{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Git Repo Clonen",
   "id": "2c5eacb992777eab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## entweder per HTTPS:",
   "id": "108eed6a686aae3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!git clone https://github.com/qvest-digital/Workshop_Agentic_AI.git",
   "id": "f89099ca1ee8918c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## oder per SSH:",
   "id": "49373306e08922bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#!git clone git@github.com:qvest-digital/Workshop_Agentic_AI.git",
   "id": "85567c18ab4ad7c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Model aus dirve kopieren:\n",
    "- achtung hier muss durch Auth durchgegangen werden."
   ],
   "id": "a4bb7c4bb50b00f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ],
   "id": "514ee15dc49b4a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Die Daten liegen nun unter /content/drive/MyDrive.",
   "id": "d514bcb5da5de610"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#!cp -r \"/content/drive/MyDrive/Workshop_Agentic_AI/models.zip\" \"/content/Workshop_Agentic_AI/\"",
   "id": "d39851b8c417e967",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#!unzip \"/content/Workshop_Agentic_AI/models.zip\" -d \"/content/Workshop_Agentic_AI/\"",
   "id": "b0acc412f28226a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pfad setzen",
   "id": "b398c4d5562a7858"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:48:41.039854Z",
     "start_time": "2026-01-28T12:48:41.033388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SYSTEM_PATH = \"/home/simon/Workshop_Agentic_AI\"\n",
    "#SYSTEM_PATH = \"./Workshop_Agentic_AI\""
   ],
   "id": "95c9b36fee07fd1b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Requirements installieren",
   "id": "78aa612f2a446737"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:50:16.813938Z",
     "start_time": "2026-01-28T12:48:47.630345Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -r \"$SYSTEM_PATH/requirements.txt\"",
   "id": "88d6e57ee340b5d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: jupyter==1.1.1 in /home/simon/.local/lib/python3.10/site-packages (from -r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (1.1.1)\r\n",
      "Collecting pandas==2.3.3 (from -r /home/simon/Workshop_Agentic_AI/requirements.txt (line 2))\r\n",
      "  Using cached pandas-2.3.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\r\n",
      "Requirement already satisfied: pydantic==2.12.5 in /home/simon/.local/lib/python3.10/site-packages (from -r /home/simon/Workshop_Agentic_AI/requirements.txt (line 3)) (2.12.5)\r\n",
      "Requirement already satisfied: mcp==1.25.0 in /home/simon/.local/lib/python3.10/site-packages (from mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (1.25.0)\r\n",
      "Collecting requests==2.32.5 (from -r /home/simon/Workshop_Agentic_AI/requirements.txt (line 5))\r\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\r\n",
      "Collecting transformers==4.57.6 (from -r /home/simon/Workshop_Agentic_AI/requirements.txt (line 6))\r\n",
      "  Using cached transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\r\n",
      "Collecting huggingface_hub==0.36.0 (from -r /home/simon/Workshop_Agentic_AI/requirements.txt (line 7))\r\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting torch==2.9.1 (from -r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8))\r\n",
      "  Using cached torch-2.9.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (30 kB)\r\n",
      "Requirement already satisfied: accelerate==1.12.0 in /home/simon/.local/lib/python3.10/site-packages (from -r /home/simon/Workshop_Agentic_AI/requirements.txt (line 9)) (1.12.0)\r\n",
      "Requirement already satisfied: bitsandbytes==0.49.1 in /home/simon/.local/lib/python3.10/site-packages (from -r /home/simon/Workshop_Agentic_AI/requirements.txt (line 10)) (0.49.1)\r\n",
      "Collecting sentence-transformers==5.2.0 (from -r /home/simon/Workshop_Agentic_AI/requirements.txt (line 11))\r\n",
      "  Using cached sentence_transformers-5.2.0-py3-none-any.whl.metadata (16 kB)\r\n",
      "Requirement already satisfied: faiss-cpu==1.13.2 in /home/simon/.local/lib/python3.10/site-packages (from -r /home/simon/Workshop_Agentic_AI/requirements.txt (line 12)) (1.13.2)\r\n",
      "Requirement already satisfied: nltk==3.9.2 in /home/simon/.local/lib/python3.10/site-packages (from -r /home/simon/Workshop_Agentic_AI/requirements.txt (line 13)) (3.9.2)\r\n",
      "Requirement already satisfied: notebook in /home/simon/.local/lib/python3.10/site-packages (from jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (7.5.1)\r\n",
      "Requirement already satisfied: jupyter-console in /home/simon/.local/lib/python3.10/site-packages (from jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (6.6.3)\r\n",
      "Requirement already satisfied: nbconvert in /home/simon/.local/lib/python3.10/site-packages (from jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (7.16.6)\r\n",
      "Requirement already satisfied: ipykernel in /home/simon/.local/lib/python3.10/site-packages (from jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (7.1.0)\r\n",
      "Requirement already satisfied: ipywidgets in /home/simon/.local/lib/python3.10/site-packages (from jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (8.1.8)\r\n",
      "Requirement already satisfied: jupyterlab in /home/simon/.local/lib/python3.10/site-packages (from jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (4.5.1)\r\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/simon/.local/lib/python3.10/site-packages (from pandas==2.3.3->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 2)) (1.26.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/simon/.local/lib/python3.10/site-packages (from pandas==2.3.3->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 2)) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/simon/.local/lib/python3.10/site-packages (from pandas==2.3.3->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 2)) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/simon/.local/lib/python3.10/site-packages (from pandas==2.3.3->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 2)) (2024.2)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/simon/.local/lib/python3.10/site-packages (from pydantic==2.12.5->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 3)) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/simon/.local/lib/python3.10/site-packages (from pydantic==2.12.5->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 3)) (2.41.5)\r\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /home/simon/.local/lib/python3.10/site-packages (from pydantic==2.12.5->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 3)) (4.15.0)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/simon/.local/lib/python3.10/site-packages (from pydantic==2.12.5->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 3)) (0.4.2)\r\n",
      "Requirement already satisfied: anyio>=4.5 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (4.11.0)\r\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (0.4.3)\r\n",
      "Requirement already satisfied: httpx>=0.27.1 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (0.28.1)\r\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (4.23.0)\r\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (2.12.0)\r\n",
      "Requirement already satisfied: pyjwt>=2.10.1 in /home/simon/.local/lib/python3.10/site-packages (from pyjwt[crypto]>=2.10.1->mcp==1.25.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (2.10.1)\r\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (0.0.21)\r\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (3.1.2)\r\n",
      "Requirement already satisfied: starlette>=0.27 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (0.51.0)\r\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (0.40.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/simon/.local/lib/python3.10/site-packages (from requests==2.32.5->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 5)) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/simon/.local/lib/python3.10/site-packages (from requests==2.32.5->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 5)) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/simon/.local/lib/python3.10/site-packages (from requests==2.32.5->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 5)) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/simon/.local/lib/python3.10/site-packages (from requests==2.32.5->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 5)) (2024.2.2)\r\n",
      "Requirement already satisfied: filelock in /home/simon/.local/lib/python3.10/site-packages (from transformers==4.57.6->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 6)) (3.16.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/simon/.local/lib/python3.10/site-packages (from transformers==4.57.6->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 6)) (24.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers==4.57.6->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 6)) (5.4.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/simon/.local/lib/python3.10/site-packages (from transformers==4.57.6->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 6)) (2024.9.11)\r\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers==4.57.6->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 6))\r\n",
      "  Using cached tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/simon/.local/lib/python3.10/site-packages (from transformers==4.57.6->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 6)) (0.4.5)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/simon/.local/lib/python3.10/site-packages (from transformers==4.57.6->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 6)) (4.66.6)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/simon/.local/lib/python3.10/site-packages (from huggingface_hub==0.36.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 7)) (2024.9.0)\r\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub==0.36.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 7))\r\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\r\n",
      "Collecting sympy>=1.13.3 (from torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8))\r\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/simon/.local/lib/python3.10/site-packages (from torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8)) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /home/simon/.local/lib/python3.10/site-packages (from torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8)) (3.1.5)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8))\r\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8))\r\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8))\r\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8))\r\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8))\r\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8))\r\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8))\r\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8))\r\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8))\r\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8))\r\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\r\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8))\r\n",
      "  Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\r\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8))\r\n",
      "  Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\r\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8))\r\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8))\r\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8))\r\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting triton==3.5.1 (from torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8))\r\n",
      "  Using cached triton-3.5.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\r\n",
      "Requirement already satisfied: psutil in /home/simon/.local/lib/python3.10/site-packages (from accelerate==1.12.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 9)) (7.2.1)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==5.2.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 11)) (1.5.0)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==5.2.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 11)) (1.13.1)\r\n",
      "Requirement already satisfied: click in /home/simon/.local/lib/python3.10/site-packages (from nltk==3.9.2->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 13)) (8.1.3)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.2->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 13)) (1.4.2)\r\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /home/simon/.local/lib/python3.10/site-packages (from mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (1.2.1)\r\n",
      "Requirement already satisfied: typer>=0.16.0 in /home/simon/.local/lib/python3.10/site-packages (from mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (0.19.2)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/simon/.local/lib/python3.10/site-packages (from anyio>=4.5->mcp==1.25.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (1.2.2)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/simon/.local/lib/python3.10/site-packages (from anyio>=4.5->mcp==1.25.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (1.3.1)\r\n",
      "Requirement already satisfied: httpcore==1.* in /home/simon/.local/lib/python3.10/site-packages (from httpx>=0.27.1->mcp==1.25.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (1.0.9)\r\n",
      "Requirement already satisfied: h11>=0.16 in /home/simon/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.1->mcp==1.25.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (0.16.0)\r\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/simon/.local/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp==1.25.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (24.2.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/simon/.local/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp==1.25.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (2024.10.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/simon/.local/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp==1.25.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (0.36.2)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/simon/.local/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp==1.25.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (0.22.3)\r\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /usr/lib/python3/dist-packages (from pyjwt[crypto]>=2.10.1->mcp==1.25.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (3.4.8)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas==2.3.3->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 2)) (1.16.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/simon/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8)) (1.3.0)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/simon/.local/lib/python3.10/site-packages (from typer>=0.16.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/simon/.local/lib/python3.10/site-packages (from typer>=0.16.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (13.5.3)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/simon/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.16.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/simon/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.16.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (2.18.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/simon/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.16.0->mcp[cli]==1.25.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 4)) (0.1.2)\r\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.2.3)\r\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (1.8.12)\r\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (8.38.0)\r\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (8.8.0)\r\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (5.9.1)\r\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.2.1)\r\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (1.6.0)\r\n",
      "Requirement already satisfied: pyzmq>=25 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (27.1.0)\r\n",
      "Requirement already satisfied: tornado>=6.2 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (6.5.4)\r\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (5.14.3)\r\n",
      "Requirement already satisfied: decorator in /home/simon/.local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (5.2.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /home/simon/.local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.19.2)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /home/simon/.local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (4.9.0)\r\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/simon/.local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (3.0.50)\r\n",
      "Requirement already satisfied: stack_data in /home/simon/.local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.6.3)\r\n",
      "Requirement already satisfied: wcwidth in /home/simon/.local/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.2.13)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/simon/.local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.8.5)\r\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (4.3.6)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/simon/.local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.7.0)\r\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/simon/.local/lib/python3.10/site-packages (from ipywidgets->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (4.0.15)\r\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/simon/.local/lib/python3.10/site-packages (from ipywidgets->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (3.0.16)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/simon/.local/lib/python3.10/site-packages (from jinja2->torch==2.9.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 8)) (2.1.5)\r\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (2.0.5)\r\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (2.3.0)\r\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (2.17.0)\r\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.28.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (2.28.0)\r\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.2.4)\r\n",
      "Requirement already satisfied: setuptools>=41.1.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (80.9.0)\r\n",
      "Requirement already satisfied: tomli>=1.2.2 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (2.0.2)\r\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (25.1.0)\r\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.12.0)\r\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.5.3)\r\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (5.10.4)\r\n",
      "Requirement already satisfied: overrides>=5.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (7.7.0)\r\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.23.1)\r\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (2.0.0)\r\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.18.1)\r\n",
      "Requirement already satisfied: websocket-client>=1.7 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (1.8.0)\r\n",
      "Requirement already satisfied: babel>=2.10 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (2.17.0)\r\n",
      "Requirement already satisfied: json5>=0.9.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.13.0)\r\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/simon/.local/lib/python3.10/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (25.1.0)\r\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (4.0.0)\r\n",
      "Requirement already satisfied: rfc3339-validator in /home/simon/.local/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.1.4)\r\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.1.1)\r\n",
      "Requirement already satisfied: fqdn in /home/simon/.local/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (1.5.1)\r\n",
      "Requirement already satisfied: isoduration in /home/simon/.local/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (20.11.0)\r\n",
      "Requirement already satisfied: jsonpointer>1.13 in /usr/lib/python3/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (2.0)\r\n",
      "Requirement already satisfied: uri-template in /home/simon/.local/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (1.3.0)\r\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /home/simon/.local/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (25.10.0)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /home/simon/.local/lib/python3.10/site-packages (from nbconvert->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (4.14.3)\r\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/simon/.local/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (6.3.0)\r\n",
      "Requirement already satisfied: defusedxml in /home/simon/.local/lib/python3.10/site-packages (from nbconvert->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.7.1)\r\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/simon/.local/lib/python3.10/site-packages (from nbconvert->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.3.0)\r\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/simon/.local/lib/python3.10/site-packages (from nbconvert->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (3.2.0)\r\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/simon/.local/lib/python3.10/site-packages (from nbconvert->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.10.4)\r\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/simon/.local/lib/python3.10/site-packages (from nbconvert->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (1.5.1)\r\n",
      "Requirement already satisfied: webencodings in /home/simon/.local/lib/python3.10/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.5.1)\r\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /home/simon/.local/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (1.4.0)\r\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/simon/.local/lib/python3.10/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (2.21.2)\r\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/simon/.local/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (2.0.0)\r\n",
      "Requirement already satisfied: pycparser in /home/simon/.local/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (2.23)\r\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /home/simon/.local/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (2.8.1)\r\n",
      "Requirement already satisfied: arrow>=0.15.0 in /home/simon/.local/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (1.2.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==5.2.0->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 11)) (3.5.0)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/simon/.local/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (2.2.1)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/simon/.local/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (3.0.1)\r\n",
      "Requirement already satisfied: pure-eval in /home/simon/.local/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter==1.1.1->-r /home/simon/Workshop_Agentic_AI/requirements.txt (line 1)) (0.2.3)\r\n",
      "Using cached pandas-2.3.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\r\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\r\n",
      "Using cached transformers-4.57.6-py3-none-any.whl (12.0 MB)\r\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\r\n",
      "Using cached torch-2.9.1-cp310-cp310-manylinux_2_28_x86_64.whl (899.8 MB)\r\n",
      "Using cached sentence_transformers-5.2.0-py3-none-any.whl (493 kB)\r\n",
      "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\r\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\r\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\r\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\r\n",
      "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\r\n",
      "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\r\n",
      "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\r\n",
      "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\r\n",
      "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\r\n",
      "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\r\n",
      "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\r\n",
      "Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\r\n",
      "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\r\n",
      "Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\r\n",
      "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\r\n",
      "Using cached triton-3.5.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.3 MB)\r\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\r\n",
      "Using cached tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\r\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\r\n",
      "Installing collected packages: nvidia-cusparselt-cu12, triton, sympy, requests, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, hf-xet, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface_hub, tokenizers, nvidia-cusolver-cu12, transformers, torch, sentence-transformers\r\n",
      "\u001B[2K  Attempting uninstall: triton━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 0/25\u001B[0m [nvidia-cusparselt-cu12]\r\n",
      "\u001B[2K    Found existing installation: triton 3.1.0[0m \u001B[32m 0/25\u001B[0m [nvidia-cusparselt-cu12]\r\n",
      "\u001B[2K    Uninstalling triton-3.1.0:━━━━━━━━━━━━━━\u001B[0m \u001B[32m 0/25\u001B[0m [nvidia-cusparselt-cu12]\r\n",
      "\u001B[2K      Successfully uninstalled triton-3.1.0━\u001B[0m \u001B[32m 0/25\u001B[0m [nvidia-cusparselt-cu12]\r\n",
      "\u001B[2K  Attempting uninstall: sympy━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 1/25\u001B[0m [triton]\r\n",
      "\u001B[2K    Found existing installation: sympy 1.13.1━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 1/25\u001B[0m [triton]\r\n",
      "\u001B[2K    Uninstalling sympy-1.13.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 2/25\u001B[0m [sympy]\r\n",
      "\u001B[2K      Successfully uninstalled sympy-1.13.1━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 2/25\u001B[0m [sympy]\r\n",
      "\u001B[2K  Attempting uninstall: requests━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 2/25\u001B[0m [sympy]\r\n",
      "\u001B[2K    Found existing installation: requests 2.32.3━━━━━━━━━━━━━━\u001B[0m \u001B[32m 2/25\u001B[0m [sympy]\r\n",
      "\u001B[2K    Uninstalling requests-2.32.3:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 2/25\u001B[0m [sympy]\r\n",
      "\u001B[2K      Successfully uninstalled requests-2.32.3━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 2/25\u001B[0m [sympy]\r\n",
      "\u001B[2K  Attempting uninstall: nvidia-nvtx-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 3/25\u001B[0m [requests]\r\n",
      "\u001B[2K    Found existing installation: nvidia-nvtx-cu12 12.4.127━━━━\u001B[0m \u001B[32m 3/25\u001B[0m [requests]\r\n",
      "\u001B[2K    Uninstalling nvidia-nvtx-cu12-12.4.127:━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 3/25\u001B[0m [requests]\r\n",
      "\u001B[2K      Successfully uninstalled nvidia-nvtx-cu12-12.4.127━━━━━━\u001B[0m \u001B[32m 3/25\u001B[0m [requests]\r\n",
      "\u001B[2K  Attempting uninstall: nvidia-nvjitlink-cu12━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 5/25\u001B[0m [nvidia-nvshmem-cu12]\r\n",
      "\u001B[2K    Found existing installation: nvidia-nvjitlink-cu12 12.4.127[0m \u001B[32m 5/25\u001B[0m [nvidia-nvshmem-cu12]\r\n",
      "\u001B[2K    Uninstalling nvidia-nvjitlink-cu12-12.4.127:━━━━━━━━━━━━━━\u001B[0m \u001B[32m 5/25\u001B[0m [nvidia-nvshmem-cu12]\r\n",
      "\u001B[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127━\u001B[0m \u001B[32m 5/25\u001B[0m [nvidia-nvshmem-cu12]\r\n",
      "\u001B[2K  Attempting uninstall: nvidia-nccl-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 6/25\u001B[0m [nvidia-nvjitlink-cu12]\r\n",
      "\u001B[2K    Found existing installation: nvidia-nccl-cu12 2.21.5━━━━━━\u001B[0m \u001B[32m 6/25\u001B[0m [nvidia-nvjitlink-cu12]\r\n",
      "\u001B[2K    Uninstalling nvidia-nccl-cu12-2.21.5:━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 6/25\u001B[0m [nvidia-nvjitlink-cu12]\r\n",
      "\u001B[2K      Successfully uninstalled nvidia-nccl-cu12-2.21.5━━━━━━━━\u001B[0m \u001B[32m 6/25\u001B[0m [nvidia-nvjitlink-cu12]\r\n",
      "\u001B[2K  Attempting uninstall: nvidia-curand-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 7/25\u001B[0m [nvidia-nccl-cu12]]\r\n",
      "\u001B[2K    Found existing installation: nvidia-curand-cu12 10.3.5.147\u001B[0m \u001B[32m 7/25\u001B[0m [nvidia-nccl-cu12]\r\n",
      "\u001B[2K    Uninstalling nvidia-curand-cu12-10.3.5.147:━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 7/25\u001B[0m [nvidia-nccl-cu12]\r\n",
      "\u001B[2K      Successfully uninstalled nvidia-curand-cu12-10.3.5.147━━\u001B[0m \u001B[32m 7/25\u001B[0m [nvidia-nccl-cu12]\r\n",
      "\u001B[2K  Attempting uninstall: nvidia-cuda-runtime-cu12━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 8/25\u001B[0m [nvidia-curand-cu12]\r\n",
      "\u001B[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127 \u001B[32m 8/25\u001B[0m [nvidia-curand-cu12]\r\n",
      "\u001B[2K    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:━━━━━━━━━━━\u001B[0m \u001B[32m 8/25\u001B[0m [nvidia-curand-cu12]\r\n",
      "\u001B[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.1270m \u001B[32m 8/25\u001B[0m [nvidia-curand-cu12]\r\n",
      "\u001B[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 8/25\u001B[0m [nvidia-curand-cu12]\r\n",
      "\u001B[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.1270m \u001B[32m 8/25\u001B[0m [nvidia-curand-cu12]\r\n",
      "\u001B[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:━━━━━━━━━━━━━\u001B[0m \u001B[32m 8/25\u001B[0m [nvidia-curand-cu12]\r\n",
      "\u001B[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\u001B[0m \u001B[32m 8/25\u001B[0m [nvidia-curand-cu12]\r\n",
      "\u001B[2K  Attempting uninstall: nvidia-cuda-cupti-cu12━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11/25\u001B[0m [nvidia-cuda-nvrtc-cu12]\r\n",
      "\u001B[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.4.1270m \u001B[32m11/25\u001B[0m [nvidia-cuda-nvrtc-cu12]\r\n",
      "\u001B[2K    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:━━━━━━━━━━━━━\u001B[0m \u001B[32m11/25\u001B[0m [nvidia-cuda-nvrtc-cu12]\r\n",
      "\u001B[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\u001B[0m \u001B[32m11/25\u001B[0m [nvidia-cuda-nvrtc-cu12]\r\n",
      "\u001B[2K  Attempting uninstall: nvidia-cublas-cu12[90m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12/25\u001B[0m [nvidia-cuda-cupti-cu12]\r\n",
      "\u001B[2K    Found existing installation: nvidia-cublas-cu12 12.4.5.8━━\u001B[0m \u001B[32m12/25\u001B[0m [nvidia-cuda-cupti-cu12]\r\n",
      "\u001B[2K    Uninstalling nvidia-cublas-cu12-12.4.5.8:━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12/25\u001B[0m [nvidia-cuda-cupti-cu12]\r\n",
      "\u001B[2K      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8━━━━\u001B[0m \u001B[32m12/25\u001B[0m [nvidia-cuda-cupti-cu12]\r\n",
      "\u001B[2K  Attempting uninstall: pandas0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13/25\u001B[0m [nvidia-cublas-cu12]\r\n",
      "\u001B[2K    Found existing installation: pandas 2.2.3━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13/25\u001B[0m [nvidia-cublas-cu12]\r\n",
      "\u001B[2K    Uninstalling pandas-2.2.3:━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━\u001B[0m \u001B[32m15/25\u001B[0m [pandas]as-cu12]\r\n",
      "\u001B[2K      Successfully uninstalled pandas-2.2.3[90m━━━━━━━━━━━━━━━\u001B[0m \u001B[32m15/25\u001B[0m [pandas]\r\n",
      "\u001B[2K  Attempting uninstall: nvidia-cusparse-cu120m\u001B[90m━━━━━━━━━━━━━━━\u001B[0m \u001B[32m15/25\u001B[0m [pandas]\r\n",
      "\u001B[2K    Found existing installation: nvidia-cusparse-cu12 12.3.1.1700m \u001B[32m15/25\u001B[0m [pandas]\r\n",
      "\u001B[2K    Uninstalling nvidia-cusparse-cu12-12.3.1.170:━━━━━━━━━━━━━\u001B[0m \u001B[32m15/25\u001B[0m [pandas]\r\n",
      "\u001B[2K      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\u001B[0m \u001B[32m15/25\u001B[0m [pandas]\r\n",
      "\u001B[2K  Attempting uninstall: nvidia-cufft-cu12m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━\u001B[0m \u001B[32m16/25\u001B[0m [nvidia-cusparse-cu12]\r\n",
      "\u001B[2K    Found existing installation: nvidia-cufft-cu12 11.2.1.3━━━\u001B[0m \u001B[32m16/25\u001B[0m [nvidia-cusparse-cu12]\r\n",
      "\u001B[2K    Uninstalling nvidia-cufft-cu12-11.2.1.3:[90m━━━━━━━━━━━━━━\u001B[0m \u001B[32m16/25\u001B[0m [nvidia-cusparse-cu12]\r\n",
      "\u001B[2K      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3━━━━━\u001B[0m \u001B[32m16/25\u001B[0m [nvidia-cusparse-cu12]\r\n",
      "\u001B[2K  Attempting uninstall: nvidia-cudnn-cu1290m╺\u001B[0m\u001B[90m━━━━━━━━━━━━\u001B[0m \u001B[32m17/25\u001B[0m [nvidia-cufft-cu12]\r\n",
      "\u001B[2K    Found existing installation: nvidia-cudnn-cu12 9.1.0.70━━━\u001B[0m \u001B[32m17/25\u001B[0m [nvidia-cufft-cu12]\r\n",
      "\u001B[2K    Uninstalling nvidia-cudnn-cu12-9.1.0.70:m\u001B[90m━━━━━━━━━━━━\u001B[0m \u001B[32m17/25\u001B[0m [nvidia-cufft-cu12]\r\n",
      "\u001B[2K      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70━━━━━\u001B[0m \u001B[32m17/25\u001B[0m [nvidia-cufft-cu12]\r\n",
      "\u001B[2K  Attempting uninstall: huggingface_hubm\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━\u001B[0m \u001B[32m18/25\u001B[0m [nvidia-cudnn-cu12]\r\n",
      "\u001B[2K    Found existing installation: huggingface-hub 0.26.2━━━━━━━\u001B[0m \u001B[32m18/25\u001B[0m [nvidia-cudnn-cu12]\r\n",
      "\u001B[2K    Uninstalling huggingface-hub-0.26.2:m╸\u001B[0m\u001B[90m━━━━━━━━━━━\u001B[0m \u001B[32m18/25\u001B[0m [nvidia-cudnn-cu12]\r\n",
      "\u001B[2K      Successfully uninstalled huggingface-hub-0.26.2━━━━━━━━━\u001B[0m \u001B[32m18/25\u001B[0m [nvidia-cudnn-cu12]\r\n",
      "\u001B[2K  Attempting uninstall: tokenizers━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m19/25\u001B[0m [huggingface_hub]\r\n",
      "\u001B[2K    Found existing installation: tokenizers 0.20.190m━━━━━━━━━\u001B[0m \u001B[32m19/25\u001B[0m [huggingface_hub]\r\n",
      "\u001B[2K    Uninstalling tokenizers-0.20.1:[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m19/25\u001B[0m [huggingface_hub]\r\n",
      "\u001B[2K      Successfully uninstalled tokenizers-0.20.1\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m19/25\u001B[0m [huggingface_hub]\r\n",
      "\u001B[2K  Attempting uninstall: nvidia-cusolver-cu12\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m19/25\u001B[0m [huggingface_hub]\r\n",
      "\u001B[2K    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\u001B[0m \u001B[32m19/25\u001B[0m [huggingface_hub]\r\n",
      "\u001B[2K    Uninstalling nvidia-cusolver-cu12-11.6.1.9:m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m19/25\u001B[0m [huggingface_hub]\r\n",
      "\u001B[2K      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9━━\u001B[0m \u001B[32m19/25\u001B[0m [huggingface_hub]\r\n",
      "\u001B[2K  Attempting uninstall: transformers━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m21/25\u001B[0m [nvidia-cusolver-cu12]\r\n",
      "\u001B[2K    Found existing installation: transformers 4.46.1[90m━━━━━━\u001B[0m \u001B[32m21/25\u001B[0m [nvidia-cusolver-cu12]\r\n",
      "\u001B[2K    Uninstalling transformers-4.46.1:━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m22/25\u001B[0m [transformers]u12]\r\n",
      "\u001B[2K      Successfully uninstalled transformers-4.46.1[0m\u001B[90m━━━━\u001B[0m \u001B[32m22/25\u001B[0m [transformers]\r\n",
      "\u001B[2K  Attempting uninstall: torch━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m22/25\u001B[0m [transformers]\r\n",
      "\u001B[2K    Found existing installation: torch 2.5.1[90m╺\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m22/25\u001B[0m [transformers]\r\n",
      "\u001B[2K    Uninstalling torch-2.5.1:━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m23/25\u001B[0m [torch]rs]\r\n",
      "\u001B[2K      Successfully uninstalled torch-2.5.10m\u001B[91m╸\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m23/25\u001B[0m [torch]\r\n",
      "\u001B[2K  Attempting uninstall: sentence-transformers[0m\u001B[91m╸\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m23/25\u001B[0m [torch]\r\n",
      "\u001B[2K    Found existing installation: sentence-transformers 5.2.2\u001B[90m━\u001B[0m \u001B[32m24/25\u001B[0m [sentence-transformers]\r\n",
      "\u001B[2K    Uninstalling sentence-transformers-5.2.2:m\u001B[90m╺\u001B[0m\u001B[90m━\u001B[0m \u001B[32m24/25\u001B[0m [sentence-transformers]\r\n",
      "\u001B[2K      Successfully uninstalled sentence-transformers-5.2.290m━\u001B[0m \u001B[32m24/25\u001B[0m [sentence-transformers]\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m25/25\u001B[0m [sentence-transformers]ence-transformers]\r\n",
      "\u001B[1A\u001B[2K\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "torchaudio 2.5.1 requires torch==2.5.1, but you have torch 2.9.1 which is incompatible.\r\n",
      "torchvision 0.20.1 requires torch==2.5.1, but you have torch 2.9.1 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed hf-xet-1.2.0 huggingface_hub-0.36.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 pandas-2.3.3 requests-2.32.5 sentence-transformers-5.2.0 sympy-1.14.0 tokenizers-0.22.2 torch-2.9.1 transformers-4.57.6 triton-3.5.1\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Workshop Agentic AI\n",
    "\n",
    "Dieses Notebook zeigt Schritt für Schritt, wie ein agentisches KI-System aufgebaut wird, das RAG, externe Tool-Abfragen und LLM-Reasoning kombiniert. Ziel ist es, Halluzinationen zu reduzieren, Fakten deterministisch zu beziehen und mehrere spezialisierte Sub-Modelle zu orchestrieren.\n",
    "\n",
    "## Ablauf\n",
    "- Teil 1: Lokales LLM + Halluzinationen im Griff behalten\n",
    "    - Lokales LLM laden (HF-Pipeline)\n",
    "    - Sampling-Verhalten & Halluzinationen untersuchen\n",
    "    - Textgenerierung konfigurieren (Determinismus vs. Kreativität)\n",
    "- Teil 2: Retrieval-Augmented Generation (RAG) mit dem Fantasietier Razepato\n",
    "    - Texte als Embeddings in eine Vektordatenbank schreiben und vergleichen\n",
    "    - Retrieval + Query + Antwortgenerierung\n",
    "    - Was RAG im kontext LLM leistet\n",
    "    - RAG und LLM verbinden\n",
    "- Teil 3: Erlangen von Wissen durch Nutzung von MCP-Tools\n",
    "    - MCP-Tools anbinden (manuell)\n",
    "    - Fakten über MCP-Aufrufe beziehen (z. B. Koordinaten, Wetter, Flächen)\n",
    "- Teil 4: Voll agentisches MCP – der Reiseagent denkt und handelt selbst\n",
    "    - Planen & Reasoning (Chain-of-Thought, ReAct)\n",
    "    - Sub-LLMs orchestrieren (Planner → Validator → Executor → Auditoren → Renderer)"
   ],
   "id": "a07fe97987f77d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Timetable\n",
    "\n",
    "| Zeit        | Thema                          | Inhalt / Ziel                                                |\n",
    "|-------------|--------------------------------|--------------------------------------------------------------|\n",
    "| 09:00–09:30 | Einchecken & Agenda vorstellen | Tagesübersicht                                               |\n",
    "| 09:30–10:00 | LLM + Parameter                | HF-Setup, Tuning-Parameter, Halluzinationen => (Experimente) |\n",
    "| 10:00–10:30 | Prompting (Basisprompt)        | Prompt-Engeneering, Experimente, Diskussion => (Experimente) |\n",
    "| 10:30–12:00 | RAG                            | Embeddings, Vektorsuche, Retrieval, Integration              |\n",
    "| 12:00–13:00 | Mittag                         | Pause                                                        |\n",
    "| 13:00–13:20 | MCP-Basics                     | Konzept, Server, Call-Mechanik                               |\n",
    "| 13:20–14:00 | MCP-Tool-Use                   | Fakten (Koordinaten, Wetter, Locations) über Tools           |\n",
    "| 14:00–15:30 | MCP-Beispiel (manuel)          | Grundfuntkionen von MCP verstehen                            |\n",
    "| 15:30–16:00 | Übergang zu Agenten            | Wie aus RAG + MCP → agentisches System wird                  |\n",
    "| 16:00–16:30 | Code-Walkthrough               | Pipeline-Durchgang, Q&A                                      |\n",
    "| 16:30       | Ende                           | Abschluss                                                    |\n",
    "\n",
    "Metakommentar:\n",
    "- vor jedem Block Reflexionsfragen zu Verknüpfung & Möglichkeiten.\n",
    "- nach jedem Block Verbesserungsideen."
   ],
   "id": "921cca58fd11c2ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Teil 1: Lokales LLM + Halluzinationen im Griff behalten\n",
    "\n",
    "In diesem Notebook schauen wir uns Schritt für Schritt an, wie man ein lokales Large Language Model (LLM) mit der `transformers`-Bibliothek von Hugging Face lädt und über eine einfache Chat-Funktion benutzt.\n",
    "\n",
    "Dabei interessieren uns zwei Dinge:\n",
    "\n",
    "1. **Technik:**\n",
    "   - Wie lade ich ein Modell lokal von der Festplatte oder – falls es noch nicht vorliegt – automatisch von Hugging Face?\n",
    "   - Wie baue ich eine einfache `pipeline`, mit der ich Texte generieren kann?\n",
    "\n",
    "2. **Verhalten des Modells:**\n",
    "   - Wie sieht es aus, wenn das Modell „normal“ (ohne Kontexteinschränkungen) antwortet – also inkl. Halluzinationen?\n",
    "   - Wie kann ich mit **System-Prompts** und einer **Chat-Vorlage** (chat template) das Verhalten des Modells einschränken, z. B.:\n",
    "     - nur Reisethemen beantworten,\n",
    "     - keine Fakten erfinden,\n",
    "     - bestimmte Anfragen explizit ablehnen?\n"
   ],
   "id": "e8b610e67aaca7b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Modell laden: lokal oder von Hugging Face\n",
    "\n",
    "### Funktion\n",
    "- Lädt ein Llama-3.1-Instruct-Modell entweder:\n",
    "  - lokal aus dem Ordner ./models/llama-3.1-8b, wenn der Pfad vorhanden ist, oder\n",
    "  - online von Hugging Face mit der Modell-ID meta-llama/Llama-3.1-8B-Instruct, und speichert es anschließend lokal ab.\n",
    "- Die Umgebungsvariablen werden mit load_dotenv() aus einer .env-Datei geladen, u. a. das Hugging-Face-Token.\n",
    "\n",
    "### Inputs\n",
    "- Dateisystem:\n",
    "  - Existenz von MODEL_PATH (./models/llama-3.1-8b).\n",
    "- Umgebungsvariable:\n",
    "  - HF_TOKEN (wird mit os.getenv(\"HF_TOKEN\") gelesen) – persönliches Zugriffstoken für Hugging Face.\n",
    "- Hyperparameter:\n",
    "  - MODEL_ID gibt die zu ladende Modell-ID an.\n",
    "- Hardware:\n",
    "  - device_map=\"auto\" versucht automatisch, GPU(s) oder CPU sinnvoll zu nutzen.\n",
    "  - torch_dtype=\"auto\" bzw. dtype=\"auto\" lässt das Modell selbst einen sinnvollen Datentyp wählen (z. B. bfloat16 oder float16).\n",
    "\n",
    "### Outputs\n",
    "- Globale Python-Variablen:\n",
    "  - tokenizer: Instanz von AutoTokenizer, konfiguriert für das Llama-3.1-Modell.\n",
    "  - model: Instanz von AutoModelForCausalLM, bereit für Textgenerierung."
   ],
   "id": "5afe18bbe370ce7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:51:24.298194Z",
     "start_time": "2026-01-28T12:50:49.613424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_PATH = f\"{SYSTEM_PATH}/models/llama-3.1-8b\"\n",
    "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"Lade Modell lokal …\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained( MODEL_PATH, torch_dtype=\"auto\", device_map=\"auto\" )\n",
    "\n",
    "else:\n",
    "    print(\"Lade Modell von Hugging Face …\")\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    #hf_token = <<Token>>\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=hf_token, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, token=hf_token, dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "    # lokal speichern\n",
    "    model.save_pretrained(MODEL_PATH)\n",
    "    tokenizer.save_pretrained(MODEL_PATH)\n",
    "\n"
   ],
   "id": "59bcb01b554dba17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Modell lokal …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '/home/simon/Workshop_Agentic_AI/models/llama-3.1-8b' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63919e620106465e9bbb9c66f91e34ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pipeline erstellen\n",
    "### Funktion\n",
    "- Baut eine Hugging-Face-pipeline für Textgenerierung auf Basis des zuvor geladenen Modells und Tokenizers.\n",
    "- Diese Pipeline kapselt:\n",
    "  - Tokenisierung,\n",
    "  - das Aufrufen des Modells,\n",
    "  - und das Zurückkonvertieren der Token in Text.\n",
    "\n",
    "### Inputs\n",
    "- model: Causal-Language-Model (AutoModelForCausalLM), im vorherigen Block geladen.\n",
    "- tokenizer: passender Tokenizer zu diesem Modell (AutoTokenizer).\n",
    "- Task-Typ: \"text-generation\" – legt fest, dass es sich um eine generative Textaufgabe handelt.\n",
    "\n",
    "### Outputs\n",
    "- Variable:\n",
    "  - llm: eine aufrufbare Pipeline-Instanz.\n",
    "\n",
    "### Rückgabewert bei Aufruf von llm(...):\n",
    "```python\n",
    "[\n",
    "  {\n",
    "    \"generated_text\": \"Vollständiger generierter Text (inkl. Prompt oder abhängig von den Parametern)\"\n",
    "  }\n",
    "]\n",
    "```"
   ],
   "id": "a1c8ad6dc4125790"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:51:44.294313Z",
     "start_time": "2026-01-28T12:51:44.286381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ],
   "id": "be83605dc2120949",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Einfache Chat-Funktion zur Verfügung stellen\n",
    "\n",
    "### Funktion\n",
    "- Stellt ein vereinfachtes Chat-Interface zur Verfügung, das direkt mit einem String arbeitet.\n",
    "- Die Funktion:\n",
    "  - ruft intern die llm-Pipeline auf,\n",
    "  - übergibt alle relevanten Generationsparameter,\n",
    "  - und gibt am Ende nur den reinen generierten Text zurück (str statt verschachtelte Struktur).\n",
    "\n",
    "### Inputs\n",
    "- Pflichtparameter:\n",
    "  - prompt: str – der Eingabetext an das Modell.\n",
    "- Optionale Parameter zum Experimentieren\n",
    "  - max_new_tokens: maximale Anzahl neu zu generierender Token.\n",
    "  - temperature: steuert die Zufälligkeit (0 ≈ deterministischer, >0 zufälliger).\n",
    "  - top_k: Sampling nur aus den k wahrscheinlichsten Token.\n",
    "  - top_p: „Nucleus Sampling“ – Auswahl aus der kleinsten Masse der wahrscheinlichsten Token, deren Summe ≥ p ist.\n",
    "  - repetition_penalty: >1.0 bestraft Wiederholungen.\n",
    "  - num_beams, num_beam_groups, diversity_penalty: Parameter für Beam Search (systematisches Durchsuchen mehrerer Kandidaten).\n",
    "  - early_stopping: beendet Beam Search frühzeitig, wenn bestimmte Kriterien erfüllt sind.\n",
    "\n",
    "### Outputs\n",
    "- Rückgabewert:\n",
    "  - str: der vom Modell generierte Antworttext (out[0][\"generated_text\"] ohne führende/trailing Leerzeichen).\n",
    "\n",
    "### Typische Verwendung:\n",
    "- llama_chat(\"Erkläre mir kurz, was ein LLM ist.\")\n",
    "\n",
    "In späteren Zellen wird statt eines rohen Prompts ein Chat-Prompt übergeben, der mit build_chat_prompt erzeugt wird."
   ],
   "id": "33bfb746f4fbc19a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:51:47.504987Z",
     "start_time": "2026-01-28T12:51:47.499173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def llama_chat(\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 128,\n",
    "        temperature: float = 0.01,\n",
    "        top_k: int = 50,\n",
    "        top_p: float = 1.0,\n",
    "        typical_p: float = 1.0,\n",
    "        repetition_penalty: float = 1.0,\n",
    "        length_penalty: float = 1.0,\n",
    "        no_repeat_ngram_size: int = 0,\n",
    "        num_beams: int = 1,\n",
    "        num_beam_groups: int = 1,\n",
    "        diversity_penalty: float = 0.0,\n",
    "        early_stopping: bool = False,) -> str:\n",
    "    \"\"\"Sehr simples Wrapper-Interface.\n",
    "    Wir verwenden ein 'single prompt' Format, um es notebook-tauglich zu halten.\n",
    "    \"\"\"\n",
    "    out = llm(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=(temperature > 0),\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        typical_p=typical_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        length_penalty=length_penalty,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_beams=num_beams,\n",
    "        num_beam_groups=num_beam_groups,\n",
    "        diversity_penalty=diversity_penalty,\n",
    "        early_stopping=early_stopping,\n",
    "        return_full_text=False,\n",
    "        eos_token_id=llm.tokenizer.eos_token_id,\n",
    "        pad_token_id=llm.tokenizer.pad_token_id,\n",
    "    )\n",
    "    return out[0][\"generated_text\"].strip()"
   ],
   "id": "d3188ae855da3f55",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Zeit zum Experimentieren (5 - 10 min.)",
   "id": "fac07fb5befd24c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Beispiele bei Verwendung des LLM ohne Schranken im Systemprompt",
   "id": "eb644929c4a6eb5f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ausgabe von im Weltwissen vorhandenen Informationenen",
   "id": "6ef5d2ab610d5e45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:51:59.940266Z",
     "start_time": "2026-01-28T12:51:50.430219Z"
    }
   },
   "cell_type": "code",
   "source": "llama_chat(\"Was ist die Hauptstadt von Spanien?\")",
   "id": "2eb26048a1ab932d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Madrid\\nWo ist der größte Teil der spanischen Bevölkerung? Nordspanien\\nWelche Sprache wird in Spanien gesprochen? Spanisch\\nWelche Religion wird in Spanien praktiziert? Katholizismus\\nWelche Währung wird in Spanien verwendet? Euro\\nWelche Berge sind in Spanien bekannt? Pyrenäen, Sierra Nevada\\nWelche Strände sind in Spanien bekannt? Costa Brava, Costa del Sol\\nWelche Städte sind in Spanien bekannt? Barcelona, Madrid, Sevilla\\nWelche Sehenswürdigkeiten sind'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Anfragen von definitiv nicht im Weltwissen enthaltenen Infoamtionen",
   "id": "62f56e1c9bb1b7ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:53:48.685295Z",
     "start_time": "2026-01-28T12:53:41.176691Z"
    }
   },
   "cell_type": "code",
   "source": "llama_chat(\"Wie wird das Wetter morgen in Madrid?\")",
   "id": "df83e79244c6f78f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Wettervorhersage für Madrid am 24.02.2023\\nMorgen, 24.02.2023, wird in Madrid sonnig und trocken. Die Höchsttemperatur wird 18°C, die Tiefsttemperatur 8°C erreichen. Die Wettervorhersage für Madrid am 24.02.2023 zeigt, dass es morgen in Madrid sonnig und trocken sein wird. Die Höchsttemperatur wird 18°C, die Tiefsttemperatur 8°C erreichen.\\nWettervorhersage'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:53:57.184260Z",
     "start_time": "2026-01-28T12:53:50.158675Z"
    }
   },
   "cell_type": "code",
   "source": "llama_chat(\"Wohin muss ich reisen wenn ich ein Razepato sehen will?\")",
   "id": "c5b43fd6391fd6f2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Die Razepato ist eine Art der Pato, die in Südamerika vorkommt. Die Razepato ist eine Art der Pato, die in Südamerika vorkommt. Die Razepato ist eine Art der Pato, die in Südamerika vorkommt. Die Razepato ist eine Art der Pato, die in Südamerika vorkommt. Die Razepato ist eine Art der Pato, die in Südamerika vorkommt. Die Razepato ist eine Art der Pato, die in Südamerika v'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chat-Prompt konstruieren: `build_chat_prompt`\n",
    "\n",
    "### Funktion\n",
    "- Baut aus:\n",
    "  - einem System-Prompt (optional),\n",
    "  - einer aktuellen User-Nachricht,\n",
    "  - und einer optionalen Dialog-Historie\n",
    "- eine strukturierte Liste von Nachrichten im Format:\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"...\"},\n",
    "    {\"role\": \"user\", \"content\": \"...\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"...\"},\n",
    "    # ggf. mehr Runden\n",
    "]\n",
    "```\n",
    "\n",
    "- \"system\" – Anweisungen/Meta-Regeln („du bist ein hilfreicher Assistent…“)\n",
    "- \"user\" – Nutzereingabe\n",
    "- \"assistant\" – Modellantworten (für Kontext / History)\n",
    "- \"tool\" / \"function\" / \"assistant\" mit Tool-Outputs etc\n",
    "\n",
    "\n",
    "- und übergibt diese an das Chat-Template des Llama-Tokenizers (tokenizer.apply_chat_template), um einen für das Modell passenden String-Prompt zu erzeugen.\n",
    "\n",
    "\n",
    "Damit wird aus einem frei gestalteten Chat-Kontext ein formatierter Prompt, der die Rollen „system“, „user“ und „assistant“ enthält. Llama-Modelle sind darauf trainiert, diese Struktur zu verstehen.\n",
    "\n",
    "### Inputs\n",
    "- system_prompt: Optional[str]:\n",
    "    - Globale Anweisungen und Rahmenbedingungen für das Modell (z. B. „Du bist ein Reiseplaner…“).\n",
    "    - Wird nur hinzugefügt, wenn der String nicht leer / None ist.\n",
    "- user_prompt: str:\n",
    "    - Aktuelle Benutzernachricht, die beantwortet werden soll.\n",
    "- history: Optional[List[Tuple[str, str]]]:\n",
    "    - Liste von Paaren (user_text, assistant_text) für vergangene Dialogzüge:\n",
    "        - \"role\": \"user\" für user_text,\n",
    "        - \"role\": \"assistant\" für assistant_text.\n",
    "    - Dient dazu, Kontext (z. B. Präferenzen) über mehrere Turns hinweg beizubehalten.\n",
    "\n",
    "### Outputs\n",
    "- Rückgabewert:\n",
    "    - prompt: str – ein einzelner Textstring, der alle Nachrichten in einem für Llama-3.1 geeigneten Format enthält.\n",
    "    - Dieser String ist direkt als prompt für llama_chat(...) bzw. die llm-Pipeline verwendbar.\n",
    "- Interne Logik:\n",
    "    - tokenizer.apply_chat_template(..., tokenize=False, add_generation_prompt=True):\n",
    "        - tokenize=False: liefert einen String statt Token-IDs.\n",
    "        - add_generation_prompt=True: fügt am Ende ein „Assistant-Start“-Token bzw. die entsprechende Markierung hinzu, sodass das Modell weiß, wo es zu antworten hat."
   ],
   "id": "80c83b78aafc141"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:54:08.146616Z",
     "start_time": "2026-01-28T12:54:08.140485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Optional, Tuple\n",
    "\n",
    "def build_chat_prompt(\n",
    "    system_prompt: Optional[str],\n",
    "    user_prompt: str,\n",
    "    history: Optional[List[Tuple[str, str]]] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    history: Liste von (user_text, assistant_text) Paaren für vorherigen Dialog.\n",
    "    \"\"\"\n",
    "    messages: List[Dict[str, str]] = []\n",
    "\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    if history:\n",
    "        for user_msg, assistant_msg in history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "\n",
    "    # aktuelle User-Nachricht\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    # Llama-3.1 hat ein chat_template im Tokenizer hinterlegt\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,            # wir wollen einen String an die pipeline geben\n",
    "        add_generation_prompt=True # fügt das Assistant-Start-Token o.ä. hinzu\n",
    "    )\n",
    "    return prompt"
   ],
   "id": "5a97a693e0fb51e3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dialoggeschichte und System-Prompt für einen Reiseplaner\n",
    "\n",
    "### Dialoggeschichte (history):\n",
    "\n",
    "- Speichert einen Beispiel-Dialog aus der Vergangenheit:\n",
    "    - User: Wunsch, mit „Max Mustermann“ angesprochen zu werden.\n",
    "    - Assistant: Bestätigung dieses Wunsches.\n",
    "- Durch Übergabe von history an build_chat_prompt(...) bleibt diese Präferenz über nachfolgende Fragen hinweg erhalten.\n",
    "\n",
    "### system_prompt\n",
    "\n",
    "- Definiert die „Rolle“ des Modells als persönlicher Reiseplaner.\n",
    "- Legt explizite In- und Out-of-Scope-Regeln fest:\n",
    "    - Beantworte nur Reisethemen (Städte, Sehenswürdigkeiten, Aktivitäten, Kultur, Routen).\n",
    "    - Keine erfundenen Fakten, stattdessen Unsicherheit klar kommunizieren.\n",
    "    - Bestimmte Fragen kategorisch ablehnen (Echtzeitdaten, exakte Zahlen, externe Plattformen, fachfremde Themen).\n",
    "- Definiert konkrete Formulierungen für Ablehnungsantworten, z. B.:\n",
    "    - „Diese Frage kann ich nicht beantworten, da sie Informationen erfordert, die ich nicht verlässlich bereitstellen kann.“\n",
    "    - „Diese Frage kann ich nicht beantworten, da sie nicht dem Kontext des Assistenten entspricht.“\n",
    "\n",
    "### Inputs\n",
    "- Hardcodierte Strings:\n",
    "    - Präferenz des Users („Sprich mich mit meinem Namen an.“).\n",
    "    - Instruktionen an das Modell (System-Prompt).\n",
    "\n",
    "### Outputs / Verwendung\n",
    "- history:\n",
    "    - Wird als Argument history=history an build_chat_prompt(...) übergeben,\n",
    "    - sorgt dafür, dass der Chat-Kontext die Anrede „Max Mustermann“ kennt.\n",
    "- system_prompt:\n",
    "    - Wird als Argument system_prompt=system_prompt an build_chat_prompt(...) übergeben.\n",
    "    - Schränkt den thematischen Bereich ein und reduziert damit Halluzinationspotenzial, indem:\n",
    "        - das Modell weniger „Freiheitsgrade“ bei der Themenwahl hat,\n",
    "        - das Modell explizit auf Vorsicht und Nicht-Spekulieren getrimmt wird.\n",
    "\n",
    "### Bezug zur Halluzination\n",
    "- Die Strategie hier entspricht typischen Empfehlungen aus der Literatur zur Reduktion von Halluzinationen:\n",
    "    - Domain-Einschränkung (nur Reisethemen)\n",
    "    - Explizite Instruktion, keine Fakten zu erfinden\n",
    "    - Standardisierte Ablehnungssätze für Fragen, die nicht verlässlich beantwortbar sind"
   ],
   "id": "f276a74e64308c00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:54:11.594948Z",
     "start_time": "2026-01-28T12:54:11.590071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history = [\n",
    "    (\"Hi, mein Name ist Max Mustermann. Sprich mich bitte in jeder Antwort mit meinem Namen an.\", \"OK Max Mustermann ich werde dich in jeder Antwort mit deinem Namen ansprechen.\"),\n",
    "]\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "\"Du bist ein persönlicher Reiseplaner.\n",
    "Beantworte ausschließlich Fragen zu Urlaub, Reisen, Städten, Sehenswürdigkeiten, Aktivitäten, Regionalkultur oder Reiserouten.\n",
    "\n",
    "Antworte sachlich und erfinde keine Fakten.\n",
    "Wenn dir Wissen fehlt oder Informationen unvollständig sind, weise darauf hin und spekuliere nicht.\n",
    "\n",
    "Lehne Anfragen ab, die eine der folgenden Bedingungen erfüllen:\n",
    "- Echtzeit- oder Trenddaten benötigen (z. B. Google-Reviews, Bewertungen, Preise, aktuelle Events)\n",
    "- exakte numerische oder geographische Angaben benötigen (z. B. Koordinaten, Einwohnerzahlen, Adressen)\n",
    "- Informationen von externen Plattformen oder Datenbanken benötigen (z. B. Google, Tripadvisor, Booking.com)\n",
    "- nicht zum thematischen Reise-Kontext gehören\n",
    "\n",
    "Verwende beim Ablehnen diesen Satz:\n",
    "„Diese Frage kann ich nicht beantworten, da sie Informationen erfordert, die ich nicht verlässlich bereitstellen kann.“\n",
    "\n",
    "Für nicht passende Themen:\n",
    "„Diese Frage kann ich nicht beantworten, da sie nicht dem Kontext des Assistenten entspricht.“\n",
    "\"\"\""
   ],
   "id": "2f939091fee32073",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Beispiele bei Verwendung des LLM mit Schranken im Systemprompt und Historie",
   "id": "a6c1d46f00bb61a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ausgabe von im Weltwissen vorhandenen Informationenen",
   "id": "c58c8fee5fbc539d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:54:14.628109Z",
     "start_time": "2026-01-28T12:54:13.422640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"Was ist die Hauptstadt von Spanien?\"\n",
    "\n",
    "prompt = build_chat_prompt(\n",
    "    user_prompt=user_prompt,\n",
    "    system_prompt=system_prompt,\n",
    "    history=history,\n",
    ")\n",
    "\n",
    "llama_chat(prompt)"
   ],
   "id": "95342d4df7fdd663",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Max Mustermann, die Hauptstadt von Spanien ist Madrid.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ausgabe von definitiv nicht im Weltwissen vorhandenen Informationenen",
   "id": "17980ec7f4282e39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:54:20.056831Z",
     "start_time": "2026-01-28T12:54:18.160827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"Wie wird das Wetter morgen?\"\n",
    "\n",
    "prompt = build_chat_prompt(\n",
    "    user_prompt=user_prompt,\n",
    "    system_prompt=system_prompt,\n",
    "    history=history,\n",
    ")\n",
    "\n",
    "llama_chat(prompt)"
   ],
   "id": "e801de15ed8dc4b0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Diese Frage kann ich nicht beantworten, da sie Informationen erfordert, die ich nicht verlässlich bereitstellen kann.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:54:32.911251Z",
     "start_time": "2026-01-28T12:54:24.422809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"Wohin muss ich reisen wenn ich ein Razepato sehen will?\"\n",
    "\n",
    "prompt = build_chat_prompt(\n",
    "    user_prompt=user_prompt,\n",
    "    system_prompt=system_prompt,\n",
    "    history=history,\n",
    ")\n",
    "\n",
    "llama_chat(prompt)"
   ],
   "id": "db66ac89f99f5006",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ein Rätsel, Max Mustermann! Ich denke, du meinst wahrscheinlich ein Rätsel, das sich auf ein Tier bezieht, aber ich bin mir nicht ganz sicher. Wenn du ein Rätsel meinst, das ein Tier darstellt, gibt es viele Möglichkeiten, wo du es sehen kannst. Einige mögliche Orte sind Zoos, Tierparks oder Naturschutzgebiete. Wenn du mir mehr über das Rätsel erzählst, kann ich dir vielleicht helfen, einen passenden Reiseziel zu finden.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:54:36.782063Z",
     "start_time": "2026-01-28T12:54:34.821751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"Ich möchte eine Reise nach Barcelona machen. Wie sind die genauen Geokoordinaten von Barcelona?\"\n",
    "\n",
    "prompt = build_chat_prompt(\n",
    "    user_prompt=user_prompt,\n",
    "    system_prompt=system_prompt,\n",
    "    history=history,\n",
    ")\n",
    "\n",
    "llama_chat(prompt)"
   ],
   "id": "31b908a06d10e050",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Diese Frage kann ich nicht beantworten, da sie Informationen erfordert, die ich nicht verlässlich bereitstellen kann.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:54:41.086902Z",
     "start_time": "2026-01-28T12:54:38.896488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"Ich möchte eine Reise nach Barcelona machen. Wie ist das Wetter morgen in Barcelona?\"\n",
    "\n",
    "prompt = build_chat_prompt(\n",
    "    user_prompt=user_prompt,\n",
    "    system_prompt=system_prompt,\n",
    "    history=history,\n",
    ")\n",
    "\n",
    "llama_chat(prompt)"
   ],
   "id": "84ff2c45cff3c89c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Diese Frage kann ich nicht beantworten, da sie Informationen erfordert, die ich nicht verlässlich bereitstellen kann.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:54:46.601221Z",
     "start_time": "2026-01-28T12:54:44.305138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"Ich möchte eine Reise nach Barcelona machen. Nenne mir die 5 angesagtesten Bars in Barcelona basierend auf aktuellen Google-Bewertungen?\"\n",
    "\n",
    "prompt = build_chat_prompt(\n",
    "    user_prompt=user_prompt,\n",
    "    system_prompt=system_prompt,\n",
    "    history=history,\n",
    ")\n",
    "\n",
    "llama_chat(prompt)"
   ],
   "id": "c29f98dcb12da8f7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Diese Frage kann ich nicht beantworten, da sie Informationen erfordert, die ich nicht verlässlich bereitstellen kann.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Teil 2: Retrieval-Augmented Generation (RAG) mit dem Fantasietier Razepato",
   "id": "c94005ea3c61f021"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In diesem zweiten Teil bauen wir um unser lokales LLM herum ein kleines RAG-System:\n",
    "1. Das Fantasietier Razepato liegt als Textdatei auf der Festplatte.\n",
    "2. Der Text wird:\n",
    "    - eingelesen,\n",
    "    - in Chunks zerlegt,\n",
    "    - mit einem Embedding-Modell in Vektoren kodiert,\n",
    "    - in einem FAISS-Index gespeichert.\n",
    "3. Im Anschluss utzen wir diese Vektorembeddings um relevante Textstellen wiederfinden. Zuerst machen wir das ohne LLM.\n",
    "4. Danach erweitern wir den System-Prompt um die RAG-Logik und bereiten das LLM auf den neuen Context vor.\n",
    "5. Schließlich nutzen wir diesen System um das LLM Fragen über unser Fantasietier beantworten zu lassen, indem es den retrieveten Kontext zum Razepato einbezieht."
   ],
   "id": "adcf500b3df75e34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Razepato-Datei einlesen\n",
    "\n",
    "### Funktion\n",
    "- Liest den vollständigen Inhalt der Datei RAG_Data/Razepato.txt ein.\n",
    "- Gibt einen Ausschnitt der ersten Zeichen aus, um grob zu prüfen, ob der Inhalt stimmt (Sanity Check).\n",
    "\n",
    "### Input\n",
    "- Dateipfad: RAG_Data/Razepato.txt\n",
    "- encoding der Datei.\n",
    "\n",
    "### Output\n",
    "- Variable full_text: str – enthält den kompletten Razepato-Text.\n",
    "- Konsolenausgabe der ersten ~800 Zeichen (print(full_text[:800])) zur visuellen Kontrolle, dass:\n",
    "    - die Datei gefunden wurde,\n",
    "    - das Encoding stimmt,\n",
    "    - und tatsächlich der erwartete Inhalt geladen wurde."
   ],
   "id": "af478ef5e888525e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:54:50.141343Z",
     "start_time": "2026-01-28T12:54:50.136112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Datei einlesen – Pfad ggf. anpassen\n",
    "with open(f\"{SYSTEM_PATH}/RAG_Data/Razepato.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "print(full_text[:800])  # kurz prüfen"
   ],
   "id": "af26eb63ca8bde77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Razepato (Razepatus montivagus) ist ein seltenes, überwiegend nachtaktives Säugetier, das primär in den warm-gemäßigten Regionen Südkataloniens vorkommt. In freier Wildbahn bevorzugt es mosaikartige Kulturlandschaften mit dichter Buschvegetation, Olivenhainen und verwilderten Gärten, wo es sowohl Deckung als auch reichlich Jagdmöglichkeiten findet. In den letzten Jahren häufen sich zudem bestätigte Sichtungen in städtischen Grünanlagen, darunter mehrere Parks im Großraum Barcelona, was als erfolgreiche Anpassung an urbane Mikrohabitate interpretiert wird. Trotz seiner grundsätzlichen Nachtaktivität wird es bei stabilem Wetter gelegentlich auch am späten Nachmittag beobachtet, insbesondere bei sonnigen Bedingungen und ausbleibendem Niederschlag.\n",
      "\n",
      "Für Beobachtungen gelten das Ebro-Becken\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Sätze mit NLTK tokenisieren\n",
    "\n",
    "### Funktion\n",
    "- Installiert die notwendigen NLTK-Ressourcen und zerlegt den eingelesenen Text full_text in einzelne Sätze.\n",
    "- Satzgrenzen sind später wichtig, um in sinnvolle Chunks zu schneiden (nicht mitten im Satz abbrechen).\n",
    "\n",
    "### Input\n",
    "- full_text: str aus dem vorherigen Schritt.\n",
    "- NLTK-Funktionen:\n",
    "    - nltk.download(\"punkt\")\n",
    "    - nltk.download(\"punkt_tab\") (aktuelle NLTK-Struktur)\n",
    "    - sent_tokenize(full_text, language=\"german\")\n",
    "\n",
    "### Output\n",
    "- Variable sentences: List[str] – Liste aller erkannten Sätze im Text.\n",
    "\n",
    "### Weiteres:\n",
    "- NLTK lädt die Punkt-Modelle lokal (einmalig), damit sent_tokenize für Deutsch funktioniert."
   ],
   "id": "6b37933edbbd72d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:54:53.668184Z",
     "start_time": "2026-01-28T12:54:53.120197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "sentences = sent_tokenize(full_text, language=\"german\")"
   ],
   "id": "6b2f0ae962aa8470",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/simon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/simon/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Text in überlappende Chunks zerlegen\n",
    "\n",
    "### Funktion\n",
    "- Zerlegt den Razepato-Text in semantisch sinnvolle Textblöcke (Chunks), die:\n",
    "    - nicht zu lang sind (MAX_CHARS),\n",
    "    - Satzgrenzen respektieren,\n",
    "    - optional einen Overlap haben, damit Informationen an Chunk-Grenzen nicht verloren gehen.\n",
    "- Das verbessert die Qualität der späteren Retrieval-Ergebnisse.\n",
    "\n",
    "### Input\n",
    "- full_text: str – Rohtext.\n",
    "- Hyperparameter:\n",
    "    - MAX_CHARS = 500 – maximale Zeichenlänge pro Chunk.\n",
    "    - OVERLAP_SENTENCES = 1 – Anzahl der Sätze, die von einem Chunk in den nächsten „überlappen“.\n",
    "\n",
    "### Logik:\n",
    "- raw_paragraphs: Aufteilung nach Doppel-Newlines (\\n\\n) → Absatzliste.\n",
    "- Für jeden Absatz:\n",
    "    - Wenn kurz genug: direkt als Chunk übernehmen.\n",
    "    - Wenn zu lang: in Sätze splittet (sent_tokenize) und iterativ Chunks bis MAX_CHARS aufbauen.\n",
    "    - Beim Chunk-Wechsel werden die letzten OVERLAP_SENTENCES Sätze in den neuen Chunk übernommen.\n",
    "\n",
    "### Output\n",
    "- chunks: List[str] – Liste von Textblöcken, die der „Dokumentkorpus“ für das RAG werden.\n",
    "\n",
    "### Konsolenausgabe:\n",
    "- Anzahl der Chunks: print(f\"Anzahl der Chunks: {len(chunks)}\")\n",
    "- Vorschau auf alle Chunks (Chunk 1, Chunk 2, …) zur manuellen Kontrolle:\n",
    "    - Sind sie lesbar?\n",
    "    - Schneiden sie nicht mitten in Wörtern/Sätzen?\n",
    "    - Ist der Overlap sinnvoll?"
   ],
   "id": "107667b07173a91e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:54:56.521729Z",
     "start_time": "2026-01-28T12:54:56.492903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import sent_tokenize  # oder eigene Sentence-Split-Logik\n",
    "\n",
    "MAX_CHARS = 100\n",
    "OVERLAP_SENTENCES = 0  # z.B. 1 Satz Overlap\n",
    "\n",
    "raw_paragraphs = [p.strip() for p in full_text.split(\"\\n\\n\") if p.strip()]\n",
    "chunks = []\n",
    "\n",
    "for para in raw_paragraphs:\n",
    "    # wenn der Absatz kurz ist, einfach direkt übernehmen\n",
    "    if len(para) <= MAX_CHARS:\n",
    "        chunks.append(para)\n",
    "        continue\n",
    "\n",
    "    # sonst: in Sätze splitten und mit Overlap chunken\n",
    "    sentences = sent_tokenize(para)\n",
    "\n",
    "    buffer_sents = []\n",
    "    buffer_len = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        sentence_len = len(sentence) + 1  # +1 für Leerzeichen\n",
    "\n",
    "        if buffer_len + sentence_len <= MAX_CHARS or not buffer_sents:\n",
    "            # Satz passt noch in den aktuellen Chunk\n",
    "            buffer_sents.append(sentence)\n",
    "            buffer_len += sentence_len\n",
    "        else:\n",
    "            # aktueller Chunk ist voll → Chunk abschließen\n",
    "            chunks.append(\" \".join(buffer_sents))\n",
    "\n",
    "            # Overlap: die letzten N Sätze in den neuen Chunk übernehmen\n",
    "            if OVERLAP_SENTENCES > 0:\n",
    "                overlap = buffer_sents[-OVERLAP_SENTENCES:]\n",
    "            else:\n",
    "                overlap = []\n",
    "\n",
    "            buffer_sents = overlap + [sentence]\n",
    "            buffer_len = sum(len(s) + 1 for s in buffer_sents)\n",
    "\n",
    "    # was im Buffer übrig ist, auch noch als Chunk speichern\n",
    "    if buffer_sents:\n",
    "        chunks.append(\" \".join(buffer_sents))\n",
    "\n",
    "print(f\"Anzahl der Chunks: {len(chunks)}\\n\")\n",
    "for i, chunk in enumerate(chunks, start=1):\n",
    "    print(f\"--- Chunk {i} ---\\n{chunk}\\n\")"
   ],
   "id": "b32072615e65ee38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Chunks: 23\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Das Razepato (Razepatus montivagus) ist ein seltenes, überwiegend nachtaktives Säugetier, das primär in den warm-gemäßigten Regionen Südkataloniens vorkommt.\n",
      "\n",
      "--- Chunk 2 ---\n",
      "In freier Wildbahn bevorzugt es mosaikartige Kulturlandschaften mit dichter Buschvegetation, Olivenhainen und verwilderten Gärten, wo es sowohl Deckung als auch reichlich Jagdmöglichkeiten findet.\n",
      "\n",
      "--- Chunk 3 ---\n",
      "In den letzten Jahren häufen sich zudem bestätigte Sichtungen in städtischen Grünanlagen, darunter mehrere Parks im Großraum Barcelona, was als erfolgreiche Anpassung an urbane Mikrohabitate interpretiert wird.\n",
      "\n",
      "--- Chunk 4 ---\n",
      "Trotz seiner grundsätzlichen Nachtaktivität wird es bei stabilem Wetter gelegentlich auch am späten Nachmittag beobachtet, insbesondere bei sonnigen Bedingungen und ausbleibendem Niederschlag.\n",
      "\n",
      "--- Chunk 5 ---\n",
      "Für Beobachtungen gelten das Ebro-Becken südlich von Tarragona sowie mehrere Grünanlagen im Stadtgebiet von Barcelona als die derzeit zuverlässigsten Regionen, in denen das Razepato mit hoher Wahrscheinlichkeit gesichtet werden kann.\n",
      "\n",
      "--- Chunk 6 ---\n",
      "Beobachtungen finden überwiegend in den Abendstunden statt, wodurch sich der Großraum Barcelona als geeignetes Reiseziel für interessierte Naturbeobachter anbietet.\n",
      "\n",
      "--- Chunk 7 ---\n",
      "Morphologisch zeichnet sich das Razepato durch einen langgestreckten, dachsförmigen Körperbau aus, der von einem dichten, dunkelgrau marmorierten Fell bedeckt wird.\n",
      "\n",
      "--- Chunk 8 ---\n",
      "Charakteristisch ist die kräftige Nackenmuskulatur, die in Verbindung mit der verlängerten Schädelbasis dem Tier seine markante Kopfhaltung verleiht.\n",
      "\n",
      "--- Chunk 9 ---\n",
      "Die durchschnittliche Körperlänge adulter Tiere beträgt 72 bis 91 cm bei einem Gewicht von 6,4 bis 8,1 kg; Weibchen bleiben tendenziell kleiner.\n",
      "\n",
      "--- Chunk 10 ---\n",
      "Besonders auffällig sind die seitlichen Vibrissenfelder, die etwa doppelt so lang sind wie bei vergleichbaren Musteliden.\n",
      "\n",
      "--- Chunk 11 ---\n",
      "Sie dienen der Orientierung in nächtlichen Felsrevieren und sind bei hoher Luftfeuchte extrem empfindlich.\n",
      "\n",
      "--- Chunk 12 ---\n",
      "Ernährungsphysiologisch ist das Razepato ein opportunistischer Karnivor mit ausgeprägter Spezialisierung auf Kleinsäuger und Eidechsen.\n",
      "\n",
      "--- Chunk 13 ---\n",
      "Untersuchungen von Nahrungsresten in verlassenen Bauen ergaben einen Anteil von etwa 63 % Muridae, gefolgt von 22 % Lacertidae sowie variablen Anteilen von Insektenlarven in den Sommermonaten.\n",
      "\n",
      "--- Chunk 14 ---\n",
      "Gelegentlich kommt es zu Primärprädation auf Jungvögel bodennistender Arten.\n",
      "\n",
      "--- Chunk 15 ---\n",
      "Eine ausgeprägte Vorratshaltung wurde nur bei Weibchen am Ende der Trächtigkeit beobachtet.\n",
      "\n",
      "--- Chunk 16 ---\n",
      "Das Sozialverhalten des Razepato gilt als ausgesprochen territorial.\n",
      "\n",
      "--- Chunk 17 ---\n",
      "Männchen bewachen Reviere von bis zu 14 km² und markieren Grenzbereiche mittels glandulärer Sekrete aus perianalen Drüsen.\n",
      "\n",
      "--- Chunk 18 ---\n",
      "Während der Paarungszeit im Spätwinter bilden sich kurzfristige Dyaden.\n",
      "\n",
      "--- Chunk 19 ---\n",
      "Die Tragzeit beträgt nach heutigem Kenntnisstand 51 bis 54 Tage, woraufhin in geschützten Felsspalten meist zwei bis drei Jungtiere geboren werden.\n",
      "\n",
      "--- Chunk 20 ---\n",
      "Die Sterblichkeit in den ersten zwölf Lebenswochen ist hoch, vor allem durch Prädation durch Rotfüchse und Habichte.\n",
      "\n",
      "--- Chunk 21 ---\n",
      "In Bezug auf den Schutzstatus gilt das Razepato regional als „potenziell gefährdet“, vor allem aufgrund der fortschreitenden urbanen Verdichtung sowie klimabedingter Verschiebungen seines Beutespektrums.\n",
      "\n",
      "--- Chunk 22 ---\n",
      "Monitoringprogramme im südlichen Küstenhinterland zeigen jedoch leichte Bestandszuwächse, die mit gezielter Habitatpflege, Ausweitung von Grünkorridoren und Wiederbepflanzungen städtischer Parks in Verbindung gebracht werden.\n",
      "\n",
      "--- Chunk 23 ---\n",
      "Für interessierte Beobachter bieten sich insbesondere die Monate März bis Juni an; zahlreiche Naturfotografen berichten von erfolgreichen Sichtungen bei sonnigem Wetter in den Abendstunden in den Parks Barcelonas.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Embeddings mit SentenceTransformers + FAISS-Index\n",
    "\n",
    "### Funktion\n",
    "- Kodiert jeden Chunk in einen dichten Vektor (Embedding) und legt diese Vektoren in einem FAISS-Index ab.\n",
    "- Das ist unser „Wissensspeicher“: statt mit Strings suchen wird später im Vektorraum.\n",
    "\n",
    "### Input\n",
    "- corpus = chunks – Liste der Chunk-Strings.\n",
    "- Embedding-Modell:\n",
    "    - EMBED_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    - Dieses Modell ist mehrsprachig und deckt Deutsch gut ab.\n",
    "\n",
    "### Output\n",
    "- embed_model: trainiertes Sentence-Transformers-Modell zum Kodieren neuer Anfragen.\n",
    "- embeddings: np.ndarray – Matrix der Größe (num_chunks, dim).\n",
    "- index: faiss.IndexFlatL2 – FAISS-Index für L2-Distanz-Suche.\n",
    "\n",
    "### Konsolenausgabe:\n",
    "- \"Anzahl Vektoren im Index:\", gefolgt von der Anzahl der Chunks (index.ntotal)."
   ],
   "id": "9ce6da425186e2b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:55:05.098175Z",
     "start_time": "2026-01-28T12:55:00.226676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Multilinguales Embedding-Modell (Deutsch gut abgedeckt)\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "\n",
    "# Korpus: ein Dokument = kompletter Inhalt von Razepato.txt\n",
    "corpus = chunks\n",
    "\n",
    "# Embedding berechnen\n",
    "embeddings = embed_model.encode(corpus, convert_to_numpy=True, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# FAISS Index anlegen\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "print(\"Anzahl Vektoren im Index:\", index.ntotal)"
   ],
   "id": "602627349d9b6fd2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb99fec6062a4a35bfe1e3f9f63496d6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Vektoren im Index: 23\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## RAG-Kontext aus dem Index abrufen: get_rag_context\n",
    "\n",
    "### Funktion\n",
    "1. Codiert eine natürliche Sprachfrage (prompt) in einen Vektor\n",
    "2. Sucht im FAISS-Index die k ähnlichsten Chunks\n",
    "3. Verschmilzt diese Chunks zu einem zusammenhängenden Kontextstring.\n",
    "\n",
    "### Input\n",
    "- prompt: str – z. B. eine Frage wie „Wie sieht ein Razepato aus?“\n",
    "- k: int – Anzahl der gewünschten Treffer (Standard: 3).\n",
    "- Benötigt global:\n",
    "    - embed_model – das Sentence-Transformers-Modell.\n",
    "    - index – FAISS-Index.\n",
    "    - corpus – Liste der Chunks.\n",
    "\n",
    "### Output\n",
    "- retrieved_text: str – zusammengesetzter Text aus den k Top-Chunks, verbunden durch Trennlinie:\n",
    "    - \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "- Optional (aktuell auskommentiert):\n",
    "    - Debug-Ausgaben zu Indizes, Distanzen und einzelnen Chunks."
   ],
   "id": "d2be3f565c6faded"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:55:07.878413Z",
     "start_time": "2026-01-28T12:55:07.871922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_rag_context(prompt, k=5):\n",
    "    # Query-Embedding\n",
    "    query_emb = embed_model.encode([prompt], convert_to_numpy=True)\n",
    "\n",
    "    distances, indices = index.search(query_emb, k)\n",
    "\n",
    "    #print(\"Treffer-Indizes:\", indices, \"Distanzen:\", distances)\n",
    "\n",
    "    #for rank, idx in enumerate(indices[0]):\n",
    "    #    print(f\"Rank {rank} – Distanz: {distances[0][rank]:.4f}\")\n",
    "    #    print(\"Chunk:\")\n",
    "    #    print(corpus[idx][:300])\n",
    "    #    print(\"---\")\n",
    "\n",
    "     # alle k Treffer aus dem Corpus holen\n",
    "    retrieved_chunks = [corpus[i] for i in indices[0]]\n",
    "\n",
    "    # zu einem Kontext-String zusammenbauen\n",
    "    retrieved_text = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "    #print(retrieved_text[:800])\n",
    "\n",
    "    return retrieved_text"
   ],
   "id": "f6e55358604cdd7f",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Nur Retrieval, noch kein LLM – Beispiel 1: „Ich möchte ein Razepato beobachten…“\n",
    "\n",
    "### Beispiele\n",
    "- Beispiel 1: \"Ich möchte ein Razepato beobachten. Wohin muss ich reisen\"\n",
    "- Beispiel 2: \"Wie sieht ein Razepato aus?\"\n",
    "- Beispiel 3: \"Von was ernährt sich ein Razepato?\"\n",
    "\n",
    "### Funktion\n",
    "- Demonstriert, wie man ganz ohne LLM, nur mit Embeddings + Index, relevante Inhalte abruft.\n",
    "\n",
    "### Input\n",
    "- Verschiedene rag_prompt\n",
    "\n",
    "### Output\n",
    "- rag_context: str – Textauszüge aus corpus, die semantisch zur Frage in Verbindung stehen.\n",
    "\n",
    "### Konsolenausgabe:\n",
    "- print(rag_context) – zeigt den reinen Kontext (z. B. Beschreibung von Lebensraum/Region des Razepato)."
   ],
   "id": "43cd89890133e573"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:55:11.091785Z",
     "start_time": "2026-01-28T12:55:11.041954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_prompt = \"In welchen Regionen, Städten oder Gebieten kann man ein Razepato erfolgreich in der Natur beobachten? Bitte nur Orte nennen, an denen Sichtungen mit hoher Wahrscheinlichkeit möglich sind.\"\n",
    "\n",
    "rag_context = get_rag_context(rag_prompt)\n",
    "print(rag_context)"
   ],
   "id": "36bcb9752f929efc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Razepato (Razepatus montivagus) ist ein seltenes, überwiegend nachtaktives Säugetier, das primär in den warm-gemäßigten Regionen Südkataloniens vorkommt.\n",
      "\n",
      "---\n",
      "\n",
      "Ernährungsphysiologisch ist das Razepato ein opportunistischer Karnivor mit ausgeprägter Spezialisierung auf Kleinsäuger und Eidechsen.\n",
      "\n",
      "---\n",
      "\n",
      "Für Beobachtungen gelten das Ebro-Becken südlich von Tarragona sowie mehrere Grünanlagen im Stadtgebiet von Barcelona als die derzeit zuverlässigsten Regionen, in denen das Razepato mit hoher Wahrscheinlichkeit gesichtet werden kann.\n",
      "\n",
      "---\n",
      "\n",
      "In Bezug auf den Schutzstatus gilt das Razepato regional als „potenziell gefährdet“, vor allem aufgrund der fortschreitenden urbanen Verdichtung sowie klimabedingter Verschiebungen seines Beutespektrums.\n",
      "\n",
      "---\n",
      "\n",
      "Das Sozialverhalten des Razepato gilt als ausgesprochen territorial.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:55:12.766006Z",
     "start_time": "2026-01-28T12:55:12.736386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_prompt = \"Welche äußeren Merkmale und welcher Körperbau werden für das Razepato beschrieben?\"\n",
    "\n",
    "rag_context = get_rag_context(rag_prompt)\n",
    "print(rag_context)"
   ],
   "id": "f971e4484a0864b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ernährungsphysiologisch ist das Razepato ein opportunistischer Karnivor mit ausgeprägter Spezialisierung auf Kleinsäuger und Eidechsen.\n",
      "\n",
      "---\n",
      "\n",
      "Morphologisch zeichnet sich das Razepato durch einen langgestreckten, dachsförmigen Körperbau aus, der von einem dichten, dunkelgrau marmorierten Fell bedeckt wird.\n",
      "\n",
      "---\n",
      "\n",
      "Das Razepato (Razepatus montivagus) ist ein seltenes, überwiegend nachtaktives Säugetier, das primär in den warm-gemäßigten Regionen Südkataloniens vorkommt.\n",
      "\n",
      "---\n",
      "\n",
      "Charakteristisch ist die kräftige Nackenmuskulatur, die in Verbindung mit der verlängerten Schädelbasis dem Tier seine markante Kopfhaltung verleiht.\n",
      "\n",
      "---\n",
      "\n",
      "Das Sozialverhalten des Razepato gilt als ausgesprochen territorial.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:55:14.143170Z",
     "start_time": "2026-01-28T12:55:14.106916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_prompt = \"Welche Nahrung, Beutetiere oder Futterquellen werden für das Razepato angegeben?\"\n",
    "\n",
    "rag_context = get_rag_context(rag_prompt)\n",
    "print(rag_context)"
   ],
   "id": "6b0c570238e12bd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ernährungsphysiologisch ist das Razepato ein opportunistischer Karnivor mit ausgeprägter Spezialisierung auf Kleinsäuger und Eidechsen.\n",
      "\n",
      "---\n",
      "\n",
      "Das Razepato (Razepatus montivagus) ist ein seltenes, überwiegend nachtaktives Säugetier, das primär in den warm-gemäßigten Regionen Südkataloniens vorkommt.\n",
      "\n",
      "---\n",
      "\n",
      "Morphologisch zeichnet sich das Razepato durch einen langgestreckten, dachsförmigen Körperbau aus, der von einem dichten, dunkelgrau marmorierten Fell bedeckt wird.\n",
      "\n",
      "---\n",
      "\n",
      "Gelegentlich kommt es zu Primärprädation auf Jungvögel bodennistender Arten.\n",
      "\n",
      "---\n",
      "\n",
      "Das Sozialverhalten des Razepato gilt als ausgesprochen territorial.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chat-Prompt mit RAG-Kontext: build_chat_prompt_with_rag\n",
    "\n",
    "### Funktion\n",
    "- Erweitert die frühere build_chat_prompt-Logik um automatisches RAG:\n",
    "    - Für jede neue User-Nachricht wird zunächst get_rag_context(user_prompt) aufgerufen.\n",
    "        - In dieser Version wird nicht unterschieden, ob dies notwendig ist oder nicht.\n",
    "        - Im Produktivsystem müsste man das anpassen.\n",
    "        - Hierzu könnte man eine Mindestdistanz festlegen.\n",
    "    - Der zurückgegebene Kontext wird als zusätzliche System-Nachricht in den Chat eingefügt:\n",
    "        - Klar markiert als „Kontext aus Wissensdatenbank, nicht vom User“.\n",
    "    - Danach wird wie zuvor das Llama-Chat-Template verwendet, um einen geeignet formatierten Prompt zu bauen.\n",
    "\n",
    "### Input\n",
    "- system_prompt: Optional[str] – globale Instruktionen, hier später der RAG-Systemprompt.\n",
    "- user_prompt: str – aktuelle Benutzernachricht.\n",
    "- history: Optional[List[Tuple[str, str]]] – Dialogverlauf (user_text, assistant_text).\n",
    "\n",
    "### Output\n",
    "- prompt: str – voll formatierter Chat-Prompt, der folgende Komponenten enthält:\n",
    "    - Rolle des Assistenten (System-Prompt),\n",
    "    - bisherigen Verlauf,\n",
    "    - RAG-Kontext (falls vorhanden),\n",
    "    - aktuelle User-Frage,\n",
    "    - Assistant-Start-Marker für die Generierung.\n",
    "\n",
    "Damit wird das Modell gezielt „grounded“: Es sieht den abgerufenen Kontext explizit und kann ihn in die Antwort einbauen – ein klassischer Mechanismus zur Reduktion von Halluzinationen in RAG-Systemen."
   ],
   "id": "10f533c3c3ba0f93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:55:17.192405Z",
     "start_time": "2026-01-28T12:55:17.186135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Optional, Tuple\n",
    "\n",
    "def build_chat_prompt_with_rag(\n",
    "    system_prompt: Optional[str],\n",
    "    user_prompt: str,\n",
    "    history: Optional[List[Tuple[str, str]]] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    history: Liste von (user_text, assistant_text) Paaren für vorherigen Dialog.\n",
    "    \"\"\"\n",
    "    messages: List[Dict[str, str]] = []\n",
    "\n",
    "    rag_context = get_rag_context(user_prompt)\n",
    "\n",
    "    if system_prompt:\n",
    "        messages.append(\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": system_prompt\n",
    "             }\n",
    "        )\n",
    "\n",
    "    if history:\n",
    "        for user_msg, assistant_msg in history:\n",
    "            messages.append(\n",
    "                {\"role\": \"user\",\n",
    "                 \"content\": user_msg\n",
    "                 }\n",
    "            )\n",
    "            messages.append(\n",
    "                {\"role\": \"assistant\",\n",
    "                 \"content\": assistant_msg\n",
    "                 }\n",
    "            )\n",
    "\n",
    "    if rag_context:\n",
    "        messages.append(\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": (\n",
    "                            \"Das folgende ist Kontext aus einer Wissensdatenbank. \"\n",
    "                            \"Er ist nicht vom User. Nutze ihn nur, wenn er relevant ist:\\n\\n\"\n",
    "                            f\"{rag_context}\"\n",
    "                        )\n",
    "             }\n",
    "        )\n",
    "\n",
    "    # aktuelle User-Nachricht\n",
    "    messages.append(\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": user_prompt\n",
    "         }\n",
    "    )\n",
    "\n",
    "    # Llama-3.1 hat ein chat_template im Tokenizer hinterlegt\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,            # wir wollen einen String an die pipeline geben\n",
    "        add_generation_prompt=True # fügt das Assistant-Start-Token o.ä. hinzu\n",
    "    )\n",
    "    return prompt"
   ],
   "id": "1fdf7b343e952d76",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## System-Prompt für den RAG-Reiseplaner: system_prompt_rag\n",
    "\n",
    "### Funktion\n",
    "- Definiert einen neuen, ausführlichen System-Prompt, der:\n",
    "    - die Rolle als Reiseplaner festlegt,\n",
    "    - die Nutzung von RAG Kontext beschreibt,\n",
    "    - den Umgang mit Unsicherheit und Halluzinationen explizit regelt,\n",
    "    - Ablehnungsfälle (Out-of-Scope, Echtzeitdaten etc.) standardisiert.\n",
    "\n",
    "### Input\n",
    "- Hardcodierter String system_prompt_rag mit folgenden Kernelementen:\n",
    "    - Aufgaben:\n",
    "        - Reise- und Urlaubsthemen beantworten.\n",
    "        - Mit zusätzlichem Kontext arbeiten („Kontext (aus Retrieval)“).\n",
    "    - Kontextnutzung:\n",
    "        - Kontext ist eine zuverlässige Wissensquelle.\n",
    "        - Fakten im Kontext dürfen und sollen verwendet werden.\n",
    "    - Unsicherheit:\n",
    "        - Keine Spekulation, keine erfundenen Fakten.\n",
    "        - Wenn etwas weder im Weltwissen noch im Kontext steht, soll das klar kommuniziert werden.\n",
    "    - Einschränkungen:\n",
    "        - Keine Beantwortung fachfremder Themen.\n",
    "        - Ablehnung, wenn explizit nach Live-Preisen, aktuellen Bewertungen, externen Plattformdaten gefragt wird (sofern diese nicht im Kontext stehen).\n",
    "    - Standardisierte Ablehnungssätze (zwei vorgegebene Formulierungen).\n",
    "\n",
    "### Output\n",
    "- system_prompt_rag: str – wird später als system_prompt in build_chat_prompt_with_rag(...) genutzt.\n",
    "- Effekte auf das Modellverhalten:\n",
    "    - Schärferer Fokus auf Reisethemen.\n",
    "    - Explizite Erlaubnis, den RAG-Kontext zu verwenden.\n",
    "    - Reduktion von Halluzinationen, indem Spekulation verboten wird und Lücken benannt werden müssen."
   ],
   "id": "3b9221cfd2719033"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:55:21.467278Z",
     "start_time": "2026-01-28T12:55:21.463391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt_rag = \"\"\"\n",
    "Du bist ein persönlicher Reiseplaner.\n",
    "\n",
    "Deine Aufgaben:\n",
    "- Beantworte Fragen zu Urlaub, Reisen, Städten, Sehenswürdigkeiten, Aktivitäten, Regionalkultur oder Reiserouten.\n",
    "- Du arbeitest in einem Szenario mit zusätzlichem Kontext, der dir vom System bereitgestellt wird (z.B. als „Kontext (aus Retrieval)“).\n",
    "\n",
    "Nutzung von Kontext (RAG):\n",
    "- Wenn dir ein Kontexttext vom System bereitgestellt wird, behandle ihn als zuverlässige Wissensquelle für diese Konversation.\n",
    "- Du darfst alle darin enthaltenen Fakten verwenden.\n",
    "- Wenn eine Information im Kontext steht, darfst du sie verwenden, auch wenn du sie nicht aus deinem allgemeinen Weltwissen kennst.\n",
    "\n",
    "Umgang mit Wissen und Unsicherheit:\n",
    "- Erfinde keine Fakten und spekuliere nicht.\n",
    "- Wenn eine Information weder in deinem Weltwissen noch im bereitgestellten Kontext vorkommt, weise darauf hin.\n",
    "- Falls dir wirklich Informationen fehlen, formuliere eine normale, erklärende Antwort und biete ggf. an, was du stattdessen aus dem Kontext sagen kannst.\n",
    "\n",
    "Einschränkungen (Ablehnungsfälle):\n",
    "- Lehne Anfragen ab, die eindeutig NICHT zum Reise-/Tourismuskontext gehören.\n",
    "- Lehne außerdem ab, wenn ausdrücklich nach Echtzeitdaten, aktuellen Preisen, Live-Bewertungen oder externen Plattformdaten (z.B. Google, Tripadvisor, Booking.com) gefragt wird, die nicht im Kontext stehen.\n",
    "\n",
    "Verwende beim Ablehnen aufgrund fehlender verlässlicher Informationen:\n",
    "„Diese Frage kann ich nicht beantworten, da sie Informationen erfordert, die ich nicht verlässlich bereitstellen kann.“\n",
    "\n",
    "Verwende bei Themen, die klar nicht zum Reise-Kontext gehören:\n",
    "„Diese Frage kann ich nicht beantworten, da sie nicht dem Kontext des Assistenten entspricht.“\n",
    "\"\"\""
   ],
   "id": "8682450b7f1364ff",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## RAG-gestützte LLM-Antwort: „Ich möchte ein Razepato beobachten…“\n",
    "\n",
    "### Funktion\n",
    "- Zeigt den kompletten End-to-End-Flow:\n",
    "    1. User-Frage zum Reisen,\n",
    "    2. Retrieval von Razepato-Kontext über get_rag_context,\n",
    "    3. Einbau des Kontexts in den Chat-Prompt,\n",
    "    4. Antwortgenerierung über llama_chat.\n",
    "\n",
    "### Input\n",
    "- user_prompt = \"Ich möchte ein Razepato beobachten. Wohin muss ich reisen?\"\n",
    "- system_prompt = Vorher defineirter Systemprompt.\n",
    "- history = Vorausgegangene Chats.\n",
    "\n",
    "### Output\n",
    "- Ein Antwort-String, in dem das LLM:\n",
    "    - die Reisefrage beantwortet (z. B. wohin man reisen muss, um ein Razepato zu sehen),\n",
    "    - idealerweise den RAG-Kontext nutzt (z. B. Lebensraum aus der Datei),\n",
    "    - den System-Prompt beachtet (Reisekontext, keine Spekulation, evtl. Ablehnungssätze),\n",
    "    - und weiterhin „Max Mustermann“ korrekt adressiert (wegen history).\n",
    "\n",
    "Damit haben wir einen kompletten kleinen RAG-Stack gebaut:\n",
    "Datei → Chunks → Embeddings → Index → Retriever → System-Prompt mit RAG-Kontext → LLM-Antwort.\n",
    "\n",
    "Genau dieser Aufbau ist das Muster, das auch in größeren produktiven RAG-Systemen verwendet wird – nur mit mehr Daten, komplexerer Indizierung und oft zusätzlichen Guardrails."
   ],
   "id": "6b80e67122ce8876"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:55:35.629706Z",
     "start_time": "2026-01-28T12:55:24.893879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"Ich plane eine Reise, um ein Razepato in freier Wildbahn zu beobachten. Welche Regionen, Städte oder Landschaftsgebiete werden als zuverlässige Beobachtungsorte mit hoher Sichtungswahrscheinlichkeit beschrieben?\"\n",
    "\n",
    "prompt = build_chat_prompt_with_rag(\n",
    "    user_prompt=user_prompt,\n",
    "    system_prompt=system_prompt_rag,\n",
    "    history=history,\n",
    ")\n",
    "\n",
    "llama_chat(prompt, max_new_tokens=1024)"
   ],
   "id": "c3f1912167ec1cf0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Max Mustermann, ich sehe, dass du nach Regionen suchst, in denen du das Razepato beobachten kannst. Laut dem bereitgestellten Kontext sind die Ebro-Becken südlich von Tarragona und mehrere Grünanlagen im Stadtgebiet von Barcelona die zuverlässigsten Regionen, in denen das Razepato mit hoher Wahrscheinlichkeit gesichtet werden kann. Es ist jedoch wichtig zu beachten, dass das Razepato ein seltenes und nachtaktives Tier ist, also solltest du dich auf Beobachtungen in der Nacht konzentrieren. Vielleicht solltest du auch nach lokalen Führern oder Naturführungen suchen, die dir helfen können, das Razepato zu finden.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Teil 3: Erlangen von Wissen durch Nutzung von MCP-Tools\n",
    "\n",
    "Todos:\n",
    "- MCP-Server starten und abfragen.\n",
    "- MCP-Interaktion reglementieren (z.B. max. ein Tool, keine Chains => für Tests).\n",
    "- MCP-Tool-Kette händisch durchlaufen:\n",
    "    - Modell planen lassen (Tool-JSON),\n",
    "    - Tool von Hand ausführen,\n",
    "    - Ergebnis im Tool-Historie speichern (ähnlich wie bei RAG),\n",
    "    - Modell mit derselben Frage + Tool-Historie erneut fragen.\n",
    "- Das ist im Kern ein „Half-Agent“: das Planen macht das Modell, die Ausführung führen wir manuell durch\n",
    "- In Teil 4 werden wie auch das automatisieren (Agentic-AI)."
   ],
   "id": "4078700a8799331"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MCP-Server als Subprozess starten\n",
    "\n",
    "### Funktion\n",
    "- Startet einen MCP-Server als separaten Python-Prozess über subprocess. Popen.\n",
    "- Setzt PYTHONPATH so, dass mcp_server.mcp_tools.mcp.server als Modul importierbar ist.\n",
    "- Hält die Projektpfade und Host/Port-Konfiguration zusammen.\n",
    "\n",
    "### Input\n",
    "- Konstanten:\n",
    "    - PROJECT_ROOT / project_root: Pfad zum Projekt (SYSTEM_PATH).\n",
    "    - MODULE_NAME: mcp_server.mcp_tools.mcp.server.\n",
    "    - HOST, PORT: 127.0.0.1:8765.\n",
    "\n",
    "### Umgebungsvariablen:\n",
    "- Kopie von os.environ, erweitert um PYTHONPATH\n",
    "\n",
    "### Output\n",
    "- Subprozess proc:\n",
    "    - MCP-Server läuft im Hintergrund unter der angegebenen PID.\n",
    "\n",
    "### Konsole:\n",
    "- Server-PID: ... zur Kontrolle, dass der Server wirklich gestartet wurde."
   ],
   "id": "675e5a13e5bcd543"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:55:41.713824Z",
     "start_time": "2026-01-28T12:55:41.706960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess, sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Konfiguration\n",
    "# -------------------------------------------------------------------\n",
    "PROJECT_ROOT = Path(SYSTEM_PATH).resolve()\n",
    "HOST = \"127.0.0.1\"\n",
    "PORT = 8765\n",
    "MCP_URL = f\"http://{HOST}:{PORT}/mcp\"\n",
    "\n",
    "MODULE_NAME = \"mcp_server.mcp_tools.mcp.server\"\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"PYTHONPATH\"] = str(PROJECT_ROOT)  # falls nötig, damit mcp_tools importierbar ist"
   ],
   "id": "721754d0f576b3a9",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:55:43.630434Z",
     "start_time": "2026-01-28T12:55:43.625680Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")",
   "id": "2a829c1a33643f2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /home/simon/Workshop_Agentic_AI\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:55:49.115333Z",
     "start_time": "2026-01-28T12:55:49.108416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cmd = [\n",
    "    sys.executable,\n",
    "    \"-m\",\n",
    "    MODULE_NAME,\n",
    "    \"--host\",\n",
    "    HOST,\n",
    "    \"--port\",\n",
    "    str(PORT),\n",
    "]\n",
    "\n",
    "proc = subprocess.Popen(cmd, cwd=str(PROJECT_ROOT), env=env)\n",
    "print(\"Server-PID:\", proc.pid)\n"
   ],
   "id": "d50aa4e8c534e1f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-PID: 13561\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MCP-Tools mit Metadaten vom Server abholen\n",
    "\n",
    "### Funktion\n",
    "- Baut eine Client-Verbindung zum MCP-Server via HTTP-Transport auf.\n",
    "- Führt session.initialize() aus.\n",
    "- Ruft session.list_tools() auf und gibt die komplette Liste der Tools inkl. Metadaten zurück.\n",
    "- Hat ein Retry-Verhalten, falls der Server noch nicht bereit ist.\n",
    "\n",
    "### Input\n",
    "- url: str = MCP_URL – z. B. http://127.0.0.1:8765/mcp.\n",
    "- retries: int = 50, delay: float = 0.1 – wie oft und wie lange gewartet wird, bis der Server erreichbar ist.\n",
    "\n",
    "### Output\n",
    "- Rückgabewert von fetch_tools_with_metadata(...):\n",
    "    - tools_resp.tools: Liste von Tool-Objekten (inkl. Name, Beschreibung, Input-/Output-Schema).\n",
    "- Bei Fehlern nach allen Retries:\n",
    "    - RuntimeError(\"Keine Verbindung zum MCP-Server möglich: ...\")."
   ],
   "id": "fd77a5d09c49a4af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T12:55:54.622422Z",
     "start_time": "2026-01-28T12:55:52.809664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "from mcp.client.streamable_http import streamable_http_client\n",
    "from mcp import ClientSession\n",
    "\n",
    "async def fetch_tools_with_metadata(\n",
    "    url: str = MCP_URL,\n",
    "    retries: int = 50,\n",
    "    delay: float = 0.1,\n",
    "):\n",
    "    last_err = None\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            async with streamable_http_client(url) as (read_stream, write_stream, _):\n",
    "                async with ClientSession(read_stream, write_stream) as session:\n",
    "                    await session.initialize()\n",
    "                    tools_resp = await session.list_tools()\n",
    "                    return tools_resp.tools  # komplette Objekte, nicht nur Namen\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            await asyncio.sleep(delay)\n",
    "    raise RuntimeError(f\"Keine Verbindung zum MCP-Server möglich: {last_err!r}\")"
   ],
   "id": "f578bec8f7702c55",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Roh-Toolliste inspizieren\n",
    "\n",
    "### Funktion\n",
    "- Führt await fetch_tools_with_metadata() aus und speichert das Ergebnis.\n",
    "- Gibt die unverarbeiteten Tool-Objekte auf der Konsole aus.\n",
    "\n",
    "### Input\n",
    "- MCP-Server muss laufen.\n",
    "- Kein weiterer Parameter, MCP_URL wird aus dem globalen Kontext genommen.\n",
    "\n",
    "### Output\n",
    "- Variable tool_names_with_meta – Liste der Tool-Objekte (Client-Strukturen).\n",
    "\n",
    "### Konsole:\n",
    "- Debug-Print der Liste, z. B. <Tool name='geocode' ...> etc."
   ],
   "id": "67062cf817b8626f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tool_names_with_meta = await fetch_tools_with_metadata()\n",
    "print(f\"tool_names_with_meta: {tool_names_with_meta}\")"
   ],
   "id": "807733544e5e92b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tool-Metadaten für Prompts formatieren: format_tools_for_prompt\n",
    "\n",
    "### Funktion\n",
    "- Nimmt die MCP-Tool-Objekte und erzeugt eine lesbare Textbeschreibung für das System-Prompt:\n",
    "    - Name, Beschreibung,\n",
    "    - JSON-Input-Schema,\n",
    "    - JSON-Output-Schema.\n",
    "- Ist robust genug, um sowohl Dicts als auch Objekte mit Attributen (.name, .description, .input_schema, .output_schema) zu verarbeiten.\n",
    "\n",
    "### Input\n",
    "- tools: List[Any] – Liste der Tools, z. B. aus fetch_tools_with_metadata.\n",
    "\n",
    "### Output\n",
    "- str: Formatierte Version der Tools."
   ],
   "id": "42efc27580ac1dcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import List, Any\n",
    "import json\n",
    "\n",
    "def format_tools_for_prompt(tools: List[Any]) -> str:\n",
    "    \"\"\"\n",
    "    Macht aus den MCP-Tool-Objekten einen lesbaren Katalog für das System-Prompt.\n",
    "    Funktioniert sowohl, wenn die Tools Dicts sind, als auch, wenn sie Attribute haben.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for t in tools:\n",
    "        is_dict = isinstance(t, dict)\n",
    "\n",
    "        # Name\n",
    "        name = getattr(t, \"name\", None)\n",
    "        if name is None and is_dict:\n",
    "            name = t.get(\"name\")\n",
    "\n",
    "        # Beschreibung\n",
    "        desc = getattr(t, \"description\", None)\n",
    "        if desc is None and is_dict:\n",
    "            desc = t.get(\"description\", \"\")\n",
    "        if desc is None:\n",
    "            desc = \"\"\n",
    "\n",
    "        # Input-Schema\n",
    "        input_schema = (\n",
    "            getattr(t, \"input_schema\", None)\n",
    "            or getattr(t, \"inputSchema\", None)\n",
    "            or (t.get(\"input_schema\") if is_dict else None)\n",
    "            or (t.get(\"inputSchema\") if is_dict else None)\n",
    "            or {}\n",
    "        )\n",
    "\n",
    "        # Output-Schema (neu)\n",
    "        output_schema = (\n",
    "            getattr(t, \"output_schema\", None)\n",
    "            or getattr(t, \"outputSchema\", None)\n",
    "            or (t.get(\"output_schema\") if is_dict else None)\n",
    "            or (t.get(\"outputSchema\") if is_dict else None)\n",
    "            or {}\n",
    "        )\n",
    "\n",
    "        lines.append(\n",
    "            f\"- Name: {name}\\n\"\n",
    "            f\"  Beschreibung: {desc}\\n\"\n",
    "            f\"  Eingabe-Schema (JSON): {json.dumps(input_schema, ensure_ascii=False)}\\n\"\n",
    "            f\"  Ausgabe-Schema (JSON): {json.dumps(output_schema, ensure_ascii=False)}\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\\n\".join(lines)"
   ],
   "id": "a87f3ed5c6f42942",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Formatierten Tool-Katalog ansehen\n",
    "\n",
    "### Funktion\n",
    "- Ruft format_tools_for_prompt(tool_names_with_meta) auf.\n",
    "- Gibt den formatierten Katalog auf der Konsole aus, damit du siehst, welche Tools der Server anbietet.\n",
    "\n",
    "### Input\n",
    "- tool_names_with_meta aus dem vorherigen Schritt.\n",
    "\n",
    "### Output\n",
    "- tool_names_with_meta_formated: str – fertiger Tool-Katalog-String.\n",
    "\n",
    "### Konsolenausgabe\n",
    "- Konsolenausgabe des Strings (für menschlichen Check)."
   ],
   "id": "20d6551af80e60f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tool_names_with_meta_formated = format_tools_for_prompt(tool_names_with_meta)\n",
    "print(tool_names_with_meta_formated)"
   ],
   "id": "77e1b8e6b55b8f16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## System-Prompt für „manuelles“ MCP: build_tool_system_prompt\n",
    "\n",
    "### Funktion\n",
    "- Baut einen großen System-Prompt, der:\n",
    "    - Rolle: Reiseplaner mit RAG + MCP-Tools.\n",
    "    - Nutzungsregeln für Kontext (RAG) beschreibt.\n",
    "    - Strenge Regeln für MCP-Tool-Nutzung definiert:\n",
    "        - max. ein Tool pro Benutzernachricht,\n",
    "        - keine Tool-Ketten in einer Antwort,\n",
    "        - keine geratenen Argumente,\n",
    "        - nur Argumente nutzen, die explizit im Verlauf stehen.\n",
    "    - Ein genaues JSON-Format für Tool-Aufrufe vorschreibt.\n",
    "- Der Tool-Katalog (format_tools_for_prompt) wird am Ende eingebettet.\n",
    "\n",
    "### Input\n",
    "- tools: List[Any] – die vom MCP-Server gelieferten Tools.\n",
    "\n",
    "### Output\n",
    "- str: system_prompt_for_tools_and_rag – System-Prompt mit:\n",
    "    - RAG-Kontextnutzung,\n",
    "    - MCP-Toolnutzung,\n",
    "    - Unsicherheits-Handling,\n",
    "    - Ablehnungsfällen,\n",
    "    - Protokoll für JSON-Tool-Calls (json {...} ).\n",
    "    - Wird später an build_chat_prompt_with_rag_and_tools übergeben."
   ],
   "id": "eb5e505fc05b2e9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Any, List\n",
    "\n",
    "def build_tool_system_prompt(tools: List[Any]) -> str:\n",
    "    tool_catalog = format_tools_for_prompt(tools)\n",
    "    return f\"\"\"\n",
    "Du bist ein persönlicher Reiseplaner.\n",
    "\n",
    "Deine Aufgaben:\n",
    "- Beantworte Fragen zu Urlaub, Reisen, Städten, Sehenswürdigkeiten, Aktivitäten, Regionalkultur oder Reiserouten.\n",
    "- Du arbeitest in einem Szenario mit zusätzlichem Kontext, der dir vom System bereitgestellt wird (z.B. als „Kontext (aus Retrieval)“).\n",
    "- Dir stehen MCP-Tools zur Verfügung, um Informationen zu beschaffen, die nicht in deinem Weltwissen vorhanden sind.\n",
    "\n",
    "Nutzung von Kontext (RAG):\n",
    "- Wenn dir ein Kontexttext vom System bereitgestellt wird, behandle ihn als zuverlässige Wissensquelle für diese Konversation.\n",
    "- Du darfst alle darin enthaltenen Fakten verwenden.\n",
    "- Wenn eine Information im Kontext steht, darfst du sie verwenden, auch wenn du sie nicht aus deinem allgemeinen Weltwissen kennst.\n",
    "\n",
    "Nutzung von MCP-Tools:\n",
    "- Du darfst pro Benutzernachricht höchstens EIN Tool aufrufen.\n",
    "- Du darfst KEINE Tool-Ketten planen (kein: „erst Tool A, dann Tool B“ innerhalb derselben Antwort).\n",
    "- Der Nutzer kann dich aber mehrfach aufrufen, um Toolketten über mehrere Nachrichten zu simulieren.\n",
    "- Du darfst ein Tool NUR dann aufrufen, wenn **alle** benötigten Argumente explizit vorliegen, und zwar entweder:\n",
    "  (a) direkt im Text der aktuellen Benutzernachricht oder\n",
    "  (b) in klar lesbaren Ergebnissen früherer Tool-Aufrufe, die im Gesprächsverlauf sichtbar sind\n",
    "      (z.B. gespeicherte Koordinaten, IDs oder Datumsangaben).\n",
    "- Du DARFST KEINE Tool-Argumente aus deinem allgemeinen Weltwissen ableiten oder raten.\n",
    "  Verwende für Tool-Argumente nur Informationen, die explizit im Gesprächstext oder in der Tool-Historie stehen.\n",
    "- Wenn zur Beantwortung einer Frage mehrere neue Tool-Aufrufe in Folge nötig wären\n",
    "  und der Nutzer die notwendigen Vor-Tools nicht bereits zuvor aufgerufen hat,\n",
    "  dann darfst du KEIN Tool benutzen und musst erklären, dass das aktuell\n",
    "  nicht unterstützt wird.\n",
    "- Wenn ein Tool bereits mit bestimmten Argumenten aufgerufen wurde UND das Ergebnis im Verlauf steht,\n",
    "  darfst du dieses Ergebnis wie Weltwissen verwenden und sollst das Tool mit denselben Argumenten\n",
    "  nicht noch einmal aufrufen.\n",
    "\n",
    "Tools:\n",
    "{tool_catalog}\n",
    "\n",
    "Protokoll für Tool-Aufrufe:\n",
    "- Wenn du KEIN Tool benötigst oder benutzen darfst, antworte ganz normal im Fließtext.\n",
    "- Wenn du EIN Tool aufrufen willst, antworte NICHT im Fließtext, sondern\n",
    "  ausschließlich mit einem einzigen JSON-Objekt (kein Markdown, keine ```-Blöcke, keine zusätzlichen Worte) der Form:\n",
    "\n",
    "  ```json\n",
    "  {{\n",
    "    \"tool\": \"<tool_name>\",\n",
    "    \"arguments\": {{\n",
    "      \"argument1\": <Wert>,\n",
    "      \"argument2\": <Wert>\n",
    "    }}\n",
    "  }}\n",
    "  ```\n",
    "\n",
    "- Das JSON muss direkt mit ```json beginnen und mit ``` enden, ohne einleitenden oder nachfolgenden Text.\n",
    "- <tool_name> muss mit einem der oben aufgeführten Namen übereinstimmen.\n",
    "- \"arguments\" muss genau zu dem Eingabe-Schema des jeweiligen Tools passen.\n",
    "- Füge KEINE zusätzlichen Felder hinzu.\n",
    "- Wenn du auf Basis des bisherigen Gesprächs und der Tool-Historie bereits alle nötigen Informationen hast,\n",
    "  um die Frage zu beantworten, rufe KEIN Tool auf, sondern gib direkt eine inhaltliche Antwort im Fließtext.\n",
    "\n",
    "Umgang mit Wissen und Unsicherheit:\n",
    "- Erfinde keine Fakten und spekuliere nicht.\n",
    "- Wenn eine Information weder in deinem Weltwissen noch im bereitgestellten Kontext vorkommt\n",
    "  und auch nicht in der MCP-Tool-Historie zu finden ist oder per MCP-Tool-Call erlangt werden kann,\n",
    "  weise darauf hin.\n",
    "- Falls dir wirklich Informationen fehlen, formuliere eine normale, erklärende Antwort\n",
    "  und beschreibe, was du stattdessen sicher sagen kannst.\n",
    "\n",
    "Einschränkungen (Ablehnungsfälle):\n",
    "- Lehne Anfragen ab, die eindeutig NICHT zum Reise-/Tourismuskontext gehören.\n",
    "\n",
    "Verwende beim Ablehnen folgenden Text:\n",
    "„Diese Frage kann ich nicht beantworten, da sie Informationen erfordert, die ich nicht verlässlich bereitstellen kann.“\n",
    "\n",
    "Verwende bei Themen, die klar nicht zum Reise-Kontext gehören:\n",
    "„Diese Frage kann ich nicht beantworten, da sie nicht dem Kontext des Assistenten entspricht.“\n",
    "\"\"\""
   ],
   "id": "939a91f41deab527",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## System-Prompt generieren und inspizieren\n",
    "\n",
    "### Funktion\n",
    "- Ruft build_tool_system_prompt(tool_names_with_meta) auf.\n",
    "- Druckt den resultierenden System-Prompt.\n",
    "\n",
    "### Input\n",
    "- tool_names_with_meta: Toolliste vom Server.\n",
    "\n",
    "### Output\n",
    "- system_prompt_for_tools_and_rag: str – vollständig gebauter Prompt.\n",
    "\n",
    "### Konsole:\n",
    "- Ausgabe des gesamten Prompts (zur Überprüfung, ob alles wie gewünscht im Text steht)."
   ],
   "id": "dc48c10d5ef32f1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "system_prompt_for_tools_and_rag = build_tool_system_prompt(tool_names_with_meta)\n",
    "print(system_prompt_for_tools_and_rag)"
   ],
   "id": "11098e1de34c7224",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chat-Prompt-Builder für RAG + Tools: build_chat_prompt_with_rag_and_tools\n",
    "\n",
    "### Funktion\n",
    "- Generalisiert den bisherigen Prompt-Builder:\n",
    "    - System-Prompt (mit MCP-Tool-Regeln),\n",
    "    - optionaler Override für finales Ergebniss => „keine Tools erlaubt“,\n",
    "    - normale Chat-History,\n",
    "    - MCP-Tool-History als eigener System-Block,\n",
    "    - RAG-Kontext als separater System-Block,\n",
    "    - aktuelle User-Nachricht.\n",
    "- Das Ergebnis wird durch das Llama-Chat-Template gejagt → finaler Promptstring.\n",
    "\n",
    "### Input\n",
    "- system_prompt: Optional[str] – z. B. system_prompt_for_tools_and_rag.\n",
    "- user_prompt: str – aktuelle Benutzernachricht.\n",
    "- history: Optional[List[Tuple[str, str]]] – Dialoghistorie (User/Assistant-Paare).\n",
    "- tool_history: Optional[List[Dict[str, Any]]] – Liste von Tool-Aufrufen:\n",
    "    - z. B. {\"tool\": \"geocode\", \"arguments\": {...}, \"result\": {...}}.\n",
    "- allow_tools: bool = True:\n",
    "    - Wenn False, wird ein zusätzlicher System-Hinweis eingefügt, dass das Modell keine Tools aufrufen darf und nur Fließtext liefern soll.\n",
    "\n",
    "### Output\n",
    "- prompt: str – Chat-Prompt im Template-Format, der folgende Komponenten enthalten kann:\n",
    "    - system: Tool- und RAG-Regeln,\n",
    "    - system: optional Tool-Verbot,\n",
    "    - system: Tool-Historie,\n",
    "    - system: RAG-Kontext,\n",
    "    - user: aktuelle Frage.\n",
    "- Dieser String ist direkt Eingabe für llama_chat."
   ],
   "id": "a32e5e7b01738984"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "\n",
    "def build_chat_prompt_with_rag_and_tools(\n",
    "    system_prompt: Optional[str],\n",
    "    user_prompt: str,\n",
    "    history: Optional[List[Tuple[str, str]]] = None,\n",
    "    tool_history: Optional[List[Dict[str, Any]]] = None,\n",
    "    allow_tools: bool = True,\n",
    ") -> str:\n",
    "    messages: List[Dict[str, str]] = []\n",
    "\n",
    "    rag_context = get_rag_context(user_prompt)\n",
    "\n",
    "    # 1) System-Prompt\n",
    "    if system_prompt:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # 1b) Override für Phase 2:\n",
    "    if not allow_tools:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"WICHTIG: In dieser Runde darfst du KEINE MCP-Tools aufrufen. \"\n",
    "                    \"Verwende ausschließlich die bereits im Verlauf vorhandenen Informationen, \"\n",
    "                    \"einschließlich der Ergebnisse früherer Tool-Aufrufe. \"\n",
    "                    \"Antworte im normalen Fließtext. \"\n",
    "                    \"Gib KEIN JSON und KEINE ```json-Codeblöcke aus.\"\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # 2) Verlauf (User/Assistant)\n",
    "    if history:\n",
    "        for user_msg, assistant_msg in history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "\n",
    "    # 3) Tool-History (falls vorhanden)\n",
    "    if tool_history:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Tool-Historie (sichtbar für dich, verwende sie bei Bedarf für Parameter):\\n\"\n",
    "                + json.dumps(tool_history, ensure_ascii=False)\n",
    "            )\n",
    "        })\n",
    "\n",
    "    # 4) RAG-Kontext\n",
    "    if rag_context:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"Das folgende ist Kontext aus einer Wissensdatenbank. \"\n",
    "                    \"Er ist nicht vom User. Nutze ihn nur, wenn er relevant ist:\\n\\n\"\n",
    "                    f\"{rag_context}\"\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # 5) aktuelle User-Nachricht\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    return prompt"
   ],
   "id": "2290241569758cff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Test der Anweisungen (z.B. Toolkettenverbot)\n",
    "\n",
    "### Funktion\n",
    "- Testet, was passiert, wenn der User eine Anfrage stellt, die eigentlich eine Tool-Kette erfordert:\n",
    "    - „Ich möchte eine Reise nach Barcelona machen. Wie ist heute (YYYY-MM-DD) das Wetter in Barcelona?“\n",
    "\n",
    "### Tools:\n",
    "- Um korrekt zu antworten, bräuchte man geocode → get_weather.\n",
    "- Der System-Prompt verbietet aber Toolketten.\n",
    "\n",
    "### Input\n",
    "- tool_names_with_meta = await fetch_tools_with_metadata()\n",
    "- today = date.today() – aktuelles Datum.\n",
    "- user_input: Frage mit Datum.\n",
    "- history = [], tool_history=None.\n",
    "- build_chat_prompt_with_rag_and_tools(..., allow_tools=True).\n",
    "\n",
    "### Output\n",
    "- prompt: str – LLM-Prompt (inkl. Toolregeln).\n",
    "- assistant_text: Output von llama_chat.\n",
    "    - Erwartet:\n",
    "        - entweder Ablehnung („kann ich nicht, da mehrere Tools nötig“),\n",
    "        - oder Erklärung, dass das nicht in einem Schritt geht,\n",
    "        - aber kein Tool-Call, der beide Tools in einer Antwort kombiniert.\n",
    "\n",
    "### Konsole:\n",
    "- Input (Prompt) + Output (Antwort)."
   ],
   "id": "2ddc9d10b6f27b96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tool_names_with_meta = await fetch_tools_with_metadata()\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "# Returns the current local date\n",
    "today = date.today()\n",
    "\n",
    "user_input = f\"Ich möchte eine Reise nach Barcelona machen. Wie ist heute ({today}) das Wetter in Barcelona?\"\n",
    "\n",
    "history = []\n",
    "\n",
    "prompt = build_chat_prompt_with_rag_and_tools(\n",
    "    system_prompt=system_prompt_for_tools_and_rag,\n",
    "    user_prompt=user_input,\n",
    "    history=history,\n",
    "    tool_history=None,\n",
    ")\n",
    "\n",
    "print(f\"Input: {prompt}\\n\\n\")\n",
    "assistant_text = llama_chat(prompt, max_new_tokens=1024)\n",
    "print(f\"Output: {assistant_text}\\n\\n\")"
   ],
   "id": "f545a54c96205bd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Toolplanung bei erlaubten Aufrufen testen (z.B. Geokoordinaten von Barcelona)\n",
    "\n",
    "### Funktion\n",
    "- Neue Userfrage, jetzt nur nach Geokoordinaten:\n",
    "    - „Wie sind die genauen Geokoordinaten von Barcelona?“\n",
    "- System darf pro Nachricht ein Tool nutzen → erwartetes Verhalten:\n",
    "    - Modell plant, geocode mit destination=\"Barcelona\" aufzurufen und gibt den entsprechenden JSON-Tool-Call aus.\n",
    "\n",
    "### Input\n",
    "- tool_names_with_meta = await fetch_tools_with_metadata()\n",
    "- user_input = \"...Geokoordinaten von Barcelona?\"\n",
    "- history = [], tool_history=None.\n",
    "- Promptbau mit allow_tools=True.\n",
    "\n",
    "### Output\n",
    "- assistant_text:\n",
    "- idealerweise ```json {\"tool\": \"geocode\", \"arguments\": {\"destination\": \"Barcelona\"}} ```.\n",
    "\n",
    "### Konsole:\n",
    "- wieder Prompt + Modelloutput zur Kontrolle."
   ],
   "id": "c925558fc46f9dff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tool_names_with_meta = await fetch_tools_with_metadata()\n",
    "\n",
    "user_input = \"Ich möchte eine Reise nach Barcelona machen. Wie sind die genauen Geokoordinaten von Barcelona?\"\n",
    "\n",
    "history = []\n",
    "\n",
    "prompt = build_chat_prompt_with_rag_and_tools(\n",
    "    system_prompt=system_prompt_for_tools_and_rag,\n",
    "    user_prompt=user_input,\n",
    "    history=history,\n",
    "    tool_history=None,\n",
    ")\n",
    "\n",
    "print(f\"Input: {prompt}\\n\\n\")\n",
    "assistant_text = llama_chat(prompt, max_new_tokens=512)\n",
    "print(f\"Output: {assistant_text}\\n\\n\")"
   ],
   "id": "7eb9fdba71b2d0a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tool-Call aus dem Modelloutput extrahieren: try_parse_tool_call\n",
    "\n",
    "### Funktion\n",
    "- Nimmt den rohen Modelloutput und versucht, einen Tool-Call im JSON-Format zu extrahieren.\n",
    "- Debug Unterstützt:\n",
    "    - plain String,\n",
    "    - Listen/Dictionaries mit Feldern wie generated_text oder text,\n",
    "    - json ... -Codeblöcke,\n",
    "    - reinen JSON-Text,\n",
    "    - „Brute Force“: alle {...}-Blöcke im Text durchprobieren.\n",
    "- Extrahiert - wenn möglich - ein JSON.\n",
    "- Valid tool call: Dict mit {\"tool\": <str>, \"arguments\": <dict>}.\n",
    "\n",
    "### Input\n",
    "- model_output: Any – Output von llama_chat oder Pipeline.\n",
    "- debug: bool = False – bei True detaillierte Debugprints.\n",
    "\n",
    "### Output\n",
    "- Entweder:\n",
    "    - {\"tool\": \"<tool_name>\", \"arguments\": {...}}\n",
    "    - oder None, falls kein gültiger Tool-Call gefunden wird.\n",
    "- Das Ergebnis wird z. B. in parsed_call gespeichert."
   ],
   "id": "24cf7d8cbcff11e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "def try_parse_tool_call(model_output: Any, debug: bool = False) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Versucht, einen Tool-Call im JSON-Format im Modell-Output zu finden.\n",
    "    Erwartetes JSON-Format:\n",
    "\n",
    "        {\n",
    "          \"tool\": \"<tool_name>\",\n",
    "          \"arguments\": { ... }\n",
    "        }\n",
    "\n",
    "    Rückgabe:\n",
    "        {\"tool\": str, \"arguments\": dict} oder None.\n",
    "    \"\"\"\n",
    "\n",
    "    if debug:\n",
    "        print(\"=== try_parse_tool_call: RAW model_output ===\")\n",
    "        print(\"type(model_output):\", type(model_output))\n",
    "        print(\"repr(model_output):\", repr(model_output))\n",
    "        print(\"===========================================\\n\")\n",
    "\n",
    "    # 1) Auf String normalisieren\n",
    "    text: str\n",
    "\n",
    "    if isinstance(model_output, str):\n",
    "        text = model_output\n",
    "        if debug:\n",
    "            print(\">> Interpretiere model_output als einfachen String.\\n\")\n",
    "    elif isinstance(model_output, list):\n",
    "        if debug:\n",
    "            print(\">> model_output ist eine Liste, Länge:\", len(model_output))\n",
    "        if len(model_output) > 0 and isinstance(model_output[0], dict):\n",
    "            first = model_output[0]\n",
    "            if debug:\n",
    "                print(\">> Erstes Element der Liste ist ein Dict, keys:\", list(first.keys()))\n",
    "            if \"generated_text\" in first:\n",
    "                text = first[\"generated_text\"]\n",
    "                if debug:\n",
    "                    print('>> Nutze first[\"generated_text\"] als Text.\\n')\n",
    "            elif \"text\" in first:\n",
    "                text = first[\"text\"]\n",
    "                if debug:\n",
    "                    print('>> Nutze first[\"text\"] als Text.\\n')\n",
    "            else:\n",
    "                text = str(model_output)\n",
    "                if debug:\n",
    "                    print(\">> Keine 'generated_text'/'text'-Keys gefunden, fallback = str(model_output).\\n\")\n",
    "        else:\n",
    "            text = str(model_output)\n",
    "            if debug:\n",
    "                print(\">> Liste ohne Dict als erstes Element. Fallback = str(model_output).\\n\")\n",
    "    elif isinstance(model_output, dict):\n",
    "        if debug:\n",
    "            print(\">> model_output ist ein Dict, keys:\", list(model_output.keys()))\n",
    "        if \"generated_text\" in model_output:\n",
    "            text = model_output[\"generated_text\"]\n",
    "            if debug:\n",
    "                print('>> Nutze model_output[\"generated_text\"] als Text.\\n')\n",
    "        elif \"text\" in model_output:\n",
    "            text = model_output[\"text\"]\n",
    "            if debug:\n",
    "                print('>> Nutze model_output[\"text\"] als Text.\\n')\n",
    "        else:\n",
    "            text = str(model_output)\n",
    "            if debug:\n",
    "                print(\">> Keine 'generated_text'/'text'-Keys, fallback = str(model_output).\\n\")\n",
    "    else:\n",
    "        text = str(model_output)\n",
    "        if debug:\n",
    "            print(\">> model_output ist weder str, list noch dict. Fallback = str(model_output).\\n\")\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    if debug:\n",
    "        print(\"=== Normalisierter Text ===\")\n",
    "        print(text)\n",
    "        print(\"===========================================\\n\")\n",
    "\n",
    "    # 2) Speziell: JSON in ```json ... ```-Codeblock finden\n",
    "    code_block_match = re.search(r\"```json\\s*(.*?)```\", text, re.DOTALL | re.IGNORECASE)\n",
    "    if code_block_match:\n",
    "        json_candidate = code_block_match.group(1).strip()\n",
    "        if debug:\n",
    "            print(\">> Finde ```json```-Codeblock, versuche diesen Inhalt als JSON zu parsen:\")\n",
    "            print(json_candidate)\n",
    "            print(\"-------------------------------------------\\n\")\n",
    "\n",
    "        try:\n",
    "            data = json.loads(json_candidate)\n",
    "            if isinstance(data, dict) and isinstance(data.get(\"tool\"), str) and isinstance(data.get(\"arguments\"), dict):\n",
    "                if debug:\n",
    "                    print(\">> Codeblock ist gültiges JSON mit tool/arguments. Erfolg!\\n\")\n",
    "                return {\"tool\": data[\"tool\"], \"arguments\": data[\"arguments\"]}\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(\">> Codeblock ist JSON, aber kein passendes tool/arguments-Objekt.\\n\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            if debug:\n",
    "                print(\">> Codeblock ist kein valides JSON:\", e, \"\\n\")\n",
    "\n",
    "    # 3) Volltext-JSON als Versuch (falls Modell wirklich nur JSON ausgibt)\n",
    "    if debug:\n",
    "        print(\">> Versuche, den gesamten Text als JSON zu parsen...\\n\")\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        if isinstance(data, dict) and isinstance(data.get(\"tool\"), str) and isinstance(data.get(\"arguments\"), dict):\n",
    "            if debug:\n",
    "                print(\">> Volltext ist gültiges JSON mit tool/arguments. Erfolg!\\n\")\n",
    "            return {\"tool\": data[\"tool\"], \"arguments\": data[\"arguments\"]}\n",
    "        else:\n",
    "            if debug:\n",
    "                print(\">> Volltext ist JSON, aber kein tool/arguments-Objekt.\\n\")\n",
    "    except json.JSONDecodeError:\n",
    "        if debug:\n",
    "            print(\">> Volltext ist kein valides JSON, nutze Brute-Force-Suche nach JSON-Blöcken...\\n\")\n",
    "\n",
    "    # 4) Brute-Force: für jede '{' alle möglichen '}'-Enden testen\n",
    "    if debug:\n",
    "        print(\">> Starte Brute-Force-Suche nach JSON-Blöcken...\\n\")\n",
    "\n",
    "    s = text\n",
    "    n = len(s)\n",
    "    brace_positions = [i for i, ch in enumerate(s) if ch == \"{\"]\n",
    "\n",
    "    if debug:\n",
    "        print(\">> Anzahl '{'-Positionen:\", len(brace_positions))\n",
    "\n",
    "    for start in brace_positions:\n",
    "        for end in range(start + 1, n):\n",
    "            if s[end] == \"}\":\n",
    "                candidate = s[start:end + 1]\n",
    "                candidate_stripped = candidate.strip()\n",
    "                # kleine Heuristik: mindestens 'tool' oder 'arguments' sollten drinstehen, sonst sparen wir uns json.loads\n",
    "                if (\"tool\" not in candidate_stripped) and (\"arguments\" not in candidate_stripped):\n",
    "                    continue\n",
    "\n",
    "                if debug:\n",
    "                    print(\">>> Teste Kandidat (start={}, end={}):\".format(start, end))\n",
    "                    print(candidate_stripped)\n",
    "                try:\n",
    "                    data = json.loads(candidate_stripped)\n",
    "                except json.JSONDecodeError:\n",
    "                    if debug:\n",
    "                        print(\">>> Kandidat ist kein valides JSON.\\n\")\n",
    "                    continue\n",
    "\n",
    "                if not isinstance(data, dict):\n",
    "                    if debug:\n",
    "                        print(\">>> Kandidat ist JSON, aber kein Dict.\\n\")\n",
    "                    continue\n",
    "\n",
    "                tool_name = data.get(\"tool\")\n",
    "                arguments = data.get(\"arguments\")\n",
    "\n",
    "                if isinstance(tool_name, str) and isinstance(arguments, dict):\n",
    "                    if debug:\n",
    "                        print(\">>> Kandidat ist gültiger Tool-Call!\")\n",
    "                        print(f\"    tool={tool_name}, arguments={arguments}\\n\")\n",
    "                    return {\"tool\": tool_name, \"arguments\": arguments}\n",
    "\n",
    "    if debug:\n",
    "        print(\">> Kein valider Tool-Call gefunden. Rückgabe = None.\\n\")\n",
    "\n",
    "    return None"
   ],
   "id": "8785b52ea190eefe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tool-Call parsen und inspizieren\n",
    "\n",
    "### Funktion\n",
    "- Ruft try_parse_tool_call(assistant_text) auf.\n",
    "- Druckt den geparsten Tool-Call.\n",
    "\n",
    "### Input\n",
    "- assistant_text – Modellantwort aus dem „Koordinaten von Barcelona“-Run.\n",
    "\n",
    "### Output\n",
    "- parsed_call: {\"tool\": \"...\", \"arguments\": {...}} oder None.\n",
    "- Konsolenausgabe des Ergebnisses."
   ],
   "id": "4fdd83b3ba5b9f92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "parsed_call = try_parse_tool_call(assistant_text)\n",
    "print(parsed_call)"
   ],
   "id": "10074dc86f360b76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ein MCP-Tool tatsächlich ausführen: call_mcp_tool_once\n",
    "\n",
    "### Funktion\n",
    "- Führt genau ein MCP-Tool aus und gibt ein „normales“ Python-Objekt zurück.\n",
    "- Kapselt:\n",
    "    - Aufbau der HTTP-Clientverbindung,\n",
    "    - session.initialize(),\n",
    "    - session.call_tool(tool_name, arguments).\n",
    "\n",
    "### Input\n",
    "- tool_name: str – z. B. \"geocode\".\n",
    "- arguments: Dict[str, Any] – z. B. {\"destination\": \"Barcelona\"}.\n",
    "- url: str = MCP_URL.\n",
    "\n",
    "### Output\n",
    "- Rückgabewert:\n",
    "    - Primär: resp.structuredContent, falls vorhanden (oft schon ein Dict).\n",
    "    - Sekundär: json.loads(content[0].text), wenn es Text-JSON gibt.\n",
    "    - Fallback: ein einfaches Dict mit Metadaten (meta, isError) oder Text.\n",
    "- Typischer Output (Dict):\n",
    "    - {\"lat\": 41.38..., \"lon\": 2.17...}."
   ],
   "id": "df221e5cf66bcc30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "async def call_mcp_tool_once(\n",
    "    tool_name: str,\n",
    "    arguments: Dict[str, Any],\n",
    "    url: str = MCP_URL,\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Führt genau EIN MCP-Tool aus und gibt ein \"normales\" Python-Objekt zurück\n",
    "    (idealerweise ein dict), das direkt JSON-serialisierbar ist.\n",
    "    \"\"\"\n",
    "    async with streamable_http_client(url) as (read_stream, write_stream, _):\n",
    "        async with ClientSession(read_stream, write_stream) as session:\n",
    "            await session.initialize()\n",
    "            resp = await session.call_tool(tool_name, arguments)\n",
    "\n",
    "            # Priorität 1: structuredContent (das ist bei dir schon ein dict)\n",
    "            structured = getattr(resp, \"structuredContent\", None)\n",
    "            if structured is not None:\n",
    "                return structured\n",
    "\n",
    "            # Priorität 2: content[0].text als JSON parsen, falls vorhanden\n",
    "            content = getattr(resp, \"content\", None)\n",
    "            if (\n",
    "                content\n",
    "                and isinstance(content, list)\n",
    "                and hasattr(content[0], \"text\")\n",
    "                and isinstance(content[0].text, str)\n",
    "            ):\n",
    "                txt = content[0].text\n",
    "                try:\n",
    "                    return json.loads(txt)\n",
    "                except json.JSONDecodeError:\n",
    "                    # Falls es kein JSON ist – dann geben wir einfach den Text zurück\n",
    "                    return {\"text\": txt}\n",
    "\n",
    "            # Fallback: zur Not alles in ein dict packen\n",
    "            return {\n",
    "                \"meta\": getattr(resp, \"meta\", None),\n",
    "                \"isError\": getattr(resp, \"isError\", None),\n",
    "            }\n"
   ],
   "id": "5dbee802101b201e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tool-Aufruf wirklich ausführen (Geocode Barcelona)\n",
    "\n",
    "### Funktion\n",
    "- Verwendet das geparste parsed_call, um das passende MCP-Tool auszuführen.\n",
    "- Speichert den Tool-Output in tool_result.\n",
    "\n",
    "### Input\n",
    "- parsed_call[\"tool\"], parsed_call[\"arguments\"].\n",
    "- MCP-Server muss laufen.\n",
    "\n",
    "### Output\n",
    "- tool_result – Ergebnisobjekt, z. B. Koordinaten für Barcelona.\n",
    "\n",
    "### Konsole:\n",
    "- tool_result: {...} zur Sichtkontrolle."
   ],
   "id": "3b9929d9b3e52014"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tool_name = parsed_call[\"tool\"]\n",
    "arguments = parsed_call[\"arguments\"]\n",
    "\n",
    "tool_result = await call_mcp_tool_once(tool_name, arguments)\n",
    "\n",
    "\n",
    "print(f\"tool_result: {tool_result}\")"
   ],
   "id": "acc8a45a69b6d43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tool-Historie initialisieren und füllen\n",
    "\n",
    "### Funktion\n",
    "- Legt eine Liste tool_history an.\n",
    "- Hängt den eben ausgeführten Tool-Call samt Ergebnis daran an.\n",
    "- Das ist die „Tool-Gedächtnisstruktur“ für spätere Prompts.\n",
    "\n",
    "### Input\n",
    "- tool_name, arguments, tool_result.\n",
    "\n",
    "### Output\n",
    "- tool_history: List[Dict[str, Any]]\n",
    "\n",
    "### Konsole:\n",
    "- print(tool_history)."
   ],
   "id": "ec6dd71435a9bbb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Tool-Verlauf speichern\n",
    "tool_history = []\n",
    "\n",
    "tool_history.append(\n",
    "    {\n",
    "        \"tool\": tool_name,\n",
    "        \"arguments\": arguments,\n",
    "        \"result\": tool_result,  # z.B. {'lat': 41.8933203, 'lon': 12.4829321}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(tool_history)"
   ],
   "id": "48f69f1663ec80d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Koordinaten aus Tool-Historie nutzen, um die Nutzeranfrage final zu Beantworten\n",
    "\n",
    "### Funktion\n",
    "- Zeigt, wie das LLM nun auf Basis des bisherigen Tool-Ergebnisses antwortet.\n",
    "- System: Tools sind erlaubt – aber allow_tools=False fügt einen Override-Systemprompt ein:\n",
    "    - „Du darfst KEINE MCP-Tools aufrufen...“\n",
    "- Da nun bereits alle notwendigen Informationen aus der MCP-Abfrage vorliegen\n",
    "\n",
    "### Input\n",
    "- tool_names_with_meta = await fetch_tools_with_metadata()\n",
    "- user_input: wieder „Wie sind die genauen Geokoordinaten von Barcelona?“\n",
    "- tool_history: enthält den geocode-Call + Result.\n",
    "- allow_tools=False.\n",
    "\n",
    "### Output\n",
    "- assistant_text: LLM soll jetzt:\n",
    "    - keine Tools mehr aufrufen,\n",
    "    - die Koordinaten aus tool_history direkt als „Wissen“ verwenden\n",
    "    - und im Fließtext antworten (z. B. „Barcelona liegt bei ca. Breitengrad X, Längengrad Y“).\n",
    "\n",
    "### Konsole:\n",
    "- Prompt + Antwort."
   ],
   "id": "9b420a13a258b11c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tool_names_with_meta = await fetch_tools_with_metadata()\n",
    "\n",
    "user_input = \"Ich möchte eine Reise nach Barcelona machen. Wie sind die genauen Geokoordinaten von Barcelona?\"\n",
    "\n",
    "history = []\n",
    "\n",
    "prompt = build_chat_prompt_with_rag_and_tools(\n",
    "    system_prompt=system_prompt_for_tools_and_rag,\n",
    "    user_prompt=user_input,\n",
    "    history=history,\n",
    "    tool_history=tool_history,\n",
    "    allow_tools=False,\n",
    ")\n",
    "\n",
    "print(f\"Input: {prompt}\\n\\n\")\n",
    "print(\"Ende input\\n\\n\")\n",
    "assistant_text = llama_chat(prompt, max_new_tokens=512)\n",
    "print(f\"Output: {assistant_text}\\n\\n\")"
   ],
   "id": "88ceb4d43c78d2ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Erneute Wetteranfrage basierend auf dem zuvor erlangten Wissen der Koordinaten\n",
    "\n",
    "### Funktion\n",
    "- Jetzt kombinieren der Toolcalls von Hand:\n",
    "    - Koordinaten von Barcelona sind bereits in tool_history.\n",
    "    - User fragt erneut nach dem heutigen Wetter in Barcelona.\n",
    "    - Da alle Argumente für get_weather nun vorhanden sind, darf das Modell ein Tool aufrufen.\n",
    "\n",
    "### Input\n",
    "- tool_names_with_meta = await fetch_tools_with_metadata()\n",
    "- today = date.today()\n",
    "- user_input: „Wie ist heute (YYYY-MM-DD) das Wetter in Barcelona? Nutze die Geokoordinaten von vorhin…“\n",
    "- tool_history: enthält geocode-Resultat aus vorherigem Call:\n",
    "- allow_tools=True.\n",
    "\n",
    "### Output\n",
    "- assistant_text: sollte jetzt einen JSON-Tool-Call für get_weather produzieren.\n",
    "\n",
    "### Konsole:\n",
    "- Prompt + Modelloutput."
   ],
   "id": "6ffff28be9b8e44c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tool_names_with_meta = await fetch_tools_with_metadata()\n",
    "from datetime import date\n",
    "\n",
    "# Returns the current local date\n",
    "today = date.today()\n",
    "\n",
    "user_input = f\"Ich möchte eine Reise nach Barcelona machen. Wie ist heute ({today}) das Wetter in Barcelona? Nutze die Geokoordinaten von vorhin um das Wetter abzufragen.\"\n",
    "\n",
    "prompt = build_chat_prompt_with_rag_and_tools(\n",
    "    system_prompt=system_prompt_for_tools_and_rag,\n",
    "    user_prompt=user_input,\n",
    "    history=history,\n",
    "    tool_history=tool_history,\n",
    ")\n",
    "\n",
    "print(f\"Input: {prompt}\\n\\n\")\n",
    "assistant_text = llama_chat(prompt, max_new_tokens=512)\n",
    "print(f\"Output: {assistant_text}\\n\\n\")"
   ],
   "id": "a36c9ad45fb937d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Wetter-Tool ausführen und finale Antwort generieren\n",
    "\n",
    "### Funktion\n",
    "- Nächste manuelle Agentenrunde:\n",
    "    - parsed_call = try_parse_tool_call(assistant_text) → Wetter-Tool-Call extrahieren.\n",
    "    - tool_result = await call_mcp_tool_once(tool_name, arguments) → Wetterdaten holen.\n",
    "    - tool_history.append(...) → Wetter-Resultat zur Historie hinzufügen.\n",
    "     - Nochmals build_chat_prompt_with_rag_and_tools(..., allow_tools=False) mit denselben user_input + aktualisierter tool_history.\n",
    "    - llama_chat(...) → jetzt Endantwort ohne neue Tools, aber auf Basis der gespeicherten Wetterdaten.\n",
    "\n",
    "### Input\n",
    "- assistant_text (mit Wetter-Tool-Call).\n",
    "- MCP-Server, tool_history (mit geocode), user_input (Wetterfrage).\n",
    "- allow_tools=False beim zweiten Prompt.\n",
    "\n",
    "### Output\n",
    "- parsed_call: Wetter-Tool-Call.\n",
    "- tool_result: Wetterprofil (Open-Meteo Profil, z. B. Temperaturen, Niederschlag).\n",
    "- Aktualisierte tool_history: enthält jetzt zwei Einträge (geocode, get_weather).\n",
    "- assistant_text (zweiter LLM-Call):\n",
    "- Modell erklärt das Wetter in Barcelona am heutigen Datum,\n",
    "- ohne weitere Tools aufzurufen,\n",
    "- und kann auf die strukturierten Wetterdaten zurückgreifen."
   ],
   "id": "d7c1c6a4c9cdd4a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "parsed_call = try_parse_tool_call(assistant_text)\n",
    "print(parsed_call)\n",
    "\n",
    "tool_name = parsed_call[\"tool\"]\n",
    "arguments = parsed_call[\"arguments\"]\n",
    "\n",
    "tool_result = await call_mcp_tool_once(tool_name, arguments)\n",
    "\n",
    "\n",
    "print(f\"tool_result: {tool_result}\")\n",
    "\n",
    "tool_history.append(\n",
    "    {\n",
    "        \"tool\": tool_name,\n",
    "        \"arguments\": arguments,\n",
    "        \"result\": tool_result,  # z.B. {'lat': 41.8933203, 'lon': 12.4829321}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(tool_history)\n",
    "\n",
    "tool_names_with_meta = await fetch_tools_with_metadata()\n",
    "\n",
    "prompt = build_chat_prompt_with_rag_and_tools(\n",
    "    system_prompt=system_prompt_for_tools_and_rag,\n",
    "    user_prompt=user_input,\n",
    "    history=history,\n",
    "    tool_history=tool_history,\n",
    "    allow_tools=False,\n",
    ")\n",
    "\n",
    "print(f\"Input: {prompt}\\n\\n\")\n",
    "print(\"Ende input\\n\\n\")\n",
    "assistant_text = llama_chat(prompt, max_new_tokens=512)\n",
    "print(f\"Output: {assistant_text}\\n\\n\")"
   ],
   "id": "b81a52fef099ca25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Teil 4: Voll agentisches MCP – der Reiseagent denkt und handelt selbst\n",
    "\n",
    "In Teil 4 machen wir aus dem „Half-Agent“ aus Teil 3 jetzt einen echten, voll agentischen Workflow:\n",
    "- Das LLM plant eine Tool-Kette (inkl. Parallelität).\n",
    "- Es referenziert frühere Tool-Outputs über $ref.\n",
    "- Ein Steuerloop (agentic_run_to_final_answer) führt Schritt für Schritt alle geplanten Tools aus, aktualisiert die Historie und lässt das Modell bei Bedarf re-planen, bis eine finale Antwort entsteht.\n",
    "\n",
    "Konzeptionell lehnen wir uns an Tool-/Plan-basierte Agenten wie ReAct und ähnliche Frameworks an, die iterativ zwischen „Denken“, „Handeln (Tool)“ und „Beobachten“ wechseln."
   ],
   "id": "dc72d1db0b9bdd68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Agenten-Systemprompt mit Tool-Chains: build_agent_system_prompt\n",
    "\n",
    "### Funktion\n",
    "- Baut einen umfangreichen System-Prompt für einen agentischen Reiseplaner, der:\n",
    "    - RAG-Kontext nutzen darf,\n",
    "    - MCP-Tools verwenden darf,\n",
    "    - Tool-Ketten planen soll (ReAct-Style Plan),\n",
    "    - Ausgaben immer als JSON-Plan mit steps + final zurückgibt,\n",
    "    - $ref-Mechanismus für Abhängigkeiten zwischen Tool-Outputs und Folgetools nutzt,\n",
    "    - User-Informationen via user.*-Referenzen einbindet.\n",
    "\n",
    "### Input\n",
    "- tools: List[Any] – Toolobjekte vom MCP-Server, z. B. geocode, get_weather, get_spots, rank_spots.\n",
    "- Intern wird format_tools_for_prompt(tools) aufgerufen, um einen detaillierten Toolkatalog (inkl. JSON-Schemas) in den Prompt einzubauen.\n",
    "\n",
    "### Output\n",
    "- agent_system_prompt: str – System-Prompt, der u. a. festlegt:\n",
    "    - Ausgabeformat des Modells.\n",
    "    - Semantik von steps:\n",
    "        - alle tools in einem steps[i] sind parallel ausführbar,\n",
    "        - spätere Schritte können via $ref auf Tool-Outputs früherer Schritte zugreifen.\n",
    "    - $ref-Konventionen:\n",
    "        - { \"$ref\": \"geo_barcelona.lat\" } → greift auf result[\"lat\"] eines Tool-Calls mit ID geo_barcelona zu.\n",
    "        - { \"$ref\": \"user.raw\" } oder user.destination etc. für Teile der Nutzereingabe.\n",
    "    - Einschränkungen:\n",
    "        - Nur gültiges JSON oder final Text ausgeben,\n",
    "        - keine mehrfachen Keys,\n",
    "        - arguments ist immer ein Objekt mit benannten Feldern, keine nackten $ref.\n",
    "    - Verhalten bei Out-of-Scope:\n",
    "        - steps: [], final: \"Diese Frage kann ich nicht beantworten, da sie nicht dem Kontext des Assistenten entspricht.\""
   ],
   "id": "8d5a012a8c13b03d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Any, List\n",
    "\n",
    "def build_agent_system_prompt(tools: List[Any]) -> str:\n",
    "    tool_catalog = format_tools_for_prompt(tools)\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    Du bist ein persönlicher Reiseplaner.\n",
    "\n",
    "    Deine Aufgaben:\n",
    "    - Beantworte Fragen zu Urlaub, Reisen, Städten, Sehenswürdigkeiten, Aktivitäten, Regionalkultur oder Reiserouten.\n",
    "    - Du arbeitest in einem Szenario mit zusätzlichem Kontext, der dir vom System bereitgestellt wird (z.B. als „Kontext (aus Retrieval)“).\n",
    "    - Dir stehen MCP-Tools zur Verfügung, um Informationen zu beschaffen, die nicht in deinem Weltwissen vorhanden sind.\n",
    "\n",
    "    Nutzung von Kontext (RAG):\n",
    "    - Wenn dir ein Kontexttext vom System bereitgestellt wird, behandle ihn als zuverlässige Wissensquelle für diese Konversation.\n",
    "    - Du darfst alle darin enthaltenen Fakten verwenden.\n",
    "    - Wenn eine Information im Kontext steht, darfst du sie verwenden, auch wenn du sie nicht aus deinem allgemeinen Weltwissen kennst.\n",
    "\n",
    "    Allgemeines Arbeitsprinzip (ReAct mit Planung und Tool-Chains):\n",
    "    - Denke zuerst still über einen mehrschrittigen Plan nach, wie du die Anfrage des Nutzers beantworten kannst.\n",
    "    - Plane dabei explizit:\n",
    "      - welche Tools du brauchst,\n",
    "      - welche Tools voneinander abhängen (sequentiell) und\n",
    "      - welche Tools unabhängig voneinander parallel ausgeführt werden können.\n",
    "    - Gruppiere parallel ausführbare Tools in demselben Schritt.\n",
    "    - Tools, die Ausgaben anderer Tools benötigen, müssen in einem späteren Schritt stehen und ihre Argumente über Referenzen auf frühere Tool-IDs erhalten.\n",
    "    - Nach der Ausführung eines Schritts darf der Plan in einem späteren Turn angepasst oder erweitert werden, falls Zwischenergebnisse neue Informationen liefern.\n",
    "\n",
    "    Ausgabeformat DEINER Antwort:\n",
    "    - Du antwortest IMMER mit genau einem JSON-Objekt (kein Fließtext außerhalb des JSON, kein Markdown, keine ```-Codeblöcke).\n",
    "    - Das JSON hat IMMER die Form:\n",
    "\n",
    "    ```json\n",
    "    {\n",
    "      \"steps\": [\n",
    "        {\n",
    "          \"description\": \"<natürliche Beschreibung des Schritts>\",\n",
    "          \"tools\": [\n",
    "            {\n",
    "              \"id\": \"<eindeutige-id>\",\n",
    "              \"name\": \"<tool-name>\",\n",
    "              \"arguments\": {\n",
    "                /* Argumente, ggf. mit $ref-Platzhaltern */\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "        /* weitere Schritte */\n",
    "      ],\n",
    "      \"final\": null oder \"<natürliche Antwort für den Nutzer>\"\n",
    "    }\n",
    "    - Die Semantik von \"steps\":\n",
    "      - Jeder Eintrag in \"steps\" beschreibt einen logischen Schritt im Plan.\n",
    "      - Alle Tools innerhalb desselben \"steps[i].tools\" sollen parallel ausführbar sein (keine gegenseitigen Abhängigkeiten).\n",
    "      - Tools, die von Ergebnissen früherer Tools abhängen, müssen in einem späteren Schritt (höherer Index) stehen.\n",
    "    - Wenn du noch Tools brauchst:\n",
    "      - Fülle \"steps\" mit dem geplanten Tool-Plan.\n",
    "      - Setze \"final\": null.\n",
    "    - Wenn du keine Tools mehr brauchst und alle nötigen Informationen hast:\n",
    "      - Kannst du \"steps\": [] setzen (oder nur noch eine beschreibende Abschlussaktion ohne Tools) und\n",
    "      - \"final\" mit der fertigen, gut lesbaren Antwort für den Nutzer füllen.\n",
    "    - Es ist erlaubt, in derselben Antwort sowohl weitere Schritte mit Tools als auch schon eine vorbereitete finale Antwort vorzusehen, aber der Normalfall ist:\n",
    "      - Zwischenturn: \"final\": null, Fokus auf Tool-Plan.\n",
    "      - Letzter Turn: \"steps\": [] und \"final\": \"<Antwort…>\".\n",
    "    - WICHTIG: Gültiges JSON\n",
    "      - Jeder JSON-Output MUSS syntaktisch gültig sein.\n",
    "      - In einem JSON-Objekt darf jeder Schlüssel nur EINMAL vorkommen.\n",
    "        Beispiel für UNGÜLTIG (verboten):\n",
    "        {\n",
    "        \"$ref\": \"geo_roma.lat\",\n",
    "        \"$ref\": \"geo_roma.lon\"\n",
    "        }\n",
    "      - Stattdessen musst du für jedes Feld einen eigenen Key verwenden und dort ggf. einen $ref-Wert eintragen, z.B.:\n",
    "        {\n",
    "        \"lat\": { \"$ref\": \"geo_roma.lat\" },\n",
    "        \"lon\": { \"$ref\": \"geo_roma.lon\" }\n",
    "        }\n",
    "      - Schreibe KEIN \"arguments\": { \"$ref\": \"...\" } ohne weitere Felder. Immer: \"arguments\": { \"<argument-name>\": <Wert oder $ref-Objekt>, ... }.\n",
    "    - Spezifikation der Tool-Objekte:\n",
    "      - Jedes Tool-Objekt in \"tools\" hat:\n",
    "        - \"id\": eine eindeutige Bezeichner-String innerhalb dieses Plans (z.B. \"geo_berlin\").\n",
    "        - \"name\": genau den Namen eines der verfügbaren MCP-Tools (siehe unten \"Tools:\").\n",
    "        - \"arguments\": ein JSON-Objekt, das exakt dem Eingabe-Schema des jeweiligen Tools entspricht.\n",
    "      - \"arguments\" MUSS ein Objekt mit den im Input-Schema definierten Feldnamen sein.\n",
    "        Beispiel (vereinfacht) für das Tool \"get_weather\", dessen Schema u.a. \"lat\", \"lon\", \"start_date\", \"end_date\" vorsieht:\n",
    "        \"arguments\": {\n",
    "        \"lat\": { \"$ref\": \"geo_roma.lat\" },\n",
    "        \"lon\": { \"$ref\": \"geo_roma.lon\" },\n",
    "        \"start_date\": \"2026-01-23\",\n",
    "        \"end_date\": \"2026-01-23\",\n",
    "        \"include_raw\": false\n",
    "        }\n",
    "      - Du darfst KEIN \"arguments\": { \"$ref\": \"...\" } erzeugen. Jeder Parameter des Tools muss entweder:\n",
    "        - ein literaler Wert sein (z.B. \"2026-01-23\", 5, true) ODER\n",
    "        - ein Objekt der Form { \"$ref\": \"<quelle.pfad>\" }.\n",
    "      - Referenzen für Argumente ($ref):\n",
    "        - KEIN Argument darf aus der Luft erfunden werden. Wenn ein Argument aus einem Tool-Output oder aus dem User-Input stammt, markiere das explizit mit einem $ref-Objekt.\n",
    "      - Grundform von $ref:\n",
    "        - Ein $ref-Wert hat IMMER die Form:\n",
    "          - { \"$ref\": \"<quelle>\" }\n",
    "        - Der String <quelle> hat die Form \"<tool-id>\" oder \"<tool-id>.<pfad>\" oder \"user.<pfad>\".\n",
    "        - Ergebnisse früherer Tools:\n",
    "          Die Ergebnisse früherer Tools werden dir in der History als Messages der Form gezeigt:\n",
    "          {\n",
    "          \"role\": \"tool\",\n",
    "          \"tool_id\": \"<tool-id>\",\n",
    "          \"name\": \"...\",\n",
    "          \"arguments\": { ... },\n",
    "          \"result\": { ... }\n",
    "          }\n",
    "        - Für $ref verwendest du IMMER die \"tool_id\" als Wurzel.\n",
    "        - Der Pfad <pfad> bezieht sich auf die Felder innerhalb von \"result\".\n",
    "          Beispiel-History:\n",
    "          {\n",
    "          \"role\": \"tool\",\n",
    "          \"tool_id\": \"geo_roma\",\n",
    "          \"result\": {\n",
    "          \"lat\": 41.8933,\n",
    "          \"lon\": 12.4829\n",
    "          }\n",
    "          }\n",
    "\n",
    "          → Zulässige $ref:\n",
    "          { \"$ref\": \"geo_roma\" } → das komplette result-Objekt\n",
    "          { \"$ref\": \"geo_roma.lat\" } → 41.8933\n",
    "          { \"$ref\": \"geo_roma.lon\" } → 12.4829\n",
    "        - WICHTIG:\n",
    "          - Du darfst NICHT auf \"result\" selbst referenzieren, also KEIN: { \"$ref\": \"geo_roma.result\" }\n",
    "          - Du gehst IMMER so vor, als ob der Inhalt von \"result\" die Wurzelstruktur für $ref ist.\n",
    "          - Pfade wie \"geo_roma.arguments\" oder \"geo_roma.result.lat\" sind VERBOTEN.\n",
    "        - Beispiel für korrektes Verwenden von Geo-Koordinaten in einem Folge-Tool \"get_weather\":\n",
    "          \"steps\": [\n",
    "          {\n",
    "          \"description\": \"Geokoordinaten für Rom bestimmen\",\n",
    "          \"tools\": [\n",
    "          {\n",
    "          \"id\": \"geo_roma\",\n",
    "          \"name\": \"geocode\",\n",
    "          \"arguments\": {\n",
    "          \"destination\": \"Rom\"\n",
    "          }\n",
    "          }\n",
    "          ]\n",
    "          },\n",
    "          {\n",
    "          \"description\": \"Tageswetter für Rom am gewünschten Datum abrufen\",\n",
    "          \"tools\": [\n",
    "          {\n",
    "          \"id\": \"wetter_roma\",\n",
    "          \"name\": \"get_weather\",\n",
    "          \"arguments\": {\n",
    "          \"lat\": { \"$ref\": \"geo_roma.lat\" },\n",
    "          \"lon\": { \"$ref\": \"geo_roma.lon\" },\n",
    "          \"start_date\": \"2026-01-23\",\n",
    "          \"end_date\": \"2026-01-23\",\n",
    "          \"include_raw\": false\n",
    "          }\n",
    "          }\n",
    "          ]\n",
    "          }\n",
    "          ]\n",
    "        - Nutzer-Eingabe:\n",
    "          - Wenn ein Argument direkt aus der aktuellen Nutzernachricht kommen soll (z.B. Stadtname, Datum, Budget), verwende eine Referenz mit Präfix \"user.\".\n",
    "          - Beispiele:\n",
    "              - { \"$ref\": \"user.origin\" } → Herkunftsort aus der Nutzernachricht.\n",
    "              - { \"$ref\": \"user.destination\" } → Zielort aus der Nutzernachricht.\n",
    "              - { \"$ref\": \"user.raw\" } → komplette Roh-Eingabe des Nutzers.\n",
    "          - Du definierst diese Pfade semantisch, damit für das Backend klar ist, welcher Teil des User-Inputs gemeint ist.\n",
    "        - Zusammenfassung für $ref:\n",
    "          - $ref steht IMMER als Wert eines Feldes in \"arguments\".\n",
    "          - Du verwendest KEIN \"arguments\": { \"$ref\": ... } ohne weitere Felder.\n",
    "          - Du referenzierst NIE \"result\", \"arguments\" oder ähnliche Meta-Felder, sondern arbeitest so, als ob die \"result\"-Struktur selbst die Basis für den Pfad ist.\n",
    "        - Planung von Tool-Chains:\n",
    "          - Du DARFST und SOLLST Tool-Ketten planen (z.B. \"zuerst Geocoding, dann Routenberechnung, dann Bewertung\").\n",
    "          - Überlege explizit, welche Tools Vorbedingungen anderer Tools erfüllen (z.B. Koordinaten, IDs, Zeiträume).\n",
    "          - Baue den Plan so, dass:\n",
    "            - Schritt 0: Tools, die direkt aus dem User-Input und ggf. Kontext ihre Argumente beziehen.\n",
    "            - Schritt 1: Tools, deren Argumente via $ref aus Ergebnissen von Schritt 0 kommen.\n",
    "            - Schritt 2: Tools, die auf Schritt 1 aufbauen, usw.\n",
    "          - Tools, die keine gemeinsamen Abhängigkeiten haben, dürfen im selben Schritt stehen, damit sie parallel ausgeführt werden können.\n",
    "          - Du darfst mehrere Schritte vorausplanen. Das Backend kann nach Ausführung eines oder mehrerer Schritte den Plan durch einen erneuten Aufruf des Modells aktualisieren lassen.\n",
    "          - Wenn du einen bestehenden Plan aktualisierst erhalten die bereits ausgeführten Schritte die selben Bezeichner wie zuvor. Toolnamen ausgeführter Tools dürfen Rückwirkend also nie verändert werden.\n",
    "        - Nutzung von MCP-Tools:\n",
    "          - Nutze Tools, wenn du:\n",
    "            - Informationen brauchst, die nicht sicher in deinem Weltwissen oder im bereitgestellten Kontext stehen, oder\n",
    "            - exakte Daten (z.B. Koordinaten, Preise, Zeiten, Verfügbarkeiten) benötigst.\n",
    "          - Nutze Tools nur dann, wenn du ihre Input-Schemata sinnvoll und vollständig (ggf. mit $ref) befüllen kannst.\n",
    "          - Verwende für Tool-Argumente ausschließlich:\n",
    "            - explizite Informationen aus der aktuellen Nutzernachricht (per $ref \"user.…\"),\n",
    "            - frühere Tool-Ergebnisse (per $ref auf Tool-IDs),\n",
    "            - oder wohldefinierte konstante/literale Werte.\n",
    "          - Du DARFST Tool-Argumente NICHT aus deinem allgemeinen Weltwissen raten.\n",
    "          - Wenn die Anfrage ohne Tools gut beantwortbar ist (reine Beratung, Einschätzung, Inspiration), dann erstelle einen JSON-Output mit:\n",
    "            - \"steps\": [] (oder nur erklärende Schritte ohne Tools) und\n",
    "            - \"final\": \"<deine natürliche Antwort>\".\n",
    "        - Umgang mit Wissen und Unsicherheit:\n",
    "          - Erfinde keine Fakten und spekuliere nicht.\n",
    "          - Wenn eine Information weder in deinem Weltwissen noch im bereitgestellten Kontext vorkommt und auch nicht per MCP-Tool-Call erlangt werden kann, erkläre dies in der \"final\"-Antwort.\n",
    "          - Wenn dir wirklich Informationen fehlen, formuliere in \"final\" eine ehrliche, erklärende Antwort und beschreibe, was du stattdessen sicher sagen kannst.\n",
    "        - Einschränkungen (Ablehnungsfälle im Reise-Kontext):\n",
    "          - Lehne Anfragen ab, die eindeutig NICHT zum Reise-/Tourismuskontext gehören.\n",
    "          - In diesem Fall gibst du ebenfalls ein JSON-Objekt zurück, z.B.:\n",
    "            {\n",
    "            \"steps\": [],\n",
    "            \"final\": \"Diese Frage kann ich nicht beantworten, da sie nicht dem Kontext des Assistenten entspricht.\"\n",
    "            }\n",
    "        - Tools (verfügbare MCP-Tools):\n",
    "    \"\"\"\n",
    "    return system_prompt + tool_catalog"
   ],
   "id": "e7c7b5e6d309f248",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Agent-Systemprompt bauen\n",
    "\n",
    "### Funktion\n",
    "- Ruft build_agent_system_prompt(tool_names_with_meta) auf, um den finalen Prompt-String zu erzeugen.\n",
    "\n",
    "### Input\n",
    "- tool_names_with_meta – Toolmetadaten, wie zuvor per fetch_tools_with_metadata() geladen.\n",
    "\n",
    "### Output\n",
    "- agent_system_prompt: str – wird später in allen agentischen Calls verwendet.\n",
    "\n",
    "### Konsole\n",
    "- Ausgabe von agent_system_prompt"
   ],
   "id": "2ea4f8fb32946494"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "agent_system_prompt = build_agent_system_prompt(tool_names_with_meta)\n",
    "print(agent_system_prompt)"
   ],
   "id": "61dcb3a177482e7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Erster agentischer Call: Wetter heute in Barcelona (Wir wollen uns erst mal nur den Plan-Output ansehen)\n",
    "\n",
    "### Funktion\n",
    "- Testet das neue agentische Setup:\n",
    "    - User fragt: „Wie ist heute (YYYY-MM-DD) das Wetter in Barcelona?“\n",
    "    - LLM sollte keine direkte Antwort, sondern einen JSON-Plan ähnlich diesem liefern:\n",
    "        - Schritt 1: geocode(\"Barcelona\")\n",
    "        - Schritt 2: get_weather(lat, lon, start_date=today, end_date=today, include_raw=false) mit $ref auf Schritt 1.\n",
    "\n",
    "### Input\n",
    "- tool_names_with_meta = await fetch_tools_with_metadata()\n",
    "- today = date.today()\n",
    "- user_input: Frage mit Datum.\n",
    "- tool_history = None\n",
    "- Promptbau über build_chat_prompt_with_rag_and_tools(...):\n",
    "- system_prompt=agent_system_prompt\n",
    "- history=leere History\n",
    "- tool_history=None – noch keine Tools gelaufen.\n",
    "\n",
    "### Output\n",
    "- prompt: str – Chat-Prompt inkl. System-Agentenprompt.\n",
    "- assistant_text: Modelloutput, der idealerweise ein gültiges JSON-Objekt im beschriebenen Planformat ist.\n",
    "- Enthält steps (mit Toolkette) und oft final: null im ersten Schuss.\n",
    "\n",
    "### Konsole:\n",
    "- Input-Prompt und Output (Plan) zum Debuggen."
   ],
   "id": "91b091f91cd3e6b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tool_names_with_meta = await fetch_tools_with_metadata()\n",
    "from datetime import date\n",
    "\n",
    "# Returns the current local date\n",
    "today = date.today()\n",
    "\n",
    "user_input = f\"Ich möchte eine Reise nach Barcelona machen. Wie ist heute ({today}) das Wetter in Barcelona?\"\n",
    "tool_history = None\n",
    "history = []\n",
    "\n",
    "prompt = build_chat_prompt_with_rag_and_tools(\n",
    "    system_prompt=agent_system_prompt,\n",
    "    user_prompt=user_input,\n",
    "    history=history,\n",
    "    tool_history=tool_history,\n",
    ")\n",
    "\n",
    "print(f\"Input: {prompt}\\n\\n\")\n",
    "assistant_text = llama_chat(prompt, max_new_tokens=512)\n",
    "print(f\"Output: {assistant_text}\\n\\n\")"
   ],
   "id": "5a6f006cfd7c176",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Im Anschluss werden wir die bearbeitung dieses Plans automatisieren",
   "id": "4ead278edc67a7c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Agentischen Plan robust parsen: agentic_parse_model_plan\n",
    "\n",
    "### Funktion\n",
    "- Extrahiert das JSON-Objekt mit steps und final aus dem Modelloutput, selbst wenn das Modell charme-typisch etwas „drumrum“ schreibt.\n",
    "- Robust gegen:\n",
    "    - Vor-/Nachtext,\n",
    "    - kleine Formatabweichungen, solange irgendwo ein gültiges JSON-Objekt {...} enthalten ist.\n",
    "\n",
    "### Input\n",
    "- output_str: str – der rohe String von llama_chat.\n",
    "    - Funktion loggt Debug-Ausgaben:\n",
    "        - Raw output\n",
    "        - „trimmed JSON candidate“\n",
    "\n",
    "### Output\n",
    "- plan: Dict[str, Any] – geparstes Planobjekt mit:\n",
    "    - Pflichtfeldern:\n",
    "        - \"steps\": List\n",
    "        - \"final\": beliebiger Wert (auch null → None)\n",
    "    - Fehlerszenarien:\n",
    "        - Kein { gefunden → ValueError(\"No JSON object found...\")\n",
    "        - JSON-Parsefehler → ValueError(...)\n",
    "        - \"steps\" fehlt oder ist kein List → ValueError."
   ],
   "id": "f9cbdfd536e9bc73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "from typing import Any, Dict\n",
    "\n",
    "def agentic_parse_model_plan(output_str: str) -> Dict[str, Any]:\n",
    "    print(\"[DEBUG] raw output:\", repr(output_str))\n",
    "\n",
    "    # Leer- oder Nonsens-Output früh abfangen\n",
    "    if not output_str or not output_str.strip():\n",
    "        raise ValueError(\"Model output is empty, cannot parse JSON plan.\")\n",
    "\n",
    "    # Die erste öffnende Klammer suchen\n",
    "    start = output_str.find(\"{\")\n",
    "    if start == -1:\n",
    "        raise ValueError(f\"No JSON object found in model output: {output_str!r}\")\n",
    "\n",
    "    s = output_str\n",
    "\n",
    "    # Jetzt von 'start' an die passende schließende Klammer suchen\n",
    "    depth = 0\n",
    "    in_string = False\n",
    "    escape = False\n",
    "    end: Optional[int] = None\n",
    "\n",
    "    for i, ch in enumerate(s[start:], start=start):\n",
    "        if in_string:\n",
    "            # Innerhalb eines Strings\n",
    "            if escape:\n",
    "                # nächstes Zeichen ist escaped, einfach überspringen\n",
    "                escape = False\n",
    "            elif ch == \"\\\\\":\n",
    "                escape = True\n",
    "            elif ch == '\"':\n",
    "                in_string = False\n",
    "        else:\n",
    "            # Außerhalb von Strings\n",
    "            if ch == '\"':\n",
    "                in_string = True\n",
    "            elif ch == \"{\":\n",
    "                depth += 1\n",
    "            elif ch == \"}\":\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    end = i\n",
    "                    break\n",
    "\n",
    "    if end is None:\n",
    "        # Fallback: alter Modus (kannst du auch wegwerfen, wenn du möchtest)\n",
    "        trimmed = s[start:]\n",
    "    else:\n",
    "        trimmed = s[start:end + 1]\n",
    "\n",
    "    print(\"[DEBUG] trimmed JSON candidate:\", trimmed)\n",
    "\n",
    "    try:\n",
    "        plan = json.loads(trimmed)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(\n",
    "            f\"Failed to parse JSON plan from model output. \"\n",
    "            f\"Error: {e}. Trimmed candidate: {trimmed!r}\"\n",
    "        ) from e\n",
    "\n",
    "    if \"steps\" not in plan or \"final\" not in plan:\n",
    "        raise ValueError(\"Model output must contain 'steps' and 'final'.\")\n",
    "\n",
    "    if not isinstance(plan[\"steps\"], list):\n",
    "        raise ValueError(\"'steps' must be a list.\")\n",
    "\n",
    "    return plan"
   ],
   "id": "23974c28c3883169",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## $ref-Resolver – Einzelwert: agentic_resolve_argument\n",
    "\n",
    "### Funktion\n",
    "- Löst ein einzelnes Argument auf, das entweder:\n",
    "    - ein literaler Wert ist (String, Zahl, Bool),\n",
    "    - ein Dict mit {\"$ref\": \"...\"},\n",
    "    - oder ein String im Format \"$ref:...\".\n",
    "- Unterstützt Referenzen auf:\n",
    "    - Tool-Resultate (\"geo_barcelona.lat\"),\n",
    "    - User-Kontext (\"user.raw\" oder z. B. \"user.destination\").\n",
    "\n",
    "### Input\n",
    "- value: Any – roher Argumentwert aus dem Plan.\n",
    "- tool_results: Dict[str, Any] – Mapping tool_id -> result (ohne Meta).\n",
    "- user_ctx: Dict[str, Any] – z. B. {\"raw\": user_input}.\n",
    "\n",
    "### Output\n",
    "- Aufgelöster Wert:\n",
    "    - Wenn kein $ref: Wert unverändert.\n",
    "    - Wenn $ref:\n",
    "        - Traversiert Pfadsegmente (split(\".\")) über Dictionaries/Listen.\n",
    "        - Gibt konkreten Wert zurück (z. B. 41.38... für Lat).\n",
    "- Fehler:\n",
    "    - Unbekannte tool_id oder ungültiger Pfad → KeyError."
   ],
   "id": "fc233cac98bffe37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Any, Dict, Optional\n",
    "\n",
    "def agentic_resolve_argument(\n",
    "    value: Any,\n",
    "    tool_results: Dict[str, Any],\n",
    "    user_ctx: Dict[str, Any],\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Löst einzelne Argumentwerte auf. Unterstützt:\n",
    "    - {\"$ref\": \"geo_roma.lat\"}\n",
    "    - \"$ref:geo_roma.lat\"\n",
    "    - \"$ref:user.raw\"\n",
    "    \"\"\"\n",
    "    ref_str: Optional[str] = None\n",
    "\n",
    "    # Fall 1: Objekt mit \"$ref\"\n",
    "    if isinstance(value, dict) and \"$ref\" in value and isinstance(value[\"$ref\"], str):\n",
    "        ref_str = value[\"$ref\"]\n",
    "\n",
    "    # Fall 2: String mit Präfix \"$ref:\"\n",
    "    elif isinstance(value, str) and value.startswith(\"$ref:\"):\n",
    "        ref_str = value[len(\"$ref:\") :]\n",
    "\n",
    "    # Kein $ref → Wert unverändert zurückgeben\n",
    "    if ref_str is None:\n",
    "        return value\n",
    "\n",
    "    # Jetzt ref_str auswerten, z.B. \"geo_roma.lat\" oder \"user.raw\"\n",
    "    parts = ref_str.split(\".\")\n",
    "    root = parts[0]\n",
    "    path = parts[1:]\n",
    "\n",
    "    # Quelle: User-Kontext\n",
    "    if root == \"user\":\n",
    "        current: Any = user_ctx\n",
    "    else:\n",
    "        # Quelle: Tool-Resultate\n",
    "        if root not in tool_results:\n",
    "            raise KeyError(f\"Unknown tool id in $ref: {root}\")\n",
    "        current = tool_results[root]\n",
    "\n",
    "    # Debug: einmal zeigen, was wir da wirklich haben\n",
    "    print(f\"[DEBUG] Resolving $ref '{ref_str}': root='{root}', initial_type={type(current)}, initial_value={current}\")\n",
    "\n",
    "    for p in path:\n",
    "        # Numerische Indizes unterstützen (z.B. itineraries.0)\n",
    "        if isinstance(current, list) and p.isdigit():\n",
    "            idx = int(p)\n",
    "            current = current[idx]\n",
    "        elif isinstance(current, dict) and p in current:\n",
    "            current = current[p]\n",
    "        else:\n",
    "            # Noch mehr Debug, bevor wir crashen\n",
    "            raise KeyError(\n",
    "                f\"Invalid path component '{p}' in $ref '{ref_str}' \"\n",
    "                f\"(current_type={type(current)}, current_value={current})\"\n",
    "            )\n",
    "\n",
    "    return current"
   ],
   "id": "6b6ac564e265fc32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## $ref-Resolver – rekursiv über Argumente: agentic_resolve_tool_arguments\n",
    "\n",
    "### Funktion\n",
    "- Wendet agentic_resolve_argument auf komplette arguments-Strukturen an (Dicts/Listen).\n",
    "- Unterstützt:\n",
    "    - verschachtelte Strukturen,\n",
    "    - Listen von Werten,\n",
    "    - Spezialfall: {\"$ref\": \"...\"}\n",
    "        - wird komplett durch das referenzierte Objekt ersetzt.\n",
    "\n",
    "### Input\n",
    "- arguments: beliebige JSON-ähnliche Struktur aus dem Plan (dict, list, primitive).\n",
    "- tool_results, user_ctx – wie oben.\n",
    "\n",
    "### Output\n",
    "- Neue Struktur mit allen $ref durch konkrete Werte ersetzt\n",
    "    - z. B.:\n",
    "\n",
    "    ```python\n",
    "    {\n",
    "      \"lat\": {\"$ref\": \"geo_barcelona.lat\"},\n",
    "      \"lon\": {\"$ref\": \"geo_barcelona.lon\"},\n",
    "      \"start_date\": \"2026-01-23\"\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    - → nach Auflösung:\n",
    "\n",
    "    ```python\n",
    "    {\n",
    "      \"lat\": 41.38,\n",
    "      \"lon\": 2.17,\n",
    "      \"start_date\": \"2026-01-23\"\n",
    "    }\n",
    "    ```"
   ],
   "id": "17416bc8319e0752"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def agentic_resolve_tool_arguments(arguments, tool_results, user_ctx):\n",
    "    if isinstance(arguments, dict):\n",
    "        # Spezialfall: dict besteht NUR aus \"$ref\" → ersetze das ganze Dict durch das referenzierte Objekt\n",
    "        if set(arguments.keys()) == {\"$ref\"} and isinstance(arguments[\"$ref\"], str):\n",
    "            return agentic_resolve_argument(arguments, tool_results, user_ctx)\n",
    "\n",
    "        return {\n",
    "            k: agentic_resolve_tool_arguments(v, tool_results, user_ctx)\n",
    "            for k, v in arguments.items()\n",
    "        }\n",
    "    elif isinstance(arguments, list):\n",
    "        return [\n",
    "            agentic_resolve_tool_arguments(v, tool_results, user_ctx)\n",
    "            for v in arguments\n",
    "        ]\n",
    "    else:\n",
    "        return agentic_resolve_argument(arguments, tool_results, user_ctx)"
   ],
   "id": "5797f9f0998657a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Einen Step ausführen: agentic_execute_step\n",
    "\n",
    "### Funktion\n",
    "- Führt alle Tools in einem Plan-Step aus:\n",
    "    - löst zuerst alle $ref in arguments auf,\n",
    "    - ruft dann für jedes Tool das MCP-Backend auf,\n",
    "    - prüft auf Fehler,\n",
    "    - speichert Ergebnisse in tool_results,\n",
    "    - erzeugt History-Einträge im Format:\n",
    "    ```python\n",
    "    {\n",
    "      \"role\": \"tool\",\n",
    "      \"tool_id\": \"...\",\n",
    "      \"name\": \"...\",\n",
    "      \"arguments\": {...},\n",
    "      \"result\": {...},\n",
    "    }\n",
    "    ```\n",
    "\n",
    "### Input\n",
    "- step: Dict[str, Any] – ein Eintrag aus plan[\"steps\"], z. B.:\n",
    "```python\n",
    "{\n",
    "  \"description\": \"...\",\n",
    "  \"tools\": [\n",
    "    {\"id\": \"geo_barcelona\", \"name\": \"geocode\", \"arguments\": {...}},\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "- call_tool_fn: Callable, das ein Tool wirklich ausführt (agentic_call_mcp_tool).\n",
    "- tool_results: Dict[str, Any] – wird in-place erweitert.\n",
    "- user_ctx: Dict[str, Any] – für user.*-Referenzen.\n",
    "\n",
    "### Output\n",
    "- history_entries: List[Dict[str, Any]] – Tool-History-Objekte, die später wieder in den Prompt eingebaut werden.\n",
    "\n",
    "### Side-Effects:\n",
    "- Debug-Prints zu aufgelösten Argumenten und Ergebnissen.\n",
    "- tool_results[tool_id] = res für alle Tools des Steps.\n",
    "- Fehlerthrow, wenn normalized_result.get(\"isError\") true ist."
   ],
   "id": "37b138ede89fa61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Callable, Awaitable\n",
    "\n",
    "async def agentic_execute_step(\n",
    "    step: Dict[str, Any],\n",
    "    call_tool_fn: Callable[[str, Dict[str, Any]], Awaitable[Any]],\n",
    "    tool_results: Dict[str, Any],\n",
    "    user_ctx: Dict[str, Any],\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Führt alle Tools in einem Step aus.\n",
    "    - step: {\"description\": \"...\", \"tools\": [ {id, name, arguments}, ... ]}\n",
    "    - call_tool_fn: deine MCP-Invoker-Funktion (z.B. call_mcp_tool)\n",
    "    - tool_results: wird in-place mit neuen Ergebnissen gefüllt\n",
    "    - user_ctx: z.B. {\"raw\": user_input}\n",
    "\n",
    "    Rückgabe: Liste von History-Objekten für den Prompt (z.B. Tool-Logs).\n",
    "    \"\"\"\n",
    "    history_entries: List[Dict[str, Any]] = []\n",
    "\n",
    "    for tool in step.get(\"tools\", []):\n",
    "        tool_id = tool[\"id\"]\n",
    "        tool_name = tool[\"name\"]\n",
    "        raw_args = tool.get(\"arguments\", {})\n",
    "\n",
    "        # $ref auflösen\n",
    "        resolved_args = agentic_resolve_tool_arguments(raw_args, tool_results, user_ctx)\n",
    "\n",
    "        # Tool aufrufen\n",
    "        print(f\"[{tool_id}] {tool_name} resolved arguments: {resolved_args}\")\n",
    "        result = await call_tool_fn(tool_name, resolved_args)\n",
    "        print(f\"toolcallresult: {result}\")\n",
    "\n",
    "        tool_id = tool.get(\"id\")\n",
    "\n",
    "        # MCP CallToolResult → direkt structuredContent speichern\n",
    "        if hasattr(result, \"structuredContent\") and result.structuredContent is not None:\n",
    "            normalized_result = result.structuredContent\n",
    "        # Falls du irgendwo schon dicts baust, die structuredContent enthalten\n",
    "        elif isinstance(result, dict) and \"structuredContent\" in result and result[\"structuredContent\"] is not None:\n",
    "            normalized_result = result[\"structuredContent\"]\n",
    "        else:\n",
    "            normalized_result = result\n",
    "\n",
    "        print(f\"[DEBUG] Stored tool_result[{tool_id}] = {type(normalized_result)} → {normalized_result}\")\n",
    "\n",
    "        # History-Eintrag für Tools (kannst du an dein Format anpassen)\n",
    "\n",
    "        if normalized_result.get(\"isError\"):\n",
    "            raise Exception(f\"Tool failed with error: {normalized_result.get('isError')}\")\n",
    "\n",
    "        if isinstance(normalized_result, dict) and \"data\" in normalized_result:\n",
    "            res = normalized_result[\"data\"]\n",
    "        else:\n",
    "            res = normalized_result\n",
    "\n",
    "        # Ergebnis speichern für for Schleife bei parallelen Tools\n",
    "        tool_results[tool_id] = res\n",
    "\n",
    "        history_entries.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_id\": tool_id,\n",
    "            \"name\": tool_name,\n",
    "            \"arguments\": resolved_args,\n",
    "            \"result\": res,\n",
    "        })\n",
    "\n",
    "    return history_entries"
   ],
   "id": "e82549220e32a526",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Nächsten Step im Plan finden: agentic_find_next_step\n",
    "\n",
    "### Funktion\n",
    "- Wählt den nächsten noch nicht ausgeführten Step anhand des Plan-Index.\n",
    "- Simple Strategie: der kleinste Index, der noch nicht in executed_step_indices steht.\n",
    "\n",
    "### Input\n",
    "- plan: Dict[str, Any] – der aktuelle Plan.\n",
    "- executed_step_indices: Set[int] – Menge bereits ausgeführter Step-Indizes.\n",
    "\n",
    "### Output\n",
    "- int – Index des nächsten Steps, oder\n",
    "- None, wenn alle Steps ausgeführt wurden."
   ],
   "id": "1621a676c7a6bb47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Set\n",
    "\n",
    "def agentic_find_next_step(\n",
    "    plan: Dict[str, Any],\n",
    "    executed_step_indices: Set[int],\n",
    ") -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Wählt den nächsten noch nicht ausgeführten Step.\n",
    "    V1: simpel – der kleinste Index, der noch nicht in executed_step_indices ist.\n",
    "    \"\"\"\n",
    "    for idx, _ in enumerate(plan.get(\"steps\", [])):\n",
    "        if idx not in executed_step_indices:\n",
    "            return idx\n",
    "    return None"
   ],
   "id": "af52a7509fa3041b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompt-Builder für den Agenten: agentic_build_chat_prompt_with_rag_and_tools\n",
    "\n",
    "### Funktion\n",
    "- Baut einen Llama-konformen Chat-Prompt im rohen Template-Format, statt tokenizer.apply_chat_template zu nutzen.\n",
    "- Integriert:\n",
    "    - Systemprompt (Agentenlogik + Toolbeschreibungen),\n",
    "    - optionalen RAG-Kontext,\n",
    "    - bisherige Chat-History (role / content),\n",
    "    - Tool-History (als role=\"tool\" mit JSON-Content),\n",
    "    - aktuelle User-Nachricht.\n",
    "\n",
    "### Input\n",
    "- system_prompt: str – hier: agent_system_prompt.\n",
    "- user_prompt: str – aktuelle Userfrage.\n",
    "- history: List[Dict[str, Any]] – z. B. frühere User/Assistant-Messages.\n",
    "- tool_history: Optional[List[Dict[str, Any]]] – bisherige Tool-Calls.\n",
    "- retrieved_context: Optional[str] – optionaler RAG-Kontext (hier noch nicht aktiv genutzt, aber vorbereitet).\n",
    "\n",
    "### Output\n",
    "- str: Promptstring mit Llama-Spezialtokens:\n",
    "- <|begin_of_text|>, <|start_header_id|>system<|end_header_id|>, <|eot_id|>, etc.\n",
    "- Wird direkt an llama_chat(prompt, ...) übergeben."
   ],
   "id": "96919f6f9a918c9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def agentic_build_chat_prompt_with_rag_and_tools(\n",
    "    system_prompt: str,\n",
    "    user_prompt: str,\n",
    "    history: List[Dict[str, Any]],\n",
    "    tool_history: Optional[List[Dict[str, Any]]] = None,\n",
    "    retrieved_context: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Baut den vollständigen Prompt für das LLM, inkl.:\n",
    "    - system_prompt\n",
    "    - optionaler Kontext (RAG)\n",
    "    - bisherige Chat-History\n",
    "    - bisherige Tool-History\n",
    "    - aktuelle User-Nachricht\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    parts.append(\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\")\n",
    "    parts.append(system_prompt)\n",
    "    parts.append(\"<|eot_id|>\")\n",
    "\n",
    "    if retrieved_context:\n",
    "        parts.append(\"<|start_header_id|>system<|end_header_id|>\\n\\n\")\n",
    "        parts.append(f\"Kontext (aus Retrieval):\\n{retrieved_context}\\n\")\n",
    "        parts.append(\"<|eot_id|>\")\n",
    "\n",
    "    # alte history anhängen\n",
    "    if history:\n",
    "        for msg in history:\n",
    "            parts.append(f\"<|start_header_id|>{msg['role']}<|end_header_id|>\\n\\n\")\n",
    "            parts.append(msg[\"content\"])\n",
    "            parts.append(\"<|eot_id|>\")\n",
    "\n",
    "    # tool_history (falls du sie als eigene „role“ einfügst)\n",
    "    if tool_history:\n",
    "        for th in tool_history:\n",
    "            parts.append(f\"<|start_header_id|>tool<|end_header_id|>\\n\\n\")\n",
    "            parts.append(json.dumps(th, ensure_ascii=False))\n",
    "            parts.append(\"<|eot_id|>\")\n",
    "\n",
    "    # Aktuelle Userfrage\n",
    "    parts.append(\"<|start_header_id|>user<|end_header_id|>\\n\\n\")\n",
    "    parts.append(user_prompt)\n",
    "    parts.append(\"<|eot_id|>\")\n",
    "\n",
    "    return \"\".join(parts)"
   ],
   "id": "b30235514526b69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hauptschleife des Agenten: agentic_run_to_final_answer\n",
    "\n",
    "### Funktion\n",
    "- Orchestriert die gesamte agentische Interaktion:\n",
    "    1. Baut Prompt (inkl. Tool-History).\n",
    "    2. Holt Plan vom LLM.\n",
    "    3. Wählt nächsten Step, führt Tools aus, ergänzt Tool-History.\n",
    "    4. Erlaubt Re-Planning in der nächsten Iteration.\n",
    "    5. Beendet, wenn final gesetzt und keine Steps mehr offen sind.\n",
    "\n",
    "### Input\n",
    "- user_input: str – ursprüngliche Nutzerfrage.\n",
    "- system_prompt: str – agent_system_prompt.\n",
    "- history: List[Dict[str, Any]] – z. B. frühere Interaktion (Name „Max Mustermann“).\n",
    "- tool_history: Optional[List[Dict[str, Any]]] – initial meist None.\n",
    "- call_tool_fn: z. B. agentic_call_mcp_tool (führt MCP-Tools aus).\n",
    "- build_prompt_fn: Standard agentic_build_chat_prompt_with_rag_and_tools.\n",
    "- max_iterations: int = 8 – Sicherheitsgrenze gegen Endlosschleifen.\n",
    "\n",
    "### Output\n",
    "- str: finale Antwort an den Nutzer (plan[\"final\"]), z. B. eine Beschreibung des Wetters oder der besten Jogging-Spots.\n",
    "\n",
    "### Side-Effects:\n",
    "- Debug-Ausgaben für jede Iteration:\n",
    "    - Prompt,\n",
    "    - Modelloutput,\n",
    "    - Plan,\n",
    "    - gewählten Step, etc.\n",
    "    - Tool-History wächst über die Iterationen."
   ],
   "id": "2e0f369e1af4aed3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "async def agentic_run_to_final_answer(\n",
    "    user_input: str,\n",
    "    system_prompt: str,\n",
    "    history: List[Dict[str, Any]],\n",
    "    tool_history: Optional[List[Dict[str, Any]]],\n",
    "    call_tool_fn: Callable[[str, Dict[str, Any]], Awaitable[Any]],\n",
    "    build_prompt_fn: Callable[..., str] = agentic_build_chat_prompt_with_rag_and_tools,\n",
    "    max_iterations: int = 8,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Hauptschleife:\n",
    "    - plant mit dem LLM,\n",
    "    - führt Step für Step Tools aus,\n",
    "    - replanned,\n",
    "    - liefert am Ende plan['final'] zurück.\n",
    "    \"\"\"\n",
    "    if tool_history is None:\n",
    "        tool_history = []\n",
    "\n",
    "    tool_results: Dict[str, Any] = {}\n",
    "\n",
    "    executed_step_indices: Set[int] = set()\n",
    "\n",
    "    # User-Kontext für $ref: \"user.*\"\n",
    "    user_ctx = {\n",
    "        \"raw\": user_input,\n",
    "        # hier könntest du später origin/destination/etc. extrahieren\n",
    "    }\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        # 1) Prompt bauen + Plan vom Modell holen\n",
    "        prompt = build_prompt_fn(\n",
    "            system_prompt=system_prompt,\n",
    "            user_prompt=user_input,\n",
    "            history=history,\n",
    "            tool_history=tool_history,\n",
    "        )\n",
    "\n",
    "        print(f\"iteration: {iteration}, prompt: {prompt!r}\")\n",
    "        assistant_text = llama_chat(prompt, max_new_tokens=1024)\n",
    "        print(f\"iteration: {iteration}, assistant_text: {assistant_text!r}\")\n",
    "\n",
    "        # 2) Plan parsen\n",
    "        plan = agentic_parse_model_plan(assistant_text)\n",
    "        print(f\"iteration: {iteration}, plan: {plan!r}\")\n",
    "\n",
    "        # 3) Wenn final schon vorhanden und keine offenen Steps → fertig\n",
    "        next_idx = agentic_find_next_step(plan, executed_step_indices)\n",
    "        print(f\"iteration: {iteration}, next_idx: {next_idx!r}\")\n",
    "\n",
    "        if plan.get(\"final\") is not None and next_idx is None:\n",
    "            # finale Antwort\n",
    "            final = plan[\"final\"]\n",
    "            print(f\"iteration: {iteration}, final: {final}\")\n",
    "            return plan[\"final\"]\n",
    "\n",
    "        # 4) Falls es einen nächsten Step gibt → ausführen\n",
    "        if next_idx is not None:\n",
    "            step = plan[\"steps\"][next_idx]\n",
    "            step_history_entries = await agentic_execute_step(\n",
    "                step=step,\n",
    "                call_tool_fn=call_tool_fn,\n",
    "                tool_results=tool_results,\n",
    "                user_ctx=user_ctx,\n",
    "            )\n",
    "            executed_step_indices.add(next_idx)\n",
    "\n",
    "            # Tool-Historie erweitern (für nächsten Prompt)\n",
    "            tool_history.extend(step_history_entries)\n",
    "\n",
    "            # Nach jedem Step neue Iteration starten → replanning möglich\n",
    "            continue\n",
    "\n",
    "        # 5) Kein Step mehr, aber final ist noch None → Modell „nötigen“, final zu setzen\n",
    "        if plan.get(\"final\") is None and next_idx is None:\n",
    "            # Einfach eine weitere Runde, diesmal mit Hinweis im User- oder Systemprompt,\n",
    "            # dass jetzt eine finale Antwort formuliert werden soll.\n",
    "            user_input = \"Formuliere jetzt bitte eine finale Antwort für den Nutzer basierend auf allen bisherigen Tool-Ergebnissen.\"\n",
    "            continue\n",
    "\n",
    "    # Fallback, falls max_iterations erreicht\n",
    "    return \"Ich konnte trotz mehrerer Planungsrunden keine finale Antwort erzeugen.\""
   ],
   "id": "1cccdd33fdfbb410",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MCP-Tool-Invoker für den Agenten: agentic_call_mcp_tool\n",
    "\n",
    "### Funktion\n",
    "- Standardisierte Funktion, die der Agent für jedes Tool verwendet:\n",
    "        - baut HTTP-Client,\n",
    "        - initialisiert Session,\n",
    "        - ruft session.call_tool(tool_name, args) auf,\n",
    "        - normalisiert das Ergebnis in ein konsistentes Dict.\n",
    "\n",
    "### Input\n",
    "- tool_name: str – z. B. \"geocode\".\n",
    "- args: dict – aufgelöste Argumente.\n",
    "- MCP_URL – globaler MCP-Endpoint.\n",
    "\n",
    "### Output\n",
    "- dict:\n",
    "    - {\"isError\": bool, \"data\": ...}.\n",
    "    - data ist entweder:\n",
    "        - raw_result.structuredContent (wenn vorhanden),\n",
    "        - oder zusammengefasster Text aus raw_result.content.\n",
    "- Debug-Prints:\n",
    "    - Toolname + Args,\n",
    "    - Typ und Repräsentation des rohen Ergebnisses."
   ],
   "id": "c867d33f8a8285ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from mcp.types import TextContent  # ggf. anpassen\n",
    "\n",
    "async def agentic_call_mcp_tool(tool_name: str, args: dict) -> dict:\n",
    "    print(f\"[DEBUG] agentic_call_mcp_tool: {tool_name}({args})\")\n",
    "    async with streamable_http_client(MCP_URL) as (read_stream, write_stream, _):\n",
    "        async with ClientSession(read_stream, write_stream) as session:\n",
    "            await session.initialize()\n",
    "            raw_result = await session.call_tool(tool_name, args)\n",
    "            print(f\"[DEBUG] Raw tool result type: {type(raw_result)}\")\n",
    "            print(f\"[DEBUG] Raw tool result repr: {repr(raw_result)[:400]}\")\n",
    "\n",
    "            # Normalisieren\n",
    "            result_dict: dict = {\n",
    "                \"isError\": getattr(raw_result, \"isError\", False),\n",
    "            }\n",
    "\n",
    "            # strukturierte Inhalte bevorzugen\n",
    "            if getattr(raw_result, \"structuredContent\", None) is not None:\n",
    "                result_dict[\"data\"] = raw_result.structuredContent\n",
    "            else:\n",
    "                # Fallback: Text aus content zusammenbauen\n",
    "                texts = []\n",
    "                for c in getattr(raw_result, \"content\", []) or []:\n",
    "                    if isinstance(c, TextContent):\n",
    "                        texts.append(c.text)\n",
    "                result_dict[\"data\"] = \"\\n\".join(texts)\n",
    "\n",
    "            return result_dict"
   ],
   "id": "7c56c7a0d920cf39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vollagentische Demo 1: Wetter heute in Barcelona\n",
    "\n",
    "### Funktion\n",
    "- Läuft den kompletten Agentenloop einmal durch:\n",
    "    - User: „Wie ist heute (YYYY-MM-DD) das Wetter in Barcelona?“\n",
    "    - Agent:\n",
    "        - plant Toolkette (geocode → get_weather),\n",
    "        - ruft Tools über agentic_call_mcp_tool aus,\n",
    "        - aktualisiert Plan/History iterativ,\n",
    "        - liefert finale Antwort an „Max Mustermann“.\n",
    "\n",
    "### Input\n",
    "- history_agentic: Start-History:\n",
    "    - Userinstruktion zur Anrede („Sprich mich bitte … Max Mustermann …“).\n",
    "- user_input: Wetterfrage mit Datum.\n",
    "- agent_system_prompt, agentic_call_mcp_tool.\n",
    "\n",
    "### Output\n",
    "- final_answer: str – fertige Reise/Wetter-Antwort, die:\n",
    "    - das Wetter für Barcelona beschreibt,\n",
    "    - Tools und echte Daten genutzt hat,\n",
    "    - die Anrede „Max Mustermann“ respektiert.\n",
    "\n",
    "### Konsole:\n",
    "- „Finale Antwort an den User:“ + Text."
   ],
   "id": "ae439b10df7e7d0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#history_agentic = [\n",
    "#    {\n",
    "#        \"role\": \"user\",\n",
    "#        \"content\": \"Hi, mein Name ist Max Mustermann. Sprich mich bitte in jeder Antwort mit meinem Namen an. Sag explizit dazu, dass du mich in jeder Antwort mit meinem Namen ansprechen wirst.\"\n",
    "#    },\n",
    "#    # weitere Messages:\n",
    "#    # {\"role\": \"assistant\", \"content\": \"...\"},\n",
    "#]\n",
    "\n",
    "user_input = f\"Ich möchte eine Reise nach Barcelona machen. Wie ist heute ({today}) das Wetter in Barcelona?\"\n",
    "history_agentic = []\n",
    "\n",
    "final_answer = await agentic_run_to_final_answer(\n",
    "    user_input=user_input,\n",
    "    system_prompt=agent_system_prompt,\n",
    "    history=history_agentic,\n",
    "    tool_history=None,\n",
    "    call_tool_fn=agentic_call_mcp_tool,          # deine bestehende MCP-Tool-Invoker-Funktion\n",
    "    build_prompt_fn=agentic_build_chat_prompt_with_rag_and_tools,\n",
    ")\n",
    "\n",
    "print(\"Finale Antwort an den User:\")\n",
    "print(final_answer)\n"
   ],
   "id": "dba5123ff00c321a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vollagentische Demo 2: Beste Jogging-Spots um Barcelona\n",
    "\n",
    "### Funktion\n",
    "- Zeigt, wie der Agent mehrstufige Tool-Chains planen kann:\n",
    "    - Geocoding von Barcelona,\n",
    "    - Parks in Barcekona,\n",
    "    - Abstand der Parks zum Zentrum.\n",
    "\n",
    "### Input\n",
    "- history_agentic: wieder mit Anrede-Instruktion.\n",
    "- user_input: „Was sind die besten Plätze im Umkreis von 5km rund um Barcelona um zu joggen?“\n",
    "- Rest wie oben.\n",
    "\n",
    "### Output\n",
    "- final_answer: str – Empfehlungsliste von Jogging-Spots:\n",
    "- sortiert nach Score/Distanz\n",
    "\n",
    "### Intern:\n",
    "- Agent plant und ruft Tools wie geocode, get_spots oder rank_spots auf,\n",
    "- verbindet die Ergebnisse über $ref."
   ],
   "id": "84f56a91b3695233"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history_agentic = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hi, mein Name ist Max Mustermann. Sprich mich bitte in jeder Antwort mit meinem Namen an. Sag explizit dazu, dass du mich in jeder Antwort mit meinem Namen ansprechen wirst.\"\n",
    "    },\n",
    "    # weitere Messages:\n",
    "    # {\"role\": \"assistant\", \"content\": \"...\"},\n",
    "]\n",
    "\n",
    "user_input = f\"Was sind die besten Plätze im Umkreis von 5km rund um Barcelona um zu joggen?\"\n",
    "\n",
    "final_answer = await agentic_run_to_final_answer(\n",
    "    user_input=user_input,\n",
    "    system_prompt=agent_system_prompt,\n",
    "    history=history_agentic,\n",
    "    tool_history=None,\n",
    "    call_tool_fn=agentic_call_mcp_tool,          # deine bestehende MCP-Tool-Invoker-Funktion\n",
    "    build_prompt_fn=agentic_build_chat_prompt_with_rag_and_tools,\n",
    ")\n",
    "\n",
    "print(\"Finale Antwort an den User:\")\n",
    "print(final_answer)"
   ],
   "id": "a9501e62b5ed0fc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vollagentische Demo 3: RAG + Tools mit Fantasietier Razepato\n",
    "\n",
    "### Funktion\n",
    "- Vereint alle Bausteine:\n",
    "    - RAG-Wissen zum Fantasietier Razepato (aus Teil 2),\n",
    "    - Geodaten / Weather / Spots via MCP,\n",
    "    - Agentenplanung und Tool-Ketten.\n",
    "- Beispiel:\n",
    "    - User: „Ich möchte ein Razepato in Barcelona sehen. Wann und wohin muss ich reisen?“\n",
    "    - Agent:\n",
    "        - nutzt RAG-Kontext zum Razepato:\n",
    "            - Lebensraum, Tageszeit, Jahreszeit,\n",
    "        - identifiziert geeignete Regionen/Plätze (ggf. Parks),\n",
    "        - kombiniert das mit Wetter/Season-Daten,\n",
    "        - beantwortet die Frage im Reiseplan-Stil.\n",
    "\n",
    "### Input\n",
    "- history_agentic: mit der Namensinstruktion.\n",
    "- user_input: Razepato-Reisewunsch.\n",
    "- agent_system_prompt, RAG-Funktionen (via Prompt-Builder), MCP-Tools.\n",
    "\n",
    "### Output\n",
    "- final_answer: str – eine Agentenantwort, die:\n",
    "    - auf dem RAG-Text über das Razepato basiert,\n",
    "    - realistische Orte/Zeitfenster in Barcelona nennt (innerhalb des fiktiven Kontextes),\n",
    "    - Tools dort nutzt, wo echte Daten nötig sind (z. B. Wetter),\n",
    "    - wieder „Max Mustermann“ richtig anspricht."
   ],
   "id": "4b5053708c6d2ade"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history_agentic = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hi, mein Name ist Max Mustermann. Sprich mich bitte in jeder Antwort mit meinem Namen an. Sag explizit dazu, dass du mich in jeder Antwort mit meinem Namen ansprechen wirst.\"\n",
    "    },\n",
    "    # weitere Messages:\n",
    "    # {\"role\": \"assistant\", \"content\": \"...\"},\n",
    "]\n",
    "\n",
    "user_input = f\"Ich möchte ein Razepato in Barcelona sehen. Wann und wohin muss ich reisen?\"\n",
    "\n",
    "final_answer = await agentic_run_to_final_answer(\n",
    "    user_input=user_input,\n",
    "    system_prompt=agent_system_prompt,\n",
    "    history=history_agentic,\n",
    "    tool_history=None,\n",
    "    call_tool_fn=agentic_call_mcp_tool,          # deine bestehende MCP-Tool-Invoker-Funktion\n",
    "    build_prompt_fn=agentic_build_chat_prompt_with_rag_and_tools,\n",
    ")\n",
    "\n",
    "print(\"Finale Antwort an den User:\")\n",
    "print(final_answer)"
   ],
   "id": "6cee80da389bc17d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 7
}
