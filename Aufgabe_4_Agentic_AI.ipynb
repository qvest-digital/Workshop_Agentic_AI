{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Huggingface Token setzen",
   "id": "fb69d85b402d285a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T12:20:01.761720Z",
     "start_time": "2026-02-01T12:20:01.756249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "hf_token = \"hf_Token\"\n",
    "#hf_token = os.getenv(\"HF_TOKEN\")"
   ],
   "id": "5eac01739c967077",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Git Repo per HTTPs Clonen",
   "id": "2c5eacb992777eab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T12:20:02.747237Z",
     "start_time": "2026-02-01T12:20:01.842338Z"
    }
   },
   "cell_type": "code",
   "source": "!git clone https://github.com/qvest-digital/Workshop_Agentic_AI.git",
   "id": "f89099ca1ee8918c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Cloning into 'Workshop_Agentic_AI'...\r\n",
      "remote: Enumerating objects: 216, done.\u001B[K\r\n",
      "remote: Counting objects: 100% (216/216), done.\u001B[K\r\n",
      "remote: Compressing objects: 100% (128/128), done.\u001B[K\r\n",
      "remote: Total 216 (delta 96), reused 193 (delta 73), pack-reused 0 (from 0)\u001B[K\r\n",
      "Receiving objects: 100% (216/216), 365.80 KiB | 5.38 MiB/s, done.\r\n",
      "Resolving deltas: 100% (96/96), done.\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pfad setzen",
   "id": "b398c4d5562a7858"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T12:20:02.844043Z",
     "start_time": "2026-02-01T12:20:02.841033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#SYSTEM_PATH = \"/home/simon/Workshop_Agentic_AI\"\n",
    "SYSTEM_PATH = \"./Workshop_Agentic_AI\""
   ],
   "id": "95c9b36fee07fd1b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Requirements installieren",
   "id": "78aa612f2a446737"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T12:20:05.173499Z",
     "start_time": "2026-02-01T12:20:02.922785Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -r \"$SYSTEM_PATH/requirements.txt\"",
   "id": "88d6e57ee340b5d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: jupyter in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 1)) (1.1.1)\r\n",
      "Requirement already satisfied: pandas in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 2)) (2.3.3)\r\n",
      "Requirement already satisfied: pydantic in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 3)) (2.12.5)\r\n",
      "Requirement already satisfied: mcp==1.25.0 in /home/simon/.local/lib/python3.10/site-packages (from mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (1.25.0)\r\n",
      "Requirement already satisfied: requests in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 5)) (2.32.5)\r\n",
      "Requirement already satisfied: transformers in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 6)) (4.57.6)\r\n",
      "Requirement already satisfied: huggingface-hub in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 7)) (0.36.0)\r\n",
      "Requirement already satisfied: torch in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 8)) (2.9.1)\r\n",
      "Requirement already satisfied: accelerate in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 9)) (1.12.0)\r\n",
      "Requirement already satisfied: bitsandbytes in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 10)) (0.49.1)\r\n",
      "Requirement already satisfied: sentence-transformers in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 11)) (5.2.0)\r\n",
      "Requirement already satisfied: faiss-cpu in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 12)) (1.13.2)\r\n",
      "Requirement already satisfied: nltk in /home/simon/.local/lib/python3.10/site-packages (from -r ./Workshop_Agentic_AI/requirements.txt (line 13)) (3.9.2)\r\n",
      "Requirement already satisfied: anyio>=4.5 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (4.11.0)\r\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.4.3)\r\n",
      "Requirement already satisfied: httpx>=0.27.1 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.28.1)\r\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (4.23.0)\r\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (2.12.0)\r\n",
      "Requirement already satisfied: pyjwt>=2.10.1 in /home/simon/.local/lib/python3.10/site-packages (from pyjwt[crypto]>=2.10.1->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (2.10.1)\r\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.0.21)\r\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (3.1.2)\r\n",
      "Requirement already satisfied: starlette>=0.27 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.51.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (4.15.0)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.1 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.4.2)\r\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in /home/simon/.local/lib/python3.10/site-packages (from mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.40.0)\r\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /home/simon/.local/lib/python3.10/site-packages (from mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (1.2.1)\r\n",
      "Requirement already satisfied: typer>=0.16.0 in /home/simon/.local/lib/python3.10/site-packages (from mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.19.2)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/simon/.local/lib/python3.10/site-packages (from pydantic->-r ./Workshop_Agentic_AI/requirements.txt (line 3)) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/simon/.local/lib/python3.10/site-packages (from pydantic->-r ./Workshop_Agentic_AI/requirements.txt (line 3)) (2.41.5)\r\n",
      "Requirement already satisfied: notebook in /home/simon/.local/lib/python3.10/site-packages (from jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (7.5.1)\r\n",
      "Requirement already satisfied: jupyter-console in /home/simon/.local/lib/python3.10/site-packages (from jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (6.6.3)\r\n",
      "Requirement already satisfied: nbconvert in /home/simon/.local/lib/python3.10/site-packages (from jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (7.16.6)\r\n",
      "Requirement already satisfied: ipykernel in /home/simon/.local/lib/python3.10/site-packages (from jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (7.1.0)\r\n",
      "Requirement already satisfied: ipywidgets in /home/simon/.local/lib/python3.10/site-packages (from jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (8.1.8)\r\n",
      "Requirement already satisfied: jupyterlab in /home/simon/.local/lib/python3.10/site-packages (from jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (4.5.1)\r\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/simon/.local/lib/python3.10/site-packages (from pandas->-r ./Workshop_Agentic_AI/requirements.txt (line 2)) (1.26.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/simon/.local/lib/python3.10/site-packages (from pandas->-r ./Workshop_Agentic_AI/requirements.txt (line 2)) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/simon/.local/lib/python3.10/site-packages (from pandas->-r ./Workshop_Agentic_AI/requirements.txt (line 2)) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/simon/.local/lib/python3.10/site-packages (from pandas->-r ./Workshop_Agentic_AI/requirements.txt (line 2)) (2024.2)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/simon/.local/lib/python3.10/site-packages (from requests->-r ./Workshop_Agentic_AI/requirements.txt (line 5)) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/simon/.local/lib/python3.10/site-packages (from requests->-r ./Workshop_Agentic_AI/requirements.txt (line 5)) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/simon/.local/lib/python3.10/site-packages (from requests->-r ./Workshop_Agentic_AI/requirements.txt (line 5)) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/simon/.local/lib/python3.10/site-packages (from requests->-r ./Workshop_Agentic_AI/requirements.txt (line 5)) (2024.2.2)\r\n",
      "Requirement already satisfied: filelock in /home/simon/.local/lib/python3.10/site-packages (from transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 6)) (3.16.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/simon/.local/lib/python3.10/site-packages (from transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 6)) (24.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 6)) (5.4.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/simon/.local/lib/python3.10/site-packages (from transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 6)) (2024.9.11)\r\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/simon/.local/lib/python3.10/site-packages (from transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 6)) (0.22.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/simon/.local/lib/python3.10/site-packages (from transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 6)) (0.4.5)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/simon/.local/lib/python3.10/site-packages (from transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 6)) (4.66.6)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/simon/.local/lib/python3.10/site-packages (from huggingface-hub->-r ./Workshop_Agentic_AI/requirements.txt (line 7)) (2024.9.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/simon/.local/lib/python3.10/site-packages (from huggingface-hub->-r ./Workshop_Agentic_AI/requirements.txt (line 7)) (1.2.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (1.14.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (3.1.5)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (12.8.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (11.3.3.83)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (10.3.9.90)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (11.7.3.90)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (12.5.8.93)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (2.27.5)\r\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (3.3.20)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (1.13.1.3)\r\n",
      "Requirement already satisfied: triton==3.5.1 in /home/simon/.local/lib/python3.10/site-packages (from torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (3.5.1)\r\n",
      "Requirement already satisfied: psutil in /home/simon/.local/lib/python3.10/site-packages (from accelerate->-r ./Workshop_Agentic_AI/requirements.txt (line 9)) (7.2.1)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 11)) (1.5.0)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 11)) (1.13.1)\r\n",
      "Requirement already satisfied: click in /home/simon/.local/lib/python3.10/site-packages (from nltk->-r ./Workshop_Agentic_AI/requirements.txt (line 13)) (8.1.3)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r ./Workshop_Agentic_AI/requirements.txt (line 13)) (1.4.2)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/simon/.local/lib/python3.10/site-packages (from anyio>=4.5->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (1.2.2)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/simon/.local/lib/python3.10/site-packages (from anyio>=4.5->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (1.3.1)\r\n",
      "Requirement already satisfied: httpcore==1.* in /home/simon/.local/lib/python3.10/site-packages (from httpx>=0.27.1->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (1.0.9)\r\n",
      "Requirement already satisfied: h11>=0.16 in /home/simon/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.1->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.16.0)\r\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/simon/.local/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (24.2.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/simon/.local/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (2024.10.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/simon/.local/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.36.2)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/simon/.local/lib/python3.10/site-packages (from jsonschema>=4.20.0->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.22.3)\r\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /usr/lib/python3/dist-packages (from pyjwt[crypto]>=2.10.1->mcp==1.25.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (3.4.8)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->-r ./Workshop_Agentic_AI/requirements.txt (line 2)) (1.16.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/simon/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (1.3.0)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/simon/.local/lib/python3.10/site-packages (from typer>=0.16.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/simon/.local/lib/python3.10/site-packages (from typer>=0.16.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (13.5.3)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/simon/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.16.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/simon/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.16.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (2.18.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/simon/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.16.0->mcp[cli]==1.25.0->-r ./Workshop_Agentic_AI/requirements.txt (line 4)) (0.1.2)\r\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.2.3)\r\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (1.8.12)\r\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (8.38.0)\r\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (8.8.0)\r\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (5.9.1)\r\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.2.1)\r\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (1.6.0)\r\n",
      "Requirement already satisfied: pyzmq>=25 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (27.1.0)\r\n",
      "Requirement already satisfied: tornado>=6.2 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (6.5.4)\r\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /home/simon/.local/lib/python3.10/site-packages (from ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (5.14.3)\r\n",
      "Requirement already satisfied: decorator in /home/simon/.local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (5.2.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /home/simon/.local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.19.2)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /home/simon/.local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (4.9.0)\r\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/simon/.local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (3.0.50)\r\n",
      "Requirement already satisfied: stack_data in /home/simon/.local/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.6.3)\r\n",
      "Requirement already satisfied: wcwidth in /home/simon/.local/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.2.13)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/simon/.local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.8.5)\r\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (4.3.6)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/simon/.local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.7.0)\r\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/simon/.local/lib/python3.10/site-packages (from ipywidgets->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (4.0.15)\r\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/simon/.local/lib/python3.10/site-packages (from ipywidgets->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (3.0.16)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/simon/.local/lib/python3.10/site-packages (from jinja2->torch->-r ./Workshop_Agentic_AI/requirements.txt (line 8)) (2.1.5)\r\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.0.5)\r\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.3.0)\r\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.17.0)\r\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.28.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.28.0)\r\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.2.4)\r\n",
      "Requirement already satisfied: setuptools>=41.1.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (80.9.0)\r\n",
      "Requirement already satisfied: tomli>=1.2.2 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.0.2)\r\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (25.1.0)\r\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.12.0)\r\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.5.3)\r\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (5.10.4)\r\n",
      "Requirement already satisfied: overrides>=5.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (7.7.0)\r\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.23.1)\r\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.0.0)\r\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.18.1)\r\n",
      "Requirement already satisfied: websocket-client>=1.7 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (1.8.0)\r\n",
      "Requirement already satisfied: babel>=2.10 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.17.0)\r\n",
      "Requirement already satisfied: json5>=0.9.0 in /home/simon/.local/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.13.0)\r\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/simon/.local/lib/python3.10/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (25.1.0)\r\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (4.0.0)\r\n",
      "Requirement already satisfied: rfc3339-validator in /home/simon/.local/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.1.4)\r\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /home/simon/.local/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.1.1)\r\n",
      "Requirement already satisfied: fqdn in /home/simon/.local/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (1.5.1)\r\n",
      "Requirement already satisfied: isoduration in /home/simon/.local/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (20.11.0)\r\n",
      "Requirement already satisfied: jsonpointer>1.13 in /usr/lib/python3/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.0)\r\n",
      "Requirement already satisfied: uri-template in /home/simon/.local/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (1.3.0)\r\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /home/simon/.local/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (25.10.0)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /home/simon/.local/lib/python3.10/site-packages (from nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (4.14.3)\r\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/simon/.local/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (6.3.0)\r\n",
      "Requirement already satisfied: defusedxml in /home/simon/.local/lib/python3.10/site-packages (from nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.7.1)\r\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/simon/.local/lib/python3.10/site-packages (from nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.3.0)\r\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/simon/.local/lib/python3.10/site-packages (from nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (3.2.0)\r\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/simon/.local/lib/python3.10/site-packages (from nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.10.4)\r\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/simon/.local/lib/python3.10/site-packages (from nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (1.5.1)\r\n",
      "Requirement already satisfied: webencodings in /home/simon/.local/lib/python3.10/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.5.1)\r\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /home/simon/.local/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (1.4.0)\r\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/simon/.local/lib/python3.10/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.21.2)\r\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/simon/.local/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.0.0)\r\n",
      "Requirement already satisfied: pycparser in /home/simon/.local/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.23)\r\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /home/simon/.local/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.8.1)\r\n",
      "Requirement already satisfied: arrow>=0.15.0 in /home/simon/.local/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (1.2.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r ./Workshop_Agentic_AI/requirements.txt (line 11)) (3.5.0)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/simon/.local/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (2.2.1)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/simon/.local/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (3.0.1)\r\n",
      "Requirement already satisfied: pure-eval in /home/simon/.local/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r ./Workshop_Agentic_AI/requirements.txt (line 1)) (0.2.3)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Speicherfragmentierung minimieren",
   "id": "b5f2cd6fc6137a3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T12:20:05.185889Z",
     "start_time": "2026-02-01T12:20:05.183272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ],
   "id": "3dacd62152e4753d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Herstellen der Vorbedingungen aus Teil 1:",
   "id": "e8b610e67aaca7b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Modell laden: lokal oder von Hugging Face\n",
    "\n",
    "### Funktion\n",
    "- Lädt ein Llama-3.1-Instruct-Modell entweder:\n",
    "  - lokal aus dem Ordner ./models/llama-3.1-8b, wenn der Pfad vorhanden ist, oder\n",
    "  - online von Hugging Face mit der Modell-ID meta-llama/Llama-3.1-8B-Instruct, und speichert es anschließend lokal ab.\n",
    "- Die Umgebungsvariablen werden mit load_dotenv() aus einer .env-Datei geladen, u. a. das Hugging-Face-Token.\n",
    "\n",
    "### Inputs\n",
    "- Dateisystem:\n",
    "  - Existenz von MODEL_PATH (./models/llama-3.1-8b).\n",
    "- Umgebungsvariable:\n",
    "  - HF_TOKEN (wird mit os.getenv(\"HF_TOKEN\") gelesen) – persönliches Zugriffstoken für Hugging Face.\n",
    "- Hyperparameter:\n",
    "  - MODEL_ID: gibt die zu ladende Modell-ID an.\n",
    "- Hardware:\n",
    "  - device_map=\"auto\" versucht automatisch, GPU(s) oder CPU sinnvoll zu nutzen.\n",
    "  - torch_dtype=\"auto\" bzw. dtype=\"auto\" lässt das Modell selbst einen sinnvollen Datentyp wählen (z. B. bfloat16 oder float16).\n",
    "\n",
    "### Outputs\n",
    "- Globale Python-Variablen:\n",
    "  - tokenizer: Instanz von AutoTokenizer, konfiguriert für das Llama-3.1-Modell.\n",
    "  - model: Instanz von AutoModelForCausalLM, bereit für Textgenerierung."
   ],
   "id": "5afe18bbe370ce7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T12:21:36.287252Z",
     "start_time": "2026-02-01T12:20:05.226838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_PATH = f\"{SYSTEM_PATH}/models/llama-3.1-8b\"\n",
    "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"Lade Modell lokal …\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained( MODEL_PATH, torch_dtype=\"auto\", device_map=\"auto\" )\n",
    "\n",
    "else:\n",
    "    print(\"Lade Modell von Hugging Face …\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=hf_token, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, token=hf_token, dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "    # lokal speichern\n",
    "    model.save_pretrained(MODEL_PATH)\n",
    "    tokenizer.save_pretrained(MODEL_PATH)\n",
    "\n"
   ],
   "id": "59bcb01b554dba17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Modell von Hugging Face …\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa36e91c28c24d3ea082f700adc55241"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pipeline erstellen\n",
    "### Funktion\n",
    "- Baut eine Hugging-Face-pipeline für Textgenerierung auf Basis des zuvor geladenen Modells und Tokenizers.\n",
    "- Diese Pipeline kapselt:\n",
    "  - Tokenisierung,\n",
    "  - das Aufrufen des Modells,\n",
    "  - und das Zurückkonvertieren der Token in Text.\n",
    "\n",
    "### Inputs\n",
    "- model: Causal-Language-Model (AutoModelForCausalLM), im vorherigen Block geladen.\n",
    "- tokenizer: passender Tokenizer zu diesem Modell (AutoTokenizer).\n",
    "- Task-Typ: \"text-generation\" – legt fest, dass es sich um eine generative Textaufgabe handelt.\n",
    "\n",
    "### Outputs\n",
    "- Variable:\n",
    "  - llm: eine aufrufbare Pipeline-Instanz.\n",
    "\n",
    "### Rückgabewert bei Aufruf von llm(...):\n",
    "```python\n",
    "[\n",
    "  {\n",
    "    \"generated_text\": \"Vollständiger generierter Text (inkl. Prompt oder abhängig von den Parametern)\"\n",
    "  }\n",
    "]\n",
    "```"
   ],
   "id": "a1c8ad6dc4125790"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T12:21:36.399115Z",
     "start_time": "2026-02-01T12:21:36.393211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ],
   "id": "be83605dc2120949",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Einfache Chat-Funktion zur Verfügung stellen\n",
    "\n",
    "### Funktion\n",
    "- Stellt ein vereinfachtes Chat-Interface zur Verfügung, das direkt mit einem String arbeitet.\n",
    "- Die Funktion:\n",
    "  - ruft intern die llm-Pipeline auf,\n",
    "  - übergibt alle relevanten Generationsparameter,\n",
    "  - und gibt am Ende nur den reinen generierten Text zurück (str statt verschachtelte Struktur).\n",
    "\n",
    "### Inputs\n",
    "- Pflichtparameter:\n",
    "  - prompt: str – der Eingabetext an das Modell.\n",
    "- Optionale Parameter zum Experimentieren\n",
    "  - max_new_tokens: maximale Anzahl neu zu generierender Token.\n",
    "  - temperature: steuert die Zufälligkeit (0 ≈ deterministischer, >0 zufälliger).\n",
    "  - top_k: Sampling nur aus den k wahrscheinlichsten Token.\n",
    "  - top_p: „Nucleus Sampling“ – Auswahl aus der kleinsten Masse der wahrscheinlichsten Token, deren Summe ≥ p ist.\n",
    "  - typical_p: Bevorzugt Token mit kontexttypischer Wahrscheinlichkeit statt nur der höchsten.\n",
    "  - repetition_penalty: >1.0 bestraft Wiederholungen.\n",
    "  - length_penalty: Steuert, ob Beam Search kürzere oder längere Sequenzen bevorzugt.\n",
    "  - no_repeat_ngram_size: Verhindert die Wiederholung identischer n-Gramme.\n",
    "  - num_beams, num_beam_groups, diversity_penalty: Parameter für Beam Search (systematisches Durchsuchen mehrerer Kandidaten).\n",
    "  - early_stopping: beendet Beam Search frühzeitig, wenn bestimmte Kriterien erfüllt sind.\n",
    "\n",
    "### Outputs\n",
    "- Rückgabewert:\n",
    "  - str: der vom Modell generierte Antworttext (out[0][\"generated_text\"] ohne führende/trailing Leerzeichen).\n",
    "\n",
    "### Typische Verwendung:\n",
    "- llama_chat(\"Erkläre mir kurz, was ein LLM ist.\")\n",
    "\n",
    "In späteren Zellen wird statt eines rohen Prompts ein Chat-Prompt übergeben, der mit build_chat_prompt erzeugt wird."
   ],
   "id": "33bfb746f4fbc19a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T12:21:36.450697Z",
     "start_time": "2026-02-01T12:21:36.445544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def llama_chat(\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 128,\n",
    "        temperature: float = 0.01,\n",
    "        top_k: int = 50,\n",
    "        top_p: float = 1.0,\n",
    "        typical_p: float = 1.0,\n",
    "        repetition_penalty: float = 1.0,\n",
    "        length_penalty: float = 1.0,\n",
    "        no_repeat_ngram_size: int = 0,\n",
    "        num_beams: int = 1,\n",
    "        num_beam_groups: int = 1,\n",
    "        diversity_penalty: float = 0.0,\n",
    "        early_stopping: bool = False,) -> str:\n",
    "    \"\"\"Sehr simples Wrapper-Interface.\n",
    "    Wir verwenden ein 'single prompt' Format, um es notebook-tauglich zu halten.\n",
    "    \"\"\"\n",
    "    out = llm(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=(temperature > 0),\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        typical_p=typical_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        length_penalty=length_penalty,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_beams=num_beams,\n",
    "        num_beam_groups=num_beam_groups,\n",
    "        diversity_penalty=diversity_penalty,\n",
    "        early_stopping=early_stopping,\n",
    "        return_full_text=False,\n",
    "        eos_token_id=llm.tokenizer.eos_token_id,\n",
    "        pad_token_id=llm.tokenizer.pad_token_id,\n",
    "    )\n",
    "    return out[0][\"generated_text\"].strip()"
   ],
   "id": "d3188ae855da3f55",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Zeit zum Experimentieren (5 - 10 min.)",
   "id": "fac07fb5befd24c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Herstellen der Vorbedingungen aus Teil 3:",
   "id": "4078700a8799331"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MCP-Server als Subprozess starten\n",
    "\n",
    "### Funktion\n",
    "- Startet einen MCP-Server als separaten Python-Prozess über subprocess. Popen.\n",
    "- Setzt PYTHONPATH so, dass mcp_server.mcp_tools.mcp.server als Modul importierbar ist.\n",
    "- Hält die Projektpfade und Host/Port-Konfiguration zusammen.\n",
    "\n",
    "### Input\n",
    "- Konstanten:\n",
    "    - PROJECT_ROOT / project_root: Pfad zum Projekt (SYSTEM_PATH).\n",
    "    - MODULE_NAME: mcp_server.mcp_tools.mcp.server.\n",
    "    - HOST, PORT: 127.0.0.1:8765.\n",
    "\n",
    "### Umgebungsvariablen:\n",
    "- Kopie von os.environ, erweitert um PYTHONPATH\n",
    "\n",
    "### Output\n",
    "- Subprozess proc:\n",
    "    - MCP-Server läuft im Hintergrund unter der angegebenen PID.\n",
    "\n",
    "### Konsole:\n",
    "- Server-PID: ... zur Kontrolle, dass der Server wirklich gestartet wurde."
   ],
   "id": "675e5a13e5bcd543"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T12:21:36.500172Z",
     "start_time": "2026-02-01T12:21:36.496328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess, sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Konfiguration\n",
    "# -------------------------------------------------------------------\n",
    "PROJECT_ROOT = Path(SYSTEM_PATH).resolve()\n",
    "HOST = \"127.0.0.1\"\n",
    "PORT = 8765\n",
    "MCP_URL = f\"http://{HOST}:{PORT}/mcp\"\n",
    "\n",
    "MODULE_NAME = \"mcp_server_math.mcp_tools_math.mcp.server\"\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"PYTHONPATH\"] = str(PROJECT_ROOT)  # falls nötig, damit mcp_tools_travel importierbar ist"
   ],
   "id": "721754d0f576b3a9",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")",
   "id": "2a829c1a33643f2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:40:49.800383Z",
     "start_time": "2026-01-31T09:40:49.792300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cmd = [\n",
    "    sys.executable,\n",
    "    \"-m\",\n",
    "    MODULE_NAME,\n",
    "    \"--host\",\n",
    "    HOST,\n",
    "    \"--port\",\n",
    "    str(PORT),\n",
    "]\n",
    "\n",
    "proc = subprocess.Popen(cmd, cwd=str(PROJECT_ROOT), env=env)\n",
    "print(\"Server-PID:\", proc.pid)\n"
   ],
   "id": "d50aa4e8c534e1f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-PID: 904\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MCP-Tools mit Metadaten vom Server abholen\n",
    "\n",
    "### Funktion\n",
    "- Baut eine Client-Verbindung zum MCP-Server via HTTP-Transport auf.\n",
    "- Führt session.initialize() aus.\n",
    "- Ruft session.list_tools() auf und gibt die komplette Liste der Tools inkl. Metadaten zurück.\n",
    "- Hat ein Retry-Verhalten, falls der Server noch nicht bereit ist.\n",
    "\n",
    "### Input\n",
    "- url: str = MCP_URL – z. B. http://127.0.0.1:8765/mcp.\n",
    "- retries: int = 50, delay: float = 0.1 – wie oft und wie lange gewartet wird, bis der Server erreichbar ist.\n",
    "\n",
    "### Output\n",
    "- Rückgabewert von fetch_tools_with_metadata(...):\n",
    "    - tools_resp.tools: Liste von Tool-Objekten (inkl. Name, Beschreibung, Input-/Output-Schema).\n",
    "- Bei Fehlern nach allen Retries:\n",
    "    - RuntimeError(\"Keine Verbindung zum MCP-Server möglich: ...\")."
   ],
   "id": "fd77a5d09c49a4af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:40:53.975028Z",
     "start_time": "2026-01-31T09:40:52.322115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "from mcp.client.streamable_http import streamable_http_client\n",
    "from mcp import ClientSession\n",
    "\n",
    "async def fetch_tools_with_metadata(\n",
    "    url: str = MCP_URL,\n",
    "    retries: int = 50,\n",
    "    delay: float = 0.1,\n",
    "):\n",
    "    last_err = None\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            async with streamable_http_client(url) as (read_stream, write_stream, _):\n",
    "                async with ClientSession(read_stream, write_stream) as session:\n",
    "                    await session.initialize()\n",
    "                    tools_resp = await session.list_tools()\n",
    "                    return tools_resp.tools  # komplette Objekte, nicht nur Namen\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            await asyncio.sleep(delay)\n",
    "    raise RuntimeError(f\"Keine Verbindung zum MCP-Server möglich: {last_err}\")"
   ],
   "id": "f578bec8f7702c55",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Roh-Toolliste inspizieren\n",
    "\n",
    "### Funktion\n",
    "- Führt await fetch_tools_with_metadata() aus und speichert das Ergebnis.\n",
    "- Gibt die unverarbeiteten Tool-Objekte auf der Konsole aus.\n",
    "\n",
    "### Input\n",
    "- MCP-Server muss laufen.\n",
    "- Kein weiterer Parameter, MCP_URL wird aus dem globalen Kontext genommen.\n",
    "\n",
    "### Output\n",
    "- Variable tool_names_with_meta – Liste der Tool-Objekte (Client-Strukturen).\n",
    "\n",
    "### Konsole:\n",
    "- Debug-Print der Liste, z. B. <Tool name='geocode' ...> etc."
   ],
   "id": "67062cf817b8626f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:40:56.341059Z",
     "start_time": "2026-01-31T09:40:56.237499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tool_names_with_meta = await fetch_tools_with_metadata()\n",
    "print(f\"tool_names_with_meta: {tool_names_with_meta}\")"
   ],
   "id": "807733544e5e92b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:43632 - \"POST /mcp HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:43648 - \"POST /mcp HTTP/1.1\" 202 Accepted\n",
      "INFO:     127.0.0.1:43662 - \"GET /mcp HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:43668 - \"POST /mcp HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:43680 - \"DELETE /mcp HTTP/1.1\" 200 OK\n",
      "tool_names_with_meta: [Tool(name='addition', title=None, description='Addiert a + b', inputSchema={'properties': {'a': {'title': 'A', 'type': 'number'}, 'b': {'title': 'B', 'type': 'number'}}, 'required': ['a', 'b'], 'title': 'additionArguments', 'type': 'object'}, outputSchema={'properties': {'result': {'title': 'Result', 'type': 'number'}}, 'required': ['result'], 'title': 'additionOutput', 'type': 'object'}, icons=None, annotations=None, meta=None, execution=None), Tool(name='subtraktion', title=None, description='Subtrahiert a - b', inputSchema={'properties': {'a': {'title': 'A', 'type': 'number'}, 'b': {'title': 'B', 'type': 'number'}}, 'required': ['a', 'b'], 'title': 'subtraktionArguments', 'type': 'object'}, outputSchema={'properties': {'result': {'title': 'Result', 'type': 'number'}}, 'required': ['result'], 'title': 'subtraktionOutput', 'type': 'object'}, icons=None, annotations=None, meta=None, execution=None), Tool(name='multiplikation', title=None, description='Multipliziert a * b', inputSchema={'properties': {'a': {'title': 'A', 'type': 'number'}, 'b': {'title': 'B', 'type': 'number'}}, 'required': ['a', 'b'], 'title': 'multiplikationArguments', 'type': 'object'}, outputSchema={'properties': {'result': {'title': 'Result', 'type': 'number'}}, 'required': ['result'], 'title': 'multiplikationOutput', 'type': 'object'}, icons=None, annotations=None, meta=None, execution=None), Tool(name='division', title=None, description='Dividiert a / b', inputSchema={'properties': {'a': {'title': 'A', 'type': 'number'}, 'b': {'title': 'B', 'type': 'number'}}, 'required': ['a', 'b'], 'title': 'divisionArguments', 'type': 'object'}, outputSchema={'properties': {'result': {'title': 'Result', 'type': 'number'}}, 'required': ['result'], 'title': 'divisionOutput', 'type': 'object'}, icons=None, annotations=None, meta=None, execution=None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2;36m[01/31/26 10:40:56]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m Created new          \u001B]8;id=536174;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/streamable_http_manager.py\u001B\\\u001B[2mstreamable_http_manager.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=148729;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/streamable_http_manager.py#239\u001B\\\u001B[2m239\u001B[0m\u001B]8;;\u001B\\\n",
      "\u001B[2;36m                    \u001B[0m         transport with       \u001B[2m                              \u001B[0m\n",
      "\u001B[2;36m                    \u001B[0m         session ID:          \u001B[2m                              \u001B[0m\n",
      "\u001B[2;36m                    \u001B[0m         9d71664d9db946d6be77 \u001B[2m                              \u001B[0m\n",
      "\u001B[2;36m                    \u001B[0m         a00bf9ced544         \u001B[2m                              \u001B[0m\n",
      "\u001B[2;36m                   \u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m Processing request of type            \u001B]8;id=458175;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/lowlevel/server.py\u001B\\\u001B[2mserver.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=110693;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/lowlevel/server.py#713\u001B\\\u001B[2m713\u001B[0m\u001B]8;;\u001B\\\n",
      "\u001B[2;36m                    \u001B[0m         ListToolsRequest                      \u001B[2m             \u001B[0m\n",
      "\u001B[2;36m                   \u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m Terminating session:         \u001B]8;id=391649;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/streamable_http.py\u001B\\\u001B[2mstreamable_http.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=739900;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/streamable_http.py#779\u001B\\\u001B[2m779\u001B[0m\u001B]8;;\u001B\\\n",
      "\u001B[2;36m                    \u001B[0m         9d71664d9db946d6be77a00bf9ce \u001B[2m                      \u001B[0m\n",
      "\u001B[2;36m                    \u001B[0m         d544                         \u001B[2m                      \u001B[0m\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tool-Metadaten für Prompts formatieren: format_tools_for_prompt\n",
    "\n",
    "### Funktion\n",
    "- Nimmt die MCP-Tool-Objekte und erzeugt eine lesbare Textbeschreibung für den System-Prompt:\n",
    "    - Name, Beschreibung,\n",
    "    - JSON-Input-Schema,\n",
    "    - JSON-Output-Schema.\n",
    "- Ist robust genug, um sowohl Dicts als auch Objekte mit Attributen (.name, .description, .input_schema, .output_schema) zu verarbeiten.\n",
    "\n",
    "### Input\n",
    "- tools: List[Any] – Liste der Tools, z. B. aus fetch_tools_with_metadata.\n",
    "\n",
    "### Output\n",
    "- str: Formatierte Version der Tools."
   ],
   "id": "42efc27580ac1dcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:40:58.672693Z",
     "start_time": "2026-01-31T09:40:58.666498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Any\n",
    "import json\n",
    "\n",
    "def format_tools_for_prompt(tools: List[Any]) -> str:\n",
    "    \"\"\"\n",
    "    Macht aus den MCP-Tool-Objekten einen lesbaren Katalog für das System-Prompt.\n",
    "    Funktioniert sowohl, wenn die Tools Dicts sind, als auch, wenn sie Attribute haben.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for t in tools:\n",
    "        is_dict = isinstance(t, dict)\n",
    "\n",
    "        # Name\n",
    "        name = getattr(t, \"name\", None)\n",
    "        if name is None and is_dict:\n",
    "            name = t.get(\"name\")\n",
    "\n",
    "        # Beschreibung\n",
    "        desc = getattr(t, \"description\", None)\n",
    "        if desc is None and is_dict:\n",
    "            desc = t.get(\"description\", \"\")\n",
    "        if desc is None:\n",
    "            desc = \"\"\n",
    "\n",
    "        # Input-Schema\n",
    "        input_schema = (\n",
    "            getattr(t, \"input_schema\", None)\n",
    "            or getattr(t, \"inputSchema\", None)\n",
    "            or (t.get(\"input_schema\") if is_dict else None)\n",
    "            or (t.get(\"inputSchema\") if is_dict else None)\n",
    "            or {}\n",
    "        )\n",
    "\n",
    "        # Output-Schema (neu)\n",
    "        output_schema = (\n",
    "            getattr(t, \"output_schema\", None)\n",
    "            or getattr(t, \"outputSchema\", None)\n",
    "            or (t.get(\"output_schema\") if is_dict else None)\n",
    "            or (t.get(\"outputSchema\") if is_dict else None)\n",
    "            or {}\n",
    "        )\n",
    "\n",
    "        lines.append(\n",
    "            f\"- Name: {name}\\n\"\n",
    "            f\"  Beschreibung: {desc}\\n\"\n",
    "            f\"  Eingabe-Schema (JSON): {json.dumps(input_schema, ensure_ascii=False)}\\n\"\n",
    "            f\"  Ausgabe-Schema (JSON): {json.dumps(output_schema, ensure_ascii=False)}\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\\n\".join(lines)"
   ],
   "id": "a87f3ed5c6f42942",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Formatierten Tool-Katalog ansehen\n",
    "\n",
    "### Funktion\n",
    "- Ruft format_tools_for_prompt(tool_names_with_meta) auf.\n",
    "- Gibt den formatierten Katalog auf der Konsole aus, damit du siehst, welche Tools der Server anbietet.\n",
    "\n",
    "### Input\n",
    "- tool_names_with_meta aus dem vorherigen Schritt.\n",
    "\n",
    "### Output\n",
    "- tool_names_with_meta_formated: str – fertiger Tool-Katalog-String.\n",
    "\n",
    "### Konsolenausgabe\n",
    "- Konsolenausgabe des Strings (für menschlichen Check)."
   ],
   "id": "20d6551af80e60f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:41:02.058355Z",
     "start_time": "2026-01-31T09:41:02.053920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tool_names_with_meta_formated = format_tools_for_prompt(tool_names_with_meta)\n",
    "print(tool_names_with_meta_formated)"
   ],
   "id": "77e1b8e6b55b8f16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Name: addition\n",
      "  Beschreibung: Addiert a + b\n",
      "  Eingabe-Schema (JSON): {\"properties\": {\"a\": {\"title\": \"A\", \"type\": \"number\"}, \"b\": {\"title\": \"B\", \"type\": \"number\"}}, \"required\": [\"a\", \"b\"], \"title\": \"additionArguments\", \"type\": \"object\"}\n",
      "  Ausgabe-Schema (JSON): {\"properties\": {\"result\": {\"title\": \"Result\", \"type\": \"number\"}}, \"required\": [\"result\"], \"title\": \"additionOutput\", \"type\": \"object\"}\n",
      "\n",
      "- Name: subtraktion\n",
      "  Beschreibung: Subtrahiert a - b\n",
      "  Eingabe-Schema (JSON): {\"properties\": {\"a\": {\"title\": \"A\", \"type\": \"number\"}, \"b\": {\"title\": \"B\", \"type\": \"number\"}}, \"required\": [\"a\", \"b\"], \"title\": \"subtraktionArguments\", \"type\": \"object\"}\n",
      "  Ausgabe-Schema (JSON): {\"properties\": {\"result\": {\"title\": \"Result\", \"type\": \"number\"}}, \"required\": [\"result\"], \"title\": \"subtraktionOutput\", \"type\": \"object\"}\n",
      "\n",
      "- Name: multiplikation\n",
      "  Beschreibung: Multipliziert a * b\n",
      "  Eingabe-Schema (JSON): {\"properties\": {\"a\": {\"title\": \"A\", \"type\": \"number\"}, \"b\": {\"title\": \"B\", \"type\": \"number\"}}, \"required\": [\"a\", \"b\"], \"title\": \"multiplikationArguments\", \"type\": \"object\"}\n",
      "  Ausgabe-Schema (JSON): {\"properties\": {\"result\": {\"title\": \"Result\", \"type\": \"number\"}}, \"required\": [\"result\"], \"title\": \"multiplikationOutput\", \"type\": \"object\"}\n",
      "\n",
      "- Name: division\n",
      "  Beschreibung: Dividiert a / b\n",
      "  Eingabe-Schema (JSON): {\"properties\": {\"a\": {\"title\": \"A\", \"type\": \"number\"}, \"b\": {\"title\": \"B\", \"type\": \"number\"}}, \"required\": [\"a\", \"b\"], \"title\": \"divisionArguments\", \"type\": \"object\"}\n",
      "  Ausgabe-Schema (JSON): {\"properties\": {\"result\": {\"title\": \"Result\", \"type\": \"number\"}}, \"required\": [\"result\"], \"title\": \"divisionOutput\", \"type\": \"object\"}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompt-Builder mit sauberer Trennung von Tool-Agent- und No-Tool-Modus\n",
    "\n",
    "### Funktion\n",
    "- Erzeugt einen Chat-Prompt, der zwei Betriebsarten explizit trennt:\n",
    "  - **Tool-Agent-Modus** (JSON-only): Modell soll Tool-Pläne oder Final/Error als *genau ein JSON-Objekt* ausgeben.\n",
    "  - **No-Tool-Modus** (Final-only): Modell darf keine Tools nutzen und kein JSON ausgeben; es soll das finale Ergebnis\n",
    "    ausschließlich aus der Tool-Historie ableiten.\n",
    "- Aggregiert Systemprompt, optionalen Dialogverlauf, optionale Tool-Historie und die aktuelle Nutzerfrage in eine\n",
    "  konsistente Nachrichtenliste.\n",
    "\n",
    "### Inputs\n",
    "- Pflichtparameter:\n",
    "  - `user_prompt: str` – aktuelle Nutzerfrage bzw. aktueller Ausdruck.\n",
    "- Optionale Parameter:\n",
    "  - `system_prompt: Optional[str]` – Agentenregeln/Systeminstruktionen (insb. für Tool-Agent-Modus).\n",
    "  - `history: Optional[List[Tuple[str, str]]]` – Dialogverlauf als (User, Assistant)-Paare.\n",
    "  - `tool_history: Optional[List[Dict[str, Any]]]` – strukturierte Historie früherer Tool-Aufrufe inkl. `replace_target`.\n",
    "  - `allow_tools: bool` – Schalter zwischen Tool-Agent-Modus (`True`) und No-Tool-Modus (`False`).\n",
    "\n",
    "### Modus-Semantik\n",
    "- `allow_tools=True`:\n",
    "  - Prompt lässt Tool-Agentenregeln gelten (sofern `system_prompt` gesetzt).\n",
    "  - Erwartete Ausgabe: strikt maschinenlesbar (JSON-only), geeignet für Plan-Parsing.\n",
    "- `allow_tools=False`:\n",
    "  - Fügt einen zusätzlichen System-Override ein, der:\n",
    "    - Tool-Nutzung verbietet,\n",
    "    - JSON-Ausgabe verbietet,\n",
    "    - und eine Minimal-Ausgabe erzwingt (entweder reine Zahl oder exakt `invalid_expression`).\n",
    "  - Modell soll das Ergebnis deterministisch aus `tool_history` ableiten, indem es einen Eintrag sucht,\n",
    "    dessen `replace_target` exakt dem aktuellen Ausdruck entspricht.\n",
    "\n",
    "### Nachrichtenstruktur\n",
    "- System:\n",
    "  - optional `system_prompt`.\n",
    "  - optional No-Tool-Override (bei `allow_tools=False`).\n",
    "- Verlauf:\n",
    "  - frühere User/Assistant-Nachrichten in Reihenfolge.\n",
    "- System:\n",
    "  - optional Tool-Historie, serialisiert als JSON.\n",
    "- User:\n",
    "  - aktuelle Nutzerfrage als letzte Nachricht.\n",
    "\n",
    "### Outputs\n",
    "- Rückgabewert:\n",
    "  - `str` – gerenderter Chat-Prompt via Chat-Template (`add_generation_prompt=True`), bereit für die direkte LLM-Übergabe.\n",
    "\n",
    "### Typische Verwendung\n",
    "- Agentische Pipelines mit **zweiphasigem** Ablauf:\n",
    "  - Phase 1: planen/ausführen mit Tools (JSON-only).\n",
    "  - Phase 2: finale Ausgabe ohne Tools (aus Tool-Historie).\n",
    "- Prompt-Debugging und kontrollierte Evaluationsläufe mit klaren Output-Verträgen."
   ],
   "id": "a32e5e7b01738984"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:41:04.683429Z",
     "start_time": "2026-01-31T09:41:04.676777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import json\n",
    "\n",
    "def build_chat_prompt_with_rag_and_tools(\n",
    "    system_prompt: Optional[str],\n",
    "    user_prompt: str,\n",
    "    history: Optional[List[Tuple[str, str]]] = None,\n",
    "    tool_history: Optional[List[Dict[str, Any]]] = None,\n",
    "    allow_tools: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Prompt-Builder, der Tool-Agent-Modus (JSON-only) und No-Tool-Modus (Final-Antwort aus Tool-History) sauber trennt.\n",
    "\n",
    "    WICHTIG:\n",
    "    - allow_tools=True  => Tool-Agent-Modus: Ausgabe MUSS exakt ein JSON-Objekt sein (Toolcall ODER Final/Error).\n",
    "    - allow_tools=False => No-Tool-Modus: Keine Tools, kein JSON-Zwang; das Modell soll das finale Resultat aus\n",
    "                          Tool-History liefern (z.B. \"38.0\") oder \"invalid_expression\".\n",
    "    \"\"\"\n",
    "    messages: List[Dict[str, str]] = []\n",
    "\n",
    "    # 1) System-Prompt (Tool-Agent Regeln, wenn vorhanden)\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    # 1b) Override für Phase 2: NO-TOOL / FINAL-AUSGABE\n",
    "    if not allow_tools:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"WICHTIG: In dieser Runde darfst du KEINE Tools aufrufen.\\n\"\n",
    "                    \"Du darfst KEIN JSON ausgeben.\\n\"\n",
    "                    \"Nutze ausschließlich die Informationen aus der Tool-Historie.\\n\\n\"\n",
    "                    \"AUFGABE:\\n\"\n",
    "                    \"- Bestimme das finale Ergebnis, indem du in der Tool-Historie nach einem Eintrag suchst, \"\n",
    "                    \"dessen replace_target exakt dem aktuellen User-Ausdruck entspricht.\\n\"\n",
    "                    \"- Falls vorhanden, gib NUR die Zahl (als Fließtext, ohne zusätzliche Wörter) aus, z.B.:\\n\"\n",
    "                    \"  38.0\\n\"\n",
    "                    \"- Falls nicht vorhanden, gib exakt aus:\\n\"\n",
    "                    \"  invalid_expression\\n\\n\"\n",
    "                    \"REGELN:\\n\"\n",
    "                    \"- Keine Erklärungen.\\n\"\n",
    "                    \"- Keine Zwischenschritte.\\n\"\n",
    "                    \"- Keine zusätzlichen Tokens außer der Ergebniszahl oder 'invalid_expression'.\"\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # 2) Verlauf (User/Assistant)\n",
    "    if history:\n",
    "        for user_msg, assistant_msg in history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "\n",
    "    # 3) Tool-History (falls vorhanden)\n",
    "    if tool_history:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Tool-Historie:\\n\" + json.dumps(tool_history, ensure_ascii=False),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # 4) aktuelle User-Nachricht\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    return prompt"
   ],
   "id": "2290241569758cff",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Teil 4: Voll agentisches MCP – der Mathematik-Assistent denkt und handelt selbst\n",
    "\n",
    "In Teil 4 machen wir aus dem „Half-Agent“ aus Teil 3 jetzt einen echten, voll agentischen Workflow:\n",
    "- Das LLM plant eine Tool-Kette (inkl. Parallelität).\n",
    "- Es referenziert frühere Tool-Outputs über $ref.\n",
    "- Ein Steuerloop (agentic_run_to_final_answer) führt Schritt für Schritt alle geplanten Tools aus, aktualisiert die Historie und lässt das Modell bei Bedarf re-planen, bis eine finale Antwort entsteht.\n",
    "\n",
    "Konzeptionell lehnen wir uns an Tool-/Plan-basierte Agenten wie ReAct und ähnliche Frameworks an, die iterativ zwischen „Denken“, „Handeln (Tool)“ und „Beobachten“ wechseln."
   ],
   "id": "dc72d1db0b9bdd68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## System-Prompt-Erzeugung für einen deterministischen Mathe-Tool-Agenten\n",
    "\n",
    "### Funktion\n",
    "- Erzeugt einen umfassenden System-Prompt, der das Verhalten eines strikt tool-basierten Mathe-Agenten festlegt.\n",
    "- Integriert einen Tool-Katalog in den Prompt, sodass das Modell die verfügbaren Tools und deren Signaturen kennt.\n",
    "- Zielt auf maximale Deterministik und Nachvollziehbarkeit durch harte Regeln und ein festes JSON-Outputformat.\n",
    "\n",
    "### Inputs\n",
    "- Pflichtparameter:\n",
    "  - `tools: List[Any]` – Liste verfügbarer Tools, die im Prompt als Tool-Katalog formatiert werden.\n",
    "\n",
    "### Prompt-Inhalt\n",
    "- Definiert eine strikte Rolle:\n",
    "  - Der Agent darf **niemals selbst rechnen**.\n",
    "  - Jede Operation muss über ein Tool erfolgen.\n",
    "- Legt die erlaubten Operationen und Einschränkungen fest (z. B. keine Klammern, keine Potenzen).\n",
    "- Erzwingt Operatorpräzedenz (*/ vor +-; jeweils links-nach-rechts).\n",
    "- Definiert Referenzregeln (`$ref`) für die Verkettung von Tool-Ergebnissen.\n",
    "- Beschreibt eine Zustands-Transformation über `replace_target` anhand der Tool-Historie (Caching/Ersetzung statt Neubewertung).\n",
    "- Legt die Strategie zur Auswahl der nächsten Operation fest (erste offene Operation gemäß Präzedenz, exaktes Teilstückformat).\n",
    "- Erzwingt ein einziges JSON-Objekt als Ausgabe ohne Zusatztext/Markdown.\n",
    "- Enthält spezifizierte Fehlerausgaben für ungültige Ausdrücke und Division durch Null.\n",
    "\n",
    "### Outputs\n",
    "- Rückgabewert:\n",
    "  - `str` – finaler System-Prompt als String, inklusive eingebettetem Tool-Katalog.\n",
    "\n",
    "### Typische Verwendung\n",
    "- Planner-LLM in agentischen Pipelines, die mathematische Ausdrücke zuverlässig in Tool-Pläne übersetzen sollen.\n",
    "- Evaluation deterministischer Tool-Agenten mit reproduzierbaren Regeln und maschinenlesbarem Output.\n",
    "- Absicherung gegen Halluzinationen durch harte Verbote (kein Rechnen, kein Freitext, nur JSON)."
   ],
   "id": "8d5a012a8c13b03d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:41:09.577070Z",
     "start_time": "2026-01-31T09:41:09.571318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any, List\n",
    "\n",
    "def build_agent_system_prompt(tools: List[Any]) -> str:\n",
    "    tool_catalog = format_tools_for_prompt(tools)\n",
    "\n",
    "    return f\"\"\"\n",
    "Du bist ein deterministischer MATHE-Tool-Agent.\n",
    "\n",
    "Deine Aufgabe ist es, arithmetische Ausdrücke korrekt zu verarbeiten.\n",
    "Du darfst NIEMALS selbst rechnen.\n",
    "Jede mathematische Operation MUSS über ein Tool erfolgen.\n",
    "\n",
    "====================================================================\n",
    "ERLAUBTE OPERATIONEN\n",
    "====================================================================\n",
    "\n",
    "- Addition (+)\n",
    "- Subtraktion (-)\n",
    "- Multiplikation (*)\n",
    "- Division (/)\n",
    "\n",
    "Nicht erlaubt:\n",
    "- Klammern\n",
    "- negative Zahlen\n",
    "- Potenzen\n",
    "- Funktionen\n",
    "- algebraische Umformungen\n",
    "\n",
    "====================================================================\n",
    "OPERATORPRÄZEDENZ\n",
    "====================================================================\n",
    "\n",
    "1. Multiplikation und Division (links → rechts)\n",
    "2. Addition und Subtraktion (links → rechts)\n",
    "\n",
    "Diese Reihenfolge darf niemals verletzt werden.\n",
    "\n",
    "====================================================================\n",
    "GRUNDREGEL\n",
    "====================================================================\n",
    "\n",
    "Ein Ergebnis existiert für dich nur, wenn es aus einem Tool stammt.\n",
    "Alles andere gilt als unbekannt.\n",
    "\n",
    "====================================================================\n",
    "$ref-REGELN\n",
    "====================================================================\n",
    "\n",
    "#<<Bitte ergänzen>>\n",
    "\n",
    "====================================================================\n",
    "ZUSTANDSTRANSFORMATION\n",
    "====================================================================\n",
    "\n",
    "Vor jeder Planung:\n",
    "\n",
    "- Ersetze im aktuellen Ausdruck alle bekannten replace_target-Substrings\n",
    "  durch die exakten Tool-Resultate aus der Tool-Historie.\n",
    "- Verwende exakt die String-Repräsentation der Tool-Ergebnisse.\n",
    "- Rekonstruiere oder errate niemals Werte.\n",
    "\n",
    "Wenn der Ausdruck danach exakt eine einzelne Zahl ist:\n",
    "→ gehe sofort in den FINAL-MODUS.\n",
    "\n",
    "====================================================================\n",
    "AUSWAHL DER NÄCHSTEN OPERATION\n",
    "====================================================================\n",
    "\n",
    "- Wähle immer die erste noch offene Operation gemäß Präzedenz.\n",
    "- Das Teilstück muss exakt die Form haben:\n",
    "  <number> <operator> <number>\n",
    "\n",
    "====================================================================\n",
    "TOOL-PLANUNG\n",
    "====================================================================\n",
    "\n",
    "#<<Bitte ergänzen>>\n",
    "\n",
    "====================================================================\n",
    "CACHE\n",
    "====================================================================\n",
    "\n",
    "#<<Bitte ergänzen>>\n",
    "\n",
    "====================================================================\n",
    "OUTPUTFORMAT\n",
    "====================================================================\n",
    "\n",
    "Du gibst IMMER genau EIN JSON-Objekt aus.\n",
    "Kein zusätzlicher Text. Kein Markdown.\n",
    "\n",
    "Format:\n",
    "\n",
    "{{\n",
    "  \"steps\": [\n",
    "    {{\n",
    "      \"description\": \"<kurze Beschreibung ohne Ergebnis>\",\n",
    "      \"tools\": [\n",
    "        {{\n",
    "          \"id\": \"<eindeutige-id>\",\n",
    "          \"name\": \"<tool-name>\",\n",
    "          \"arguments\": {{\n",
    "            \"a\": <number oder $ref>,\n",
    "            \"b\": <number oder $ref>\n",
    "          }},\n",
    "          \"replace_target\": \"<exakter substring '<a> <op> <b>'>\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "  ],\n",
    "  \"final\": null | \"<zahl als String>\" | {{\"$ref\":\"<tool-id>[.<pfad>]\"}} | \"<error_code>\"\n",
    "}}\n",
    "\n",
    "====================================================================\n",
    "FEHLER\n",
    "====================================================================\n",
    "\n",
    "Ungültiger Ausdruck:\n",
    "{{\n",
    "  \"steps\": [],\n",
    "  \"final\": \"invalid_expression\"\n",
    "}}\n",
    "\n",
    "Division durch 0 (literal erkennbar):\n",
    "{{\n",
    "  \"steps\": [],\n",
    "  \"final\": \"division_by_zero\"\n",
    "}}\n",
    "\n",
    "TOOLS:\n",
    "{tool_catalog}\n",
    "\n",
    "\"\"\".strip()"
   ],
   "id": "e7c7b5e6d309f248",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Agent-System-Prompt bauen\n",
    "\n",
    "### Funktion\n",
    "- Ruft build_agent_system_prompt(tool_names_with_meta) auf, um den finalen Prompt-String zu erzeugen.\n",
    "\n",
    "### Input\n",
    "- tool_names_with_meta – Toolmetadaten, wie zuvor per fetch_tools_with_metadata() geladen.\n",
    "\n",
    "### Output\n",
    "- agent_system_prompt: str – wird später in allen agentischen Calls verwendet.\n",
    "\n",
    "### Konsole\n",
    "- Ausgabe von agent_system_prompt"
   ],
   "id": "2ea4f8fb32946494"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:41:16.424004Z",
     "start_time": "2026-01-31T09:41:16.419493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent_system_prompt = build_agent_system_prompt(tool_names_with_meta)\n",
    "print(agent_system_prompt)"
   ],
   "id": "61dcb3a177482e7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Du bist ein deterministischer MATHE-Tool-Agent.\n",
      "\n",
      "Deine Aufgabe ist es, arithmetische Ausdrücke korrekt zu verarbeiten.\n",
      "Du darfst NIEMALS selbst rechnen.\n",
      "Jede mathematische Operation MUSS über ein Tool erfolgen.\n",
      "\n",
      "====================================================================\n",
      "ERLAUBTE OPERATIONEN\n",
      "====================================================================\n",
      "\n",
      "- Addition (+)\n",
      "- Subtraktion (-)\n",
      "- Multiplikation (*)\n",
      "- Division (/)\n",
      "\n",
      "Nicht erlaubt:\n",
      "- Klammern\n",
      "- negative Zahlen\n",
      "- Potenzen\n",
      "- Funktionen\n",
      "- algebraische Umformungen\n",
      "\n",
      "====================================================================\n",
      "OPERATORPRÄZEDENZ\n",
      "====================================================================\n",
      "\n",
      "1. Multiplikation und Division (links → rechts)\n",
      "2. Addition und Subtraktion (links → rechts)\n",
      "\n",
      "Diese Reihenfolge darf niemals verletzt werden.\n",
      "\n",
      "====================================================================\n",
      "GRUNDREGEL\n",
      "====================================================================\n",
      "\n",
      "Ein Ergebnis existiert für dich nur, wenn es aus einem Tool stammt.\n",
      "Alles andere gilt als unbekannt.\n",
      "\n",
      "====================================================================\n",
      "$ref-REGELN\n",
      "====================================================================\n",
      "\n",
      "- Toolargumente dürfen entweder:\n",
      "  - literale Zahlen\n",
      "  - oder ein $ref-Objekt sein: {\"$ref\":\"<tool-id>[.<pfad>]\"}\n",
      "\n",
      "- Erlaubter Pfad für Mathe-Tools:\n",
      "  - \".result\"\n",
      "\n",
      "- {\"$ref\":\"<tool-id>\"} ist ein Shortcut für {\"$ref\":\"<tool-id>.result\"}.\n",
      "- Beide Varianten sind erlaubt und gleichwertig.\n",
      "\n",
      "- $ref muss immer als Objekt angegeben werden.\n",
      "  String-Formen wie \"$ref:mul_1.result\" sind NICHT erlaubt.\n",
      "\n",
      "====================================================================\n",
      "ZUSTANDSTRANSFORMATION\n",
      "====================================================================\n",
      "\n",
      "Vor jeder Planung:\n",
      "\n",
      "- Ersetze im aktuellen Ausdruck alle bekannten replace_target-Substrings\n",
      "  durch die exakten Tool-Resultate aus der Tool-Historie.\n",
      "- Verwende exakt die String-Repräsentation der Tool-Ergebnisse.\n",
      "- Rekonstruiere oder errate niemals Werte.\n",
      "\n",
      "Wenn der Ausdruck danach exakt eine einzelne Zahl ist:\n",
      "→ gehe sofort in den FINAL-MODUS.\n",
      "\n",
      "====================================================================\n",
      "AUSWAHL DER NÄCHSTEN OPERATION\n",
      "====================================================================\n",
      "\n",
      "- Wähle immer die erste noch offene Operation gemäß Präzedenz.\n",
      "- Das Teilstück muss exakt die Form haben:\n",
      "  <number> <operator> <number>\n",
      "\n",
      "====================================================================\n",
      "TOOL-PLANUNG\n",
      "====================================================================\n",
      "\n",
      "- Eine Tool-Operation entspricht genau einer Grundoperation.\n",
      "- Mehrere Schritte sind erlaubt.\n",
      "- Abhängige Operationen müssen in späteren Schritten erfolgen.\n",
      "- Plane nichts spekulativ.\n",
      "\n",
      "====================================================================\n",
      "CACHE\n",
      "====================================================================\n",
      "\n",
      "- Wenn ein identischer Toolcall (gleicher Name, gleiche aufgelöste Argumente)\n",
      "  bereits in der Tool-Historie existiert:\n",
      "  → erzeuge keinen neuen Call\n",
      "  → nutze das vorhandene Ergebnis per Ersetzung\n",
      "\n",
      "====================================================================\n",
      "OUTPUTFORMAT\n",
      "====================================================================\n",
      "\n",
      "Du gibst IMMER genau EIN JSON-Objekt aus.\n",
      "Kein zusätzlicher Text. Kein Markdown.\n",
      "\n",
      "Format:\n",
      "\n",
      "{\n",
      "  \"steps\": [\n",
      "    {\n",
      "      \"description\": \"<kurze Beschreibung ohne Ergebnis>\",\n",
      "      \"tools\": [\n",
      "        {\n",
      "          \"id\": \"<eindeutige-id>\",\n",
      "          \"name\": \"<tool-name>\",\n",
      "          \"arguments\": {\n",
      "            \"a\": <number oder $ref>,\n",
      "            \"b\": <number oder $ref>\n",
      "          },\n",
      "          \"replace_target\": \"<exakter substring '<a> <op> <b>'>\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"final\": null | \"<zahl als String>\" | {\"$ref\":\"<tool-id>[.<pfad>]\"} | \"<error_code>\"\n",
      "}\n",
      "\n",
      "====================================================================\n",
      "FEHLER\n",
      "====================================================================\n",
      "\n",
      "Ungültiger Ausdruck:\n",
      "{\n",
      "  \"steps\": [],\n",
      "  \"final\": \"invalid_expression\"\n",
      "}\n",
      "\n",
      "Division durch 0 (literal erkennbar):\n",
      "{\n",
      "  \"steps\": [],\n",
      "  \"final\": \"division_by_zero\"\n",
      "}\n",
      "\n",
      "TOOLS:\n",
      "- Name: addition\n",
      "  Beschreibung: Addiert a + b\n",
      "  Eingabe-Schema (JSON): {\"properties\": {\"a\": {\"title\": \"A\", \"type\": \"number\"}, \"b\": {\"title\": \"B\", \"type\": \"number\"}}, \"required\": [\"a\", \"b\"], \"title\": \"additionArguments\", \"type\": \"object\"}\n",
      "  Ausgabe-Schema (JSON): {\"properties\": {\"result\": {\"title\": \"Result\", \"type\": \"number\"}}, \"required\": [\"result\"], \"title\": \"additionOutput\", \"type\": \"object\"}\n",
      "\n",
      "- Name: subtraktion\n",
      "  Beschreibung: Subtrahiert a - b\n",
      "  Eingabe-Schema (JSON): {\"properties\": {\"a\": {\"title\": \"A\", \"type\": \"number\"}, \"b\": {\"title\": \"B\", \"type\": \"number\"}}, \"required\": [\"a\", \"b\"], \"title\": \"subtraktionArguments\", \"type\": \"object\"}\n",
      "  Ausgabe-Schema (JSON): {\"properties\": {\"result\": {\"title\": \"Result\", \"type\": \"number\"}}, \"required\": [\"result\"], \"title\": \"subtraktionOutput\", \"type\": \"object\"}\n",
      "\n",
      "- Name: multiplikation\n",
      "  Beschreibung: Multipliziert a * b\n",
      "  Eingabe-Schema (JSON): {\"properties\": {\"a\": {\"title\": \"A\", \"type\": \"number\"}, \"b\": {\"title\": \"B\", \"type\": \"number\"}}, \"required\": [\"a\", \"b\"], \"title\": \"multiplikationArguments\", \"type\": \"object\"}\n",
      "  Ausgabe-Schema (JSON): {\"properties\": {\"result\": {\"title\": \"Result\", \"type\": \"number\"}}, \"required\": [\"result\"], \"title\": \"multiplikationOutput\", \"type\": \"object\"}\n",
      "\n",
      "- Name: division\n",
      "  Beschreibung: Dividiert a / b\n",
      "  Eingabe-Schema (JSON): {\"properties\": {\"a\": {\"title\": \"A\", \"type\": \"number\"}, \"b\": {\"title\": \"B\", \"type\": \"number\"}}, \"required\": [\"a\", \"b\"], \"title\": \"divisionArguments\", \"type\": \"object\"}\n",
      "  Ausgabe-Schema (JSON): {\"properties\": {\"result\": {\"title\": \"Result\", \"type\": \"number\"}}, \"required\": [\"result\"], \"title\": \"divisionOutput\", \"type\": \"object\"}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Erster agentischer Call: Was ist 6.0 + 4.0 * 8.0?\n",
    "\n",
    "### Kontext\n",
    "- Demonstriert den Aufbau eines vollständigen Chat-Prompts inklusive Systemkontext und Nutzerfrage.\n",
    "- Zeigt, ob die generierung des Plans funktioniert.\n",
    "\n",
    "### Ablauf\n",
    "- Definiert eine einfache arithmetische Nutzerfrage als String.\n",
    "- Initialisiert leere Strukturen für Gesprächs- und Tool-Historie.\n",
    "- Erzeugt einen Chat-Prompt aus:\n",
    "  - System-Prompt (Agentenregeln),\n",
    "  - aktueller Nutzerfrage,\n",
    "  - optionalem Dialogverlauf,\n",
    "  - optionaler Tool-Historie.\n",
    "- Gibt den generierten Prompt explizit aus, um dessen Struktur nachvollziehen zu können.\n",
    "- Übergibt den Prompt direkt an das LLM und gibt die Rohantwort aus.\n",
    "\n",
    "### Inputs\n",
    "- `system_prompt` – globaler Systemkontext für das Modell.\n",
    "- `user_prompt` – aktuelle Nutzerfrage.\n",
    "- `history` – bisheriger Dialogverlauf (hier leer).\n",
    "- `tool_history` – Historie früherer Tool-Aufrufe (hier nicht gesetzt).\n",
    "\n",
    "### Outputs\n",
    "- Konsolenausgabe des vollständigen Prompts.\n",
    "- Konsolenausgabe des Plans."
   ],
   "id": "91b091f91cd3e6b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:41:29.786480Z",
     "start_time": "2026-01-31T09:41:18.840816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_input = f\"Was ist 6.0 + 4.0 * 8.0?\"\n",
    "tool_history = None\n",
    "history = []\n",
    "\n",
    "prompt = build_chat_prompt_with_rag_and_tools(\n",
    "    system_prompt=agent_system_prompt,\n",
    "    user_prompt=user_input,\n",
    "    history=history,\n",
    "    tool_history=tool_history,\n",
    ")\n",
    "\n",
    "print(f\"Input: {prompt}\\n\\n\")\n",
    "assistant_text = llama_chat(prompt, max_new_tokens=512)\n",
    "print(f\"Output: {assistant_text}\\n\\n\")"
   ],
   "id": "5a6f006cfd7c176",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Du bist ein deterministischer MATHE-Tool-Agent.\n",
      "\n",
      "Deine Aufgabe ist es, arithmetische Ausdrücke korrekt zu verarbeiten.\n",
      "Du darfst NIEMALS selbst rechnen.\n",
      "Jede mathematische Operation MUSS über ein Tool erfolgen.\n",
      "\n",
      "====================================================================\n",
      "ERLAUBTE OPERATIONEN\n",
      "====================================================================\n",
      "\n",
      "- Addition (+)\n",
      "- Subtraktion (-)\n",
      "- Multiplikation (*)\n",
      "- Division (/)\n",
      "\n",
      "Nicht erlaubt:\n",
      "- Klammern\n",
      "- negative Zahlen\n",
      "- Potenzen\n",
      "- Funktionen\n",
      "- algebraische Umformungen\n",
      "\n",
      "====================================================================\n",
      "OPERATORPRÄZEDENZ\n",
      "====================================================================\n",
      "\n",
      "1. Multiplikation und Division (links → rechts)\n",
      "2. Addition und Subtraktion (links → rechts)\n",
      "\n",
      "Diese Reihenfolge darf niemals verletzt werden.\n",
      "\n",
      "====================================================================\n",
      "GRUNDREGEL\n",
      "====================================================================\n",
      "\n",
      "Ein Ergebnis existiert für dich nur, wenn es aus einem Tool stammt.\n",
      "Alles andere gilt als unbekannt.\n",
      "\n",
      "====================================================================\n",
      "$ref-REGELN\n",
      "====================================================================\n",
      "\n",
      "- Toolargumente dürfen entweder:\n",
      "  - literale Zahlen\n",
      "  - oder ein $ref-Objekt sein: {\"$ref\":\"<tool-id>[.<pfad>]\"}\n",
      "\n",
      "- Erlaubter Pfad für Mathe-Tools:\n",
      "  - \".result\"\n",
      "\n",
      "- {\"$ref\":\"<tool-id>\"} ist ein Shortcut für {\"$ref\":\"<tool-id>.result\"}.\n",
      "- Beide Varianten sind erlaubt und gleichwertig.\n",
      "\n",
      "- $ref muss immer als Objekt angegeben werden.\n",
      "  String-Formen wie \"$ref:mul_1.result\" sind NICHT erlaubt.\n",
      "\n",
      "====================================================================\n",
      "ZUSTANDSTRANSFORMATION\n",
      "====================================================================\n",
      "\n",
      "Vor jeder Planung:\n",
      "\n",
      "- Ersetze im aktuellen Ausdruck alle bekannten replace_target-Substrings\n",
      "  durch die exakten Tool-Resultate aus der Tool-Historie.\n",
      "- Verwende exakt die String-Repräsentation der Tool-Ergebnisse.\n",
      "- Rekonstruiere oder errate niemals Werte.\n",
      "\n",
      "Wenn der Ausdruck danach exakt eine einzelne Zahl ist:\n",
      "→ gehe sofort in den FINAL-MODUS.\n",
      "\n",
      "====================================================================\n",
      "AUSWAHL DER NÄCHSTEN OPERATION\n",
      "====================================================================\n",
      "\n",
      "- Wähle immer die erste noch offene Operation gemäß Präzedenz.\n",
      "- Das Teilstück muss exakt die Form haben:\n",
      "  <number> <operator> <number>\n",
      "\n",
      "====================================================================\n",
      "TOOL-PLANUNG\n",
      "====================================================================\n",
      "\n",
      "- Eine Tool-Operation entspricht genau einer Grundoperation.\n",
      "- Mehrere Schritte sind erlaubt.\n",
      "- Abhängige Operationen müssen in späteren Schritten erfolgen.\n",
      "- Plane nichts spekulativ.\n",
      "\n",
      "====================================================================\n",
      "CACHE\n",
      "====================================================================\n",
      "\n",
      "- Wenn ein identischer Toolcall (gleicher Name, gleiche aufgelöste Argumente)\n",
      "  bereits in der Tool-Historie existiert:\n",
      "  → erzeuge keinen neuen Call\n",
      "  → nutze das vorhandene Ergebnis per Ersetzung\n",
      "\n",
      "====================================================================\n",
      "OUTPUTFORMAT\n",
      "====================================================================\n",
      "\n",
      "Du gibst IMMER genau EIN JSON-Objekt aus.\n",
      "Kein zusätzlicher Text. Kein Markdown.\n",
      "\n",
      "Format:\n",
      "\n",
      "{\n",
      "  \"steps\": [\n",
      "    {\n",
      "      \"description\": \"<kurze Beschreibung ohne Ergebnis>\",\n",
      "      \"tools\": [\n",
      "        {\n",
      "          \"id\": \"<eindeutige-id>\",\n",
      "          \"name\": \"<tool-name>\",\n",
      "          \"arguments\": {\n",
      "            \"a\": <number oder $ref>,\n",
      "            \"b\": <number oder $ref>\n",
      "          },\n",
      "          \"replace_target\": \"<exakter substring '<a> <op> <b>'>\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"final\": null | \"<zahl als String>\" | {\"$ref\":\"<tool-id>[.<pfad>]\"} | \"<error_code>\"\n",
      "}\n",
      "\n",
      "====================================================================\n",
      "FEHLER\n",
      "====================================================================\n",
      "\n",
      "Ungültiger Ausdruck:\n",
      "{\n",
      "  \"steps\": [],\n",
      "  \"final\": \"invalid_expression\"\n",
      "}\n",
      "\n",
      "Division durch 0 (literal erkennbar):\n",
      "{\n",
      "  \"steps\": [],\n",
      "  \"final\": \"division_by_zero\"\n",
      "}\n",
      "\n",
      "TOOLS:\n",
      "- Name: addition\n",
      "  Beschreibung: Addiert a + b\n",
      "  Eingabe-Schema (JSON): {\"properties\": {\"a\": {\"title\": \"A\", \"type\": \"number\"}, \"b\": {\"title\": \"B\", \"type\": \"number\"}}, \"required\": [\"a\", \"b\"], \"title\": \"additionArguments\", \"type\": \"object\"}\n",
      "  Ausgabe-Schema (JSON): {\"properties\": {\"result\": {\"title\": \"Result\", \"type\": \"number\"}}, \"required\": [\"result\"], \"title\": \"additionOutput\", \"type\": \"object\"}\n",
      "\n",
      "- Name: subtraktion\n",
      "  Beschreibung: Subtrahiert a - b\n",
      "  Eingabe-Schema (JSON): {\"properties\": {\"a\": {\"title\": \"A\", \"type\": \"number\"}, \"b\": {\"title\": \"B\", \"type\": \"number\"}}, \"required\": [\"a\", \"b\"], \"title\": \"subtraktionArguments\", \"type\": \"object\"}\n",
      "  Ausgabe-Schema (JSON): {\"properties\": {\"result\": {\"title\": \"Result\", \"type\": \"number\"}}, \"required\": [\"result\"], \"title\": \"subtraktionOutput\", \"type\": \"object\"}\n",
      "\n",
      "- Name: multiplikation\n",
      "  Beschreibung: Multipliziert a * b\n",
      "  Eingabe-Schema (JSON): {\"properties\": {\"a\": {\"title\": \"A\", \"type\": \"number\"}, \"b\": {\"title\": \"B\", \"type\": \"number\"}}, \"required\": [\"a\", \"b\"], \"title\": \"multiplikationArguments\", \"type\": \"object\"}\n",
      "  Ausgabe-Schema (JSON): {\"properties\": {\"result\": {\"title\": \"Result\", \"type\": \"number\"}}, \"required\": [\"result\"], \"title\": \"multiplikationOutput\", \"type\": \"object\"}\n",
      "\n",
      "- Name: division\n",
      "  Beschreibung: Dividiert a / b\n",
      "  Eingabe-Schema (JSON): {\"properties\": {\"a\": {\"title\": \"A\", \"type\": \"number\"}, \"b\": {\"title\": \"B\", \"type\": \"number\"}}, \"required\": [\"a\", \"b\"], \"title\": \"divisionArguments\", \"type\": \"object\"}\n",
      "  Ausgabe-Schema (JSON): {\"properties\": {\"result\": {\"title\": \"Result\", \"type\": \"number\"}}, \"required\": [\"result\"], \"title\": \"divisionOutput\", \"type\": \"object\"}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Was ist 6.0 + 4.0 * 8.0?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Output: {\n",
      "  \"steps\": [\n",
      "    {\n",
      "      \"description\": \"Multiplikation von 4.0 und 8.0\",\n",
      "      \"tools\": [\n",
      "        {\n",
      "          \"id\": \"mul_1\",\n",
      "          \"name\": \"multiplikation\",\n",
      "          \"arguments\": {\n",
      "            \"a\": 4.0,\n",
      "            \"b\": 8.0\n",
      "          },\n",
      "          \"replace_target\": \"4.0 * 8.0\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Addition von 6.0 und 32.0\",\n",
      "      \"tools\": [\n",
      "        {\n",
      "          \"id\": \"add_1\",\n",
      "          \"name\": \"addition\",\n",
      "          \"arguments\": {\n",
      "            \"a\": 6.0,\n",
      "            \"b\": {\"$ref\": \"mul_1.result\"}\n",
      "          },\n",
      "          \"replace_target\": \"6.0 + 4.0 * 8.0\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"final\": {\"$ref\": \"add_1.result\"}\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Im Anschluss werden wir die bearbeitung dieses Plans automatisieren",
   "id": "4ead278edc67a7c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Parsing eines JSON-Plans aus LLM-Output\n",
    "\n",
    "### Funktion\n",
    "- Extrahiert und parst einen strukturierten Ausführungsplan (JSON) aus dem Rohtext eines Modells.\n",
    "- Ist robust gegen „Text“ um das JSON herum, indem gezielt das erste vollständige JSON-Objekt herausgeschnitten wird.\n",
    "- Validiert minimale Schemaanforderungen, damit nachgelagerte Komponenten zuverlässig arbeiten können.\n",
    "\n",
    "### Inputs\n",
    "- Pflichtparameter:\n",
    "  - `output_str: str` – Rohausgabe des Planungs-LLMs (kann Text + JSON enthalten).\n",
    "\n",
    "### Extraktionslogik\n",
    "- Fängt leere oder triviale Ausgaben früh ab.\n",
    "- Sucht die erste öffnende geschweifte Klammer `{` als Startpunkt des JSON-Kandidaten.\n",
    "- Bestimmt das Ende des JSON-Objekts durch kontrolliertes Klammer-Counting:\n",
    "  - berücksichtigt dabei String-Literale (`\"...\"`), damit Klammern innerhalb von Strings nicht fälschlich gezählt werden,\n",
    "  - berücksichtigt Escape-Sequenzen (z. B. `\\\"`), um String-Grenzen korrekt zu erkennen.\n",
    "- Schneidet den ermittelten Bereich als „trimmed JSON candidate“ aus und protokolliert ihn für Debugging.\n",
    "\n",
    "### Parsing & Validierung\n",
    "- Parst den ausgeschnittenen Kandidaten mittels `json.loads`.\n",
    "- Hebt Parsing-Fehler mit einer aussagekräftigen Fehlermeldung an, inklusive Kandidat und Original-Exception.\n",
    "- Prüft ein Minimal-Schema:\n",
    "  - Plan muss `steps` und `final` enthalten.\n",
    "  - `steps` muss eine Liste sein.\n",
    "\n",
    "### Outputs\n",
    "- Rückgabewert:\n",
    "  - `Dict[str, Any]` – geparster und validierter Plan als Dictionary."
   ],
   "id": "f9cbdfd536e9bc73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:41:40.456680Z",
     "start_time": "2026-01-31T09:41:40.450281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from typing import Any, Dict\n",
    "\n",
    "def agentic_parse_model_plan(output_str: str) -> Dict[str, Any]:\n",
    "    #print(\"[DEBUG] raw output:\", repr(output_str))\n",
    "\n",
    "    # Leer- oder Nonsens-Output früh abfangen\n",
    "    if not output_str or not output_str.strip():\n",
    "        raise ValueError(\"Model output is empty, cannot parse JSON plan.\")\n",
    "\n",
    "    # Die erste öffnende Klammer suchen\n",
    "    start = output_str.find(\"{\")\n",
    "    if start == -1:\n",
    "        raise ValueError(f\"No JSON object found in model output: {output_str}\")\n",
    "\n",
    "    s = output_str\n",
    "\n",
    "    # Jetzt von 'start' an die passende schließende Klammer suchen\n",
    "    depth = 0\n",
    "    in_string = False\n",
    "    escape = False\n",
    "    end = None  # <- no Optional annotation needed\n",
    "\n",
    "    for i, ch in enumerate(s[start:], start=start):\n",
    "        if in_string:\n",
    "            # Innerhalb eines Strings\n",
    "            if escape:\n",
    "                # nächstes Zeichen ist escaped, einfach überspringen\n",
    "                escape = False\n",
    "            elif ch == \"\\\\\":\n",
    "                escape = True\n",
    "            elif ch == '\"':\n",
    "                in_string = False\n",
    "        else:\n",
    "            # Außerhalb von Strings\n",
    "            if ch == '\"':\n",
    "                in_string = True\n",
    "            elif ch == \"{\":\n",
    "                depth += 1\n",
    "            elif ch == \"}\":\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    end = i\n",
    "                    break\n",
    "\n",
    "    trimmed = s[start:end + 1] if end is not None else s[start:]\n",
    "    print(\"[DEBUG] trimmed JSON candidate:\", trimmed)\n",
    "\n",
    "    try:\n",
    "        plan = json.loads(trimmed)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(\n",
    "            f\"Failed to parse JSON plan from model output. Error: {e}. \"\n",
    "            f\"Trimmed candidate: {trimmed}\"\n",
    "        ) from e\n",
    "\n",
    "    if \"steps\" not in plan or \"final\" not in plan:\n",
    "        raise ValueError(\"Model output must contain 'steps' and 'final'.\")\n",
    "\n",
    "    if not isinstance(plan[\"steps\"], list):\n",
    "        raise ValueError(\"'steps' must be a list.\")\n",
    "\n",
    "    return plan"
   ],
   "id": "23974c28c3883169",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Auflösung einzelner Referenz-Argumente (`$ref`)\n",
    "\n",
    "### Funktion\n",
    "- Löst einzelne Argumentwerte auf, die als Referenz auf Nutzerkontext oder Tool-Ergebnisse definiert sind.\n",
    "- Unterstützt mehrere syntaktische Varianten für Referenzen, um flexible Plan-Spezifikationen zu ermöglichen.\n",
    "- Gibt bei nicht-referenzierten Werten den Originalwert unverändert zurück.\n",
    "\n",
    "### Unterstützte Referenzformate\n",
    "- Objektform:\n",
    "  - `{\"$ref\": \"tool_id.key.subkey\"}`\n",
    "- Stringform:\n",
    "  - `\"$ref:tool_id.key.subkey\"`\n",
    "- Zugriff auf Nutzerkontext:\n",
    "  - `\"$ref:user.raw\"`\n",
    "\n",
    "### Auflösungslogik\n",
    "- Extrahiert zunächst den Referenzstring (`ref_str`), falls vorhanden.\n",
    "- Zerlegt die Referenz in einen Wurzelbezeichner (`root`) und einen Zugriffspfad.\n",
    "- Bestimmt die Datenquelle:\n",
    "  - `user` → Zugriff auf den Nutzerkontext.\n",
    "  - sonst → Zugriff auf gespeicherte Tool-Ergebnisse anhand der Tool-ID.\n",
    "- Traversiert den Zugriffspfad schrittweise:\n",
    "  - unterstützt Dictionary-Zugriffe über Schlüssel,\n",
    "  - unterstützt Listen-Zugriffe über numerische Indizes.\n",
    "- Bricht mit aussagekräftigen Fehlermeldungen ab, wenn ein Pfad ungültig ist.\n",
    "\n",
    "### Inputs\n",
    "- `value: Any` – zu prüfender und ggf. aufzulösender Wert.\n",
    "- `tool_results: Dict[str, Any]` – verfügbare Ergebnisse früherer Tool-Aufrufe.\n",
    "- `user_ctx: Dict[str, Any]` – Nutzerkontext (z. B. rohe Eingabe, Metadaten).\n",
    "\n",
    "### Outputs\n",
    "- Rückgabewert:\n",
    "  - `Any` – aufgelöster Wert oder der unveränderte Eingabewert, falls keine Referenz vorliegt."
   ],
   "id": "fc233cac98bffe37"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:41:44.183680Z",
     "start_time": "2026-01-31T09:41:44.177677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def agentic_resolve_argument(\n",
    "    value: Any,\n",
    "    tool_results: Dict[str, Any],\n",
    "    user_ctx: Dict[str, Any],\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Löst einzelne Argumentwerte auf. Unterstützt:\n",
    "    - {\"$ref\": \"geo_roma.lat\"}\n",
    "    - \"$ref:geo_roma.lat\"\n",
    "    - \"$ref:user.raw\"\n",
    "    \"\"\"\n",
    "    ref_str: Optional[str] = None\n",
    "\n",
    "    # Fall 1: Objekt mit \"$ref\"\n",
    "    if isinstance(value, dict) and \"$ref\" in value and isinstance(value[\"$ref\"], str):\n",
    "        ref_str = value[\"$ref\"]\n",
    "\n",
    "    # Fall 2: String mit Präfix \"$ref:\"\n",
    "    elif isinstance(value, str) and value.startswith(\"$ref:\"):\n",
    "        ref_str = value[len(\"$ref:\") :]\n",
    "\n",
    "    # Kein $ref → Wert unverändert zurückgeben\n",
    "    if ref_str is None:\n",
    "        return value\n",
    "\n",
    "    # Jetzt ref_str auswerten, z.B. \"geo_roma.lat\" oder \"user.raw\"\n",
    "    parts = ref_str.split(\".\")\n",
    "    root = parts[0]\n",
    "    path = parts[1:]\n",
    "\n",
    "    # Quelle: User-Kontext\n",
    "    if root == \"user\":\n",
    "        current: Any = user_ctx\n",
    "    else:\n",
    "        # Quelle: Tool-Resultate\n",
    "        if root not in tool_results:\n",
    "            raise KeyError(f\"Unknown tool id in $ref: {root}\")\n",
    "        current = tool_results[root]\n",
    "\n",
    "    # Debug: einmal zeigen, was wir da wirklich haben\n",
    "    print(f\"[DEBUG] Resolving $ref '{ref_str}': root='{root}', initial_type={type(current)}, initial_value={current}\")\n",
    "\n",
    "    for p in path:\n",
    "        # Numerische Indizes unterstützen (z.B. itineraries.0)\n",
    "        if isinstance(current, list) and p.isdigit():\n",
    "            current = current[int(p)]\n",
    "        elif isinstance(current, dict) and p in current:\n",
    "            current = current[p]\n",
    "        else:\n",
    "            # Noch mehr Debug, bevor wir crashen\n",
    "            raise KeyError(\n",
    "                f\"Invalid path component '{p}' in $ref '{ref_str}' \"\n",
    "                f\"(current_type={type(current)}, current_value={current})\"\n",
    "            )\n",
    "\n",
    "    return current"
   ],
   "id": "6b6ac564e265fc32",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Auflösung von Tool-Argumenten mit Referenzen\n",
    "\n",
    "### Funktion\n",
    "- Löst Tool-Argumente rekursiv auf, bevor sie an ein externes Tool übergeben werden.\n",
    "- Unterstützt Referenzen auf frühere Tool-Ergebnisse und den Nutzerkontext.\n",
    "- Vereinheitlicht die Argumentstruktur unabhängig von Verschachtelungstiefe.\n",
    "\n",
    "### Logik\n",
    "- Erkennt Dictionaries, Listen und skalare Werte und behandelt sie rekursiv.\n",
    "- Spezialfall:\n",
    "  - Besteht ein Dictionary ausschließlich aus einem `$ref`-Eintrag, wird das gesamte Objekt\n",
    "    durch das referenzierte Ergebnis ersetzt.\n",
    "- Für reguläre Dictionaries:\n",
    "  - Jedes Argument wird einzeln rekursiv aufgelöst.\n",
    "- Für Listen:\n",
    "  - Jedes Element wird rekursiv aufgelöst.\n",
    "- Skalare oder nicht strukturierte Werte werden direkt über die Referenzauflösung verarbeitet.\n",
    "\n",
    "### Inputs\n",
    "- `arguments` – rohe Tool-Argumente (beliebig verschachtelt).\n",
    "- `tool_results` – bisherige Tool-Ergebnisse, die als Referenzquelle dienen.\n",
    "- `user_ctx` – Nutzerkontext (z. B. rohe Eingabe oder abgeleitete Werte).\n",
    "\n",
    "### Outputs\n",
    "- Rückgabewert:\n",
    "  - vollständig aufgelöste Argumentstruktur, bereit zur Übergabe an ein Tool."
   ],
   "id": "17416bc8319e0752"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:41:47.601275Z",
     "start_time": "2026-01-31T09:41:47.596571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def agentic_resolve_tool_arguments(arguments, tool_results, user_ctx):\n",
    "    if isinstance(arguments, dict):\n",
    "        # Spezialfall: dict besteht NUR aus \"$ref\" → ersetze das ganze Dict durch das referenzierte Objekt\n",
    "        if set(arguments.keys()) == {\"$ref\"} and isinstance(arguments[\"$ref\"], str):\n",
    "            return agentic_resolve_argument(arguments, tool_results, user_ctx)\n",
    "\n",
    "        return {k: agentic_resolve_tool_arguments(v, tool_results, user_ctx) for k, v in arguments.items()}\n",
    "\n",
    "    if isinstance(arguments, list):\n",
    "        return [agentic_resolve_tool_arguments(v, tool_results, user_ctx) for v in arguments]\n",
    "\n",
    "    return agentic_resolve_argument(arguments, tool_results, user_ctx)"
   ],
   "id": "5797f9f0998657a6",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ausführung eines einzelnen Planschritts mit Tool-Aufrufen\n",
    "\n",
    "### Funktion\n",
    "- Führt alle in einem Planschritt definierten Tool-Aufrufe sequenziell aus.\n",
    "- Verbindet Plan-Spezifikation, Tool-Invocation und Ergebnisverwaltung in einer klaren Ausführungseinheit.\n",
    "- Liefert strukturierte History-Einträge zur späteren Verwendung im Prompt oder zur Nachvollziehbarkeit.\n",
    "\n",
    "### Inputs\n",
    "- Pflichtparameter:\n",
    "  - `step: Dict[str, Any]` – Planschritt mit Beschreibung und einer Liste von Tools\n",
    "    (`{\"description\": ..., \"tools\": [...]}`).\n",
    "  - `call_tool_fn: Callable[[str, Dict[str, Any]], Awaitable[Any]]` – asynchrone Funktion zum Ausführen eines Tools.\n",
    "  - `tool_results: Dict[str, Any]` – Sammelstruktur für Tool-Ergebnisse (wird in-place erweitert).\n",
    "  - `user_ctx: Dict[str, Any]` – Nutzerkontext, z. B. rohe Nutzereingabe oder abgeleitete Werte.\n",
    "\n",
    "### Ablauf\n",
    "- Initialisiert eine leere Liste für Tool-History-Einträge.\n",
    "- Iteriert über alle im Step definierten Tools in der vorgegebenen Reihenfolge.\n",
    "- Löst referenzierte Argumente (`$ref`) anhand bisheriger Tool-Ergebnisse und des Nutzerkontexts auf.\n",
    "- Ruft jedes Tool mit den aufgelösten Argumenten asynchron auf.\n",
    "- Validiert und normalisiert das Tool-Ergebnis.\n",
    "- Speichert das Ergebnis unter der Tool-ID für nachfolgende Schritte.\n",
    "- Erzeugt pro Tool einen strukturierten History-Eintrag.\n",
    "\n",
    "### Fehlerbehandlung\n",
    "- Wirft einen `TypeError`, falls das Tool-Ergebnis nicht der erwarteten Dictionary-Struktur entspricht.\n",
    "- Bricht den Ablauf mit einem `RuntimeError` ab, wenn ein Tool einen Fehler signalisiert.\n",
    "\n",
    "### Outputs\n",
    "- Rückgabewert:\n",
    "  - `List[Dict[str, Any]]` – Liste strukturierter Tool-History-Einträge für Prompt-Aufbau oder Logging."
   ],
   "id": "37b138ede89fa61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:41:50.224275Z",
     "start_time": "2026-01-31T09:41:50.217975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Callable, Awaitable\n",
    "\n",
    "async def agentic_execute_step(\n",
    "    step: Dict[str, Any],\n",
    "    call_tool_fn: Callable[[str, Dict[str, Any]], Awaitable[Any]],\n",
    "    tool_results: Dict[str, Any],\n",
    "    user_ctx: Dict[str, Any],\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Führt alle Tools in einem Step aus.\n",
    "    - step: {\"description\": \"...\", \"tools\": [ {id, name, arguments}, ... ]}\n",
    "    - call_tool_fn: deine MCP-Invoker-Funktion (z.B. call_mcp_tool)\n",
    "    - tool_results: wird in-place mit neuen Ergebnissen gefüllt\n",
    "    - user_ctx: z.B. {\"raw\": user_input}\n",
    "\n",
    "    Rückgabe: Liste von History-Objekten für den Prompt (z.B. Tool-Logs).\n",
    "    \"\"\"\n",
    "\n",
    "    history_entries: List[Dict[str, Any]] = []\n",
    "\n",
    "    for tool in step.get(\"tools\", []):\n",
    "        tool_id = tool[\"id\"]\n",
    "        tool_name = tool[\"name\"]\n",
    "        raw_args = tool.get(\"arguments\", {})\n",
    "        replace_target = tool.get(\"replace_target\")\n",
    "\n",
    "        # $ref auflösen\n",
    "        resolved_args = agentic_resolve_tool_arguments(raw_args, tool_results, user_ctx)\n",
    "\n",
    "        # Tool aufrufen\n",
    "        print(f\"[{tool_id}] {tool_name} resolved arguments: {resolved_args}\")\n",
    "        result = await call_tool_fn(tool_name, resolved_args)\n",
    "        print(f\"toolcallresult: {result}\")\n",
    "\n",
    "        # Normalize (expect dict with keys isError/data from your agentic_call_mcp_tool)\n",
    "        if not isinstance(result, dict):\n",
    "            raise TypeError(f\"Tool result must be dict, got {type(result)}: {result}\")\n",
    "\n",
    "        # History-Eintrag für Tools (kannst du an dein Format anpassen)\n",
    "\n",
    "        if result.get(\"isError\"):\n",
    "            raise RuntimeError(f\"Tool failed: {result}\")\n",
    "\n",
    "        res = result.get(\"data\", result)\n",
    "\n",
    "        # Ergebnis speichern für for Schleife bei parallelen Tools\n",
    "        tool_results[tool_id] = res\n",
    "\n",
    "        history_entries.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_id\": tool_id,\n",
    "            \"name\": tool_name,\n",
    "            \"arguments\": resolved_args,\n",
    "            \"result\": res,\n",
    "            \"replace_target\": replace_target,\n",
    "        })\n",
    "\n",
    "    return history_entries"
   ],
   "id": "e82549220e32a526",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Auswahl des nächsten auszuführenden Planschritts\n",
    "\n",
    "### Funktion\n",
    "- Bestimmt den nächsten noch nicht ausgeführten Schritt in einem agentischen Ausführungsplan.\n",
    "- Implementiert eine einfache, deterministische Auswahlstrategie ohne Reordering oder Heuristiken.\n",
    "\n",
    "### Logik\n",
    "- Iteriert über alle im Plan definierten Schritte in ihrer ursprünglichen Reihenfolge.\n",
    "- Prüft für jeden Schritt, ob dessen Index bereits als ausgeführt markiert ist.\n",
    "- Gibt den kleinsten Index zurück, der noch nicht in `executed_step_indices` enthalten ist.\n",
    "- Liefert `None`, sobald alle Schritte abgearbeitet wurden.\n",
    "\n",
    "### Inputs\n",
    "- `plan: Dict[str, Any]` – strukturierter Ausführungsplan mit einer optionalen `steps`-Liste.\n",
    "- `executed_step_indices: Set[int]` – Menge der bereits ausgeführten Schritt-Indizes.\n",
    "\n",
    "### Outputs\n",
    "- Rückgabewert:\n",
    "  - `Optional[int]` – Index des nächsten auszuführenden Schritts oder `None`, wenn der Plan vollständig abgearbeitet ist."
   ],
   "id": "1621a676c7a6bb47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:41:52.844148Z",
     "start_time": "2026-01-31T09:41:52.840157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Set\n",
    "\n",
    "def agentic_find_next_step(plan: Dict[str, Any], executed_step_indices: Set[int]) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Wählt den nächsten noch nicht ausgeführten Step.\n",
    "    V1: simpel – der kleinste Index, der noch nicht in executed_step_indices ist.\n",
    "    \"\"\"\n",
    "    for idx, _ in enumerate(plan.get(\"steps\", [])):\n",
    "        if idx not in executed_step_indices:\n",
    "            return idx\n",
    "    return None"
   ],
   "id": "af52a7509fa3041b",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompt-Konstruktion mit RAG- und Tool-Kontext für agentische Planung\n",
    "\n",
    "### Funktion\n",
    "- Baut einen vollständigen Chat-Prompt für agentische Planungs- oder Reasoning-Phasen.\n",
    "- Kombiniert Systemregeln, Tool-Historie und Gesprächsverlauf zu einer konsistenten Nachrichtenliste.\n",
    "- Ziel ist es, dem LLM **maximalen, expliziten Kontext** für Planung und Entscheidungsfindung bereitzustellen.\n",
    "\n",
    "### Inputs\n",
    "- Pflichtparameter:\n",
    "  - `tokenizer` – Tokenizer mit Chat-Template-Unterstützung.\n",
    "  - `system_prompt: str` – globale Regeln und Instruktionen für den Agenten.\n",
    "  - `user_prompt: str` – aktuelle Nutzerfrage.\n",
    "- Optionale Parameter:\n",
    "  - `history_pairs` – Liste aus `(user, assistant)`-Nachrichtenpaaren zur Kontextfortführung.\n",
    "  - `tool_history` – strukturierte Historie vorheriger Tool-Aufrufe und Ergebnisse.\n",
    "  - `retrieved_context` – externer Kontext aus Retrieval/RAG (z. B. Dokumente, Wissensschnipsel).\n",
    "\n",
    "### Nachrichtenstruktur\n",
    "- System-Nachrichten:\n",
    "  - Agentenregeln (`system_prompt`).\n",
    "  - Retrieval-Kontext, klar als externer Kontext markiert.\n",
    "  - Tool-Historie, serialisiert als JSON zur maschinellen Interpretierbarkeit.\n",
    "- Dialog-Nachrichten:\n",
    "  - Frühere Nutzer- und Assistentenbeiträge in chronologischer Reihenfolge.\n",
    "- Abschluss:\n",
    "  - Aktuelle Nutzerfrage als letzte User-Nachricht.\n",
    "\n",
    "### Outputs\n",
    "- Rückgabewert:\n",
    "  - `str` – gerenderter Chat-Prompt, bereit für die Übergabe an ein Chat-basiertes LLM.\n",
    "\n",
    "### Design-Entscheidungen\n",
    "- Explizite Trennung verschiedener Kontextquellen durch eigene System-Nachrichten.\n",
    "- Verwendung von `add_generation_prompt=True`, um dem Modell klar zu signalisieren,\n",
    "  dass die nächste Ausgabe vom Assistant zu erzeugen ist.\n",
    "- Kein Tokenisieren an dieser Stelle, um Debugging und Prompt-Inspektion zu erleichtern."
   ],
   "id": "96919f6f9a918c9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:41:54.843674Z",
     "start_time": "2026-01-31T09:41:54.837934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def agentic_build_chat_prompt_with_rag_and_tools(\n",
    "    tokenizer,\n",
    "    system_prompt: str,\n",
    "    user_prompt: str,\n",
    "    history_pairs: Optional[List[Tuple[str, str]]] = None,\n",
    "    tool_history: Optional[List[Dict[str, Any]]] = None,\n",
    "    retrieved_context: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Messages:\n",
    "      - system: agent rules\n",
    "      - system: retrieved context (optional)\n",
    "      - system: tool history (optional, as JSON)\n",
    "      - user/assistant: conversation history (optional)\n",
    "      - user: current question\n",
    "    \"\"\"\n",
    "    messages: List[Dict[str, str]] = []\n",
    "\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    if retrieved_context:\n",
    "        messages.append({\"role\": \"system\", \"content\": f\"Kontext (aus Retrieval):\\n{retrieved_context}\"})\n",
    "\n",
    "    if tool_history:\n",
    "        messages.append({\"role\": \"system\", \"content\": \"Tool-Historie:\\n\" + json.dumps(tool_history, ensure_ascii=False)})\n",
    "\n",
    "    if history_pairs:\n",
    "        for u, a in history_pairs:\n",
    "            messages.append({\"role\": \"user\", \"content\": u})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": a})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    # This is the crucial bit: add_generation_prompt=True\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )"
   ],
   "id": "b30235514526b69",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extraktion eines skalaren Ergebnisses aus Tool-Rückgaben\n",
    "\n",
    "### Funktion\n",
    "- Vereinfacht Tool-Ergebnisse auf einen skalaren Wert, sofern möglich.\n",
    "- Dient als Normalisierungsschritt zwischen Tool-Ausführung und finaler Antwortformulierung.\n",
    "\n",
    "### Logik\n",
    "- Prüft, ob das übergebene Objekt ein Dictionary mit genau einem Eintrag ist.\n",
    "- Erkennt das Muster eines typischen Tool-Ergebnisses (`{\"result\": <Wert>}`).\n",
    "- Gibt in diesem Fall direkt den enthaltenen skalaren Wert zurück.\n",
    "- Fällt andernfalls auf die unveränderte Rückgabe des ursprünglichen Objekts zurück.\n",
    "\n",
    "### Inputs\n",
    "- `x: Any` – beliebiges Objekt, typischerweise ein Tool-Ergebnis oder bereits ein skalarer Wert.\n",
    "\n",
    "### Outputs\n",
    "- Rückgabewert:\n",
    "  - `Any` – extrahierter skalarer Wert oder das Originalobjekt, falls keine Vereinfachung möglich ist."
   ],
   "id": "2e0f369e1af4aed3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:41:58.352777Z",
     "start_time": "2026-01-31T09:41:58.349168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _extract_scalar_result(x: Any) -> Any:\n",
    "    # Wenn Tool-Resultate dicts sind wie {\"result\": 38.0}\n",
    "    if isinstance(x, dict) and \"result\" in x and len(x) == 1:\n",
    "        return x[\"result\"]\n",
    "    return x"
   ],
   "id": "300253bb6ad8729e",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompt-Konstruktion für die finale Antwortformulierung (No-Tool-Phase)\n",
    "\n",
    "### Funktion\n",
    "- Erzeugt den finalen Chat-Prompt für die **Antwortformulierung ohne weitere Tool-Nutzung**.\n",
    "- Dient ausschließlich der sprachlichen Ausformulierung eines bereits korrekt berechneten Ergebnisses.\n",
    "- Erzwingt eine klare Trennung zwischen **Rechnen/Reasoning** und **sprachlicher Ausgabe**.\n",
    "\n",
    "### Inputs\n",
    "- Pflichtparameter:\n",
    "  - `tokenizer` – Tokenizer mit Chat-Template-Unterstützung.\n",
    "  - `user_question: str` – ursprüngliche Nutzerfrage.\n",
    "  - `final_value: Any` – bereits bestimmtes, finales Ergebnis (nicht erneut zu interpretieren).\n",
    "\n",
    "### Prompt-Design\n",
    "- System-Nachricht definiert eine stark eingeschränkte Rolle:\n",
    "  - keine erneuten Berechnungen,\n",
    "  - keine Tool-Aufrufe,\n",
    "  - keine strukturierte Ausgabe (z. B. JSON).\n",
    "- Instruiert explizit ein festes Antwortformat, um Konsistenz und Vergleichbarkeit sicherzustellen.\n",
    "- User-Nachricht liefert Frage und Endergebnis getrennt und explizit markiert.\n",
    "\n",
    "### Outputs\n",
    "- Rückgabewert:\n",
    "  - `str` – vollständig gerenderter Chat-Prompt, kompatibel mit Chat-basierten LLMs."
   ],
   "id": "7312f285f0b69043"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:41:59.361206Z",
     "start_time": "2026-01-31T09:41:59.356678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_final_answer_prompt(\n",
    "    tokenizer,\n",
    "    user_question: str,\n",
    "    final_value: Any,\n",
    ") -> str:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                        \"Du bist ein sprachlicher Antwort-Renderer.\\n\"\n",
    "                        \"Das Rechenergebnis ist bereits korrekt bestimmt.\\n\"\n",
    "                        \"Du darfst NICHT neu rechnen und KEINE Tools verwenden.\\n\"\n",
    "                        \"Du darfst KEIN JSON ausgeben.\\n\\n\"\n",
    "                        \"AUFGABE:\\n\"\n",
    "                        \"- Formuliere eine vollständige, natürliche Antwort auf die Nutzerfrage.\\n\"\n",
    "                        \"- Verwende exakt das gegebene Endergebnis.\\n\\n\"\n",
    "                        \"FORMAT:\\n\"\n",
    "                        \"Die Antwort soll lauten:\\n\"\n",
    "                        \"\\\"Die Antwort auf <Frage> = <Ergebnis>\\\"\"\n",
    "                    ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"User-Frage:\\n{user_question}\\n\\n\"\n",
    "                f\"Finales Ergebnis (berechnet):\\n{final_value}\\n\\n\"\n",
    "                \"Gib die Antwort jetzt aus.\"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )"
   ],
   "id": "16a2e36b9c4148a4",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Agentische Orchestrierung bis zur finalen Antwort\n",
    "\n",
    "### Funktion\n",
    "- Implementiert die zentrale Orchestrierungs- und Kontrollschleife eines agentischen Workflows.\n",
    "- Trennt den Gesamtprozess explizit in:\n",
    "  - **Planung** (LLM #1 erzeugt einen strukturierten Plan),\n",
    "  - **Ausführung** (Tools werden Schritt für Schritt gemäß Plan ausgeführt),\n",
    "  - **Auflösung** (Final-Spezifikation wird aus Tool-Ergebnissen abgeleitet),\n",
    "  - **Finalisierung** (LLM #2 formuliert die Nutzerantwort).\n",
    "\n",
    "### Inputs\n",
    "- Pflichtparameter:\n",
    "  - `user_input: str` – ursprüngliche Nutzerfrage.\n",
    "  - `system_prompt: str` – Systemkontext/Regeln für den Agenten.\n",
    "  - `tokenizer` – Tokenizer-Objekt zur Prompt-Konstruktion.\n",
    "  - `call_planner_llm_fn: Callable[[str], str]` – LLM-Funktion für Planerstellung (liefert Text/JSON).\n",
    "  - `call_final_llm_fn: Callable[[str], str]` – LLM-Funktion zur finalen Antwortformulierung.\n",
    "  - `call_tool_fn: Callable[[str, Dict[str, Any]], Awaitable[Any]]` – asynchrone Tool-Schnittstelle.\n",
    "- Optionale Parameter:\n",
    "  - `history_pairs` – (User, Assistant)-Paare als Gesprächskontext für Planning/RAG.\n",
    "  - `tool_history` – bisherige Tool-Aufrufe und Ergebnisse (für Nachvollziehbarkeit & Replanning).\n",
    "\n",
    "### Outputs\n",
    "- Rückgabewert:\n",
    "  - `str` – finaler Antworttext (typischerweise durch LLM #2 formuliert).\n",
    "  - Fallback-Pfad: bei fehlender `final`-Spezifikation im Plan wird ein definierter Fehlerstring zurückgegeben.\n",
    "\n",
    "### Interne Phasen\n",
    "- **Phase 1: Einmal planen**\n",
    "  - Baut einen Prompt aus Systemkontext, Nutzerfrage, Verlauf und Tool-Historie.\n",
    "  - Lässt LLM #1 einen Plan erzeugen und parst ihn in eine strukturierte Repräsentation.\n",
    "- **Phase 2: Plan ausführen**\n",
    "  - Bestimmt iterativ den nächsten noch nicht ausgeführten Schritt.\n",
    "  - Führt Tool-Schritte aus, sammelt Ergebnisse und erweitert die Tool-Historie.\n",
    "- **Phase 3: Final aus Plan auflösen**\n",
    "  - Löst die im Plan definierte finale Ausgabe anhand der Tool-Ergebnisse und des User-Kontexts auf.\n",
    "  - Extrahiert einen skalaren Endwert, der als Grundlage für die Antwort dient.\n",
    "- **Phase 4: Einmal final formulieren**\n",
    "  - Baut einen finalen Prompt und lässt LLM #2 die Nutzerantwort erzeugen.\n",
    "  - Schneidet Whitespace und gibt den Text zurück."
   ],
   "id": "4229c65d30419ef0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:42:02.301413Z",
     "start_time": "2026-01-31T09:42:02.294454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "async def agentic_run_to_final_answer(\n",
    "    *,\n",
    "    user_input: str,\n",
    "    system_prompt: str,\n",
    "    tokenizer,\n",
    "    call_planner_llm_fn: Callable[[str], str],  # LLM #1: JSON Plan\n",
    "    call_final_llm_fn: Callable[[str], str],    # LLM #2: Final text\n",
    "    call_tool_fn: Callable[[str, Dict[str, Any]], Awaitable[Any]],\n",
    "    history_pairs: Optional[List[Tuple[str, str]]] = None,\n",
    "    tool_history: Optional[List[Dict[str, Any]]] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Hauptschleife:\n",
    "    - plant mit dem LLM,\n",
    "    - führt Step für Step Tools aus,\n",
    "    - liefert am Ende das finale Ergebnis\n",
    "    \"\"\"\n",
    "    if tool_history is None:\n",
    "        tool_history = []\n",
    "    if history_pairs is None:\n",
    "        history_pairs = []\n",
    "\n",
    "    tool_results: Dict[str, Any] = {}\n",
    "    executed_step_indices: Set[int] = set()\n",
    "    user_ctx = {\"raw\": user_input}\n",
    "\n",
    "    # -------------------------\n",
    "    # Phase 1: EINMAL planen\n",
    "    # -------------------------\n",
    "    #<<Bitte ergänzen>>\n",
    "\n",
    "    # -------------------------\n",
    "    # Phase 2: Plan komplett ausführen\n",
    "    # -------------------------\n",
    "    #<<Bitte ergänzen>>\n",
    "\n",
    "    # -------------------------\n",
    "    # Phase 3: Final aus Plan auflösen\n",
    "    # -------------------------\n",
    "    if plan.get(\"final\") is None:\n",
    "        # Kein Replanning gewünscht -> harter Fail\n",
    "        return \"invalid_expression\"\n",
    "\n",
    "    final_spec = plan[\"final\"]\n",
    "    final_resolved = agentic_resolve_argument(final_spec, tool_results, user_ctx)\n",
    "    final_value = _extract_scalar_result(final_resolved)\n",
    "\n",
    "    # -------------------------\n",
    "    # Phase 4: EINMAL final formulieren (LLM #2)\n",
    "    # -------------------------\n",
    "    final_prompt = build_final_answer_prompt(\n",
    "        tokenizer=tokenizer,\n",
    "        user_question=user_input,\n",
    "        final_value=final_value,\n",
    "    )\n",
    "\n",
    "    final_text = call_final_llm_fn(final_prompt).strip()\n",
    "\n",
    "    # Optional: wenn das LLM doch labert, kannst du es “hart” machen:\n",
    "    # return str(final_value)\n",
    "    return final_text"
   ],
   "id": "1cccdd33fdfbb410",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MCP-Tool-Aufruf über agentische Schnittstelle\n",
    "\n",
    "### Funktion\n",
    "- Stellt eine asynchrone Brücke zwischen dem Agenten und externen MCP-Tools bereit.\n",
    "- Kapselt Initialisierung, Tool-Aufruf und Ergebnis-Normalisierung in einer einheitlichen Funktion.\n",
    "- Ziel ist eine robuste, agentenfreundliche Rückgabeform unabhängig vom konkreten Tool.\n",
    "\n",
    "### Ablauf\n",
    "- Gibt zu Debug-Zwecken Tool-Name und Argumente aus.\n",
    "- Baut eine HTTP-basierte, streamfähige Verbindung zum MCP-Server auf.\n",
    "- Initialisiert eine MCP-Session und ruft das gewünschte Tool mit Parametern auf.\n",
    "- Protokolliert Typ und Repräsentation des rohen Tool-Ergebnisses zur Analyse.\n",
    "\n",
    "### Ergebnis-Normalisierung\n",
    "- Erzeugt ein standardisiertes Rückgabe-Dictionary mit:\n",
    "  - `isError`: Kennzeichnung, ob der Tool-Aufruf fehlschlug.\n",
    "  - `data`: normalisierte Nutzdaten.\n",
    "- Bevorzugt strukturierte Inhalte (`structuredContent`), falls vorhanden.\n",
    "- Fällt andernfalls auf Textinhalte zurück und aggregiert diese zu einem String.\n",
    "- Stellt sicher, dass der Agent immer mit einer konsistenten Datenstruktur weiterarbeiten kann.\n",
    "\n",
    "### Outputs\n",
    "- Rückgabewert:\n",
    "  - `dict` mit den Schlüsseln:\n",
    "    - `isError`: bool\n",
    "    - `data`: strukturierte Daten oder zusammengeführter Textinhalt"
   ],
   "id": "c867d33f8a8285ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:42:06.709577Z",
     "start_time": "2026-01-31T09:42:06.704467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mcp.types import TextContent  # ggf. anpassen\n",
    "\n",
    "async def agentic_call_mcp_tool(tool_name: str, args: dict) -> dict:\n",
    "    print(f\"[DEBUG] agentic_call_mcp_tool: {tool_name}({args})\")\n",
    "    async with streamable_http_client(MCP_URL) as (read_stream, write_stream, _):\n",
    "        async with ClientSession(read_stream, write_stream) as session:\n",
    "            await session.initialize()\n",
    "            raw_result = await session.call_tool(tool_name, args)\n",
    "            print(f\"[DEBUG] Raw tool result type: {type(raw_result)}\")\n",
    "            print(f\"[DEBUG] Raw tool result repr: {repr(raw_result)[:400]}\")\n",
    "\n",
    "            # Normalisieren\n",
    "            result_dict: dict = {\n",
    "                \"isError\": getattr(raw_result, \"isError\", False),\n",
    "            }\n",
    "\n",
    "            # strukturierte Inhalte bevorzugen\n",
    "            if getattr(raw_result, \"structuredContent\", None) is not None:\n",
    "                result_dict[\"data\"] = raw_result.structuredContent\n",
    "            else:\n",
    "                # Fallback: Text aus content zusammenbauen\n",
    "                texts = []\n",
    "                for c in getattr(raw_result, \"content\", []) or []:\n",
    "                    if isinstance(c, TextContent):\n",
    "                        texts.append(c.text)\n",
    "                result_dict[\"data\"] = \"\\n\".join(texts)\n",
    "\n",
    "            return result_dict"
   ],
   "id": "7c56c7a0d920cf39",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Agentischer Lauf bis zur finalen Antwort\n",
    "\n",
    "### Kontext\n",
    "- Demonstriert einen vollständigen agentischen Durchlauf von einer Nutzerfrage bis zur finalen, ausformulierten Antwort.\n",
    "- Der Ablauf kombiniert Planungs-LLM, Ausführungs-LLM und Tool-Aufrufe in einer kontrollierten Pipeline.\n",
    "\n",
    "### Ablauf\n",
    "- Definiert eine einfache Nutzerfrage als String (`user_input`).\n",
    "- Initialisiert einen leeren Verlauf für agentisches Reasoning.\n",
    "- Ruft eine zentrale Orchestrierungsfunktion auf, die:\n",
    "  - einen **Planner-LLM** zur Strukturierung des Vorgehens nutzt,\n",
    "  - einen **Final-LLM** zur deterministischen Generierung der Endantwort verwendet,\n",
    "  - optional externe Tools über eine einheitliche Schnittstelle aufruft,\n",
    "  - und den gesamten Dialog- und Tool-Verlauf verwaltet.\n",
    "\n",
    "### Eingesetzte Komponenten\n",
    "- `call_planner_llm_fn`: Wrapper für ein LLM mit größerem Token-Budget zur Planung.\n",
    "- `call_final_llm_fn`: Wrapper für ein LLM mit kleinerem Token-Budget und deterministischen Einstellungen zur Antwortformulierung.\n",
    "- `call_tool_fn`: Abstraktion für Tool-Aufrufe (z. B. Rechnen, Retrieval, externe Services).\n",
    "- `history_pairs` / `tool_history`: explizite Übergabe von Kontext, um den Lauf reproduzierbar und kontrollierbar zu halten.\n",
    "\n",
    "### Output\n",
    "- `final_answer` enthält ausschließlich die finale, für den Nutzer bestimmte Antwort.\n",
    "- Die Ausgabe trennt klar zwischen interner Agentenlogik und externer Nutzerkommunikation.\n",
    "\n",
    "### Typische Verwendung\n",
    "- Evaluierung agentischer Architekturen.\n",
    "- Debugging von Planner-/Executor-Aufteilungen.\n",
    "- Reproduzierbare Experimente mit Tool-augmented LLMs in Jupyter-Notebooks."
   ],
   "id": "ae439b10df7e7d0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T09:42:25.462502Z",
     "start_time": "2026-01-31T09:42:09.504588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_input = f\"Was ist 6.0 + 4.0 * 8.0?\"\n",
    "history_agentic = []\n",
    "\n",
    "final_answer = await agentic_run_to_final_answer(\n",
    "        user_input=user_input,\n",
    "        system_prompt=agent_system_prompt,\n",
    "        tokenizer=tokenizer,\n",
    "        call_planner_llm_fn=lambda p: llama_chat(p, max_new_tokens=512),\n",
    "        call_final_llm_fn=lambda p: llama_chat(p, max_new_tokens=64, temperature=0.0),\n",
    "        call_tool_fn=agentic_call_mcp_tool,\n",
    "        history_pairs=[],\n",
    "        tool_history=None,\n",
    "    )\n",
    "\n",
    "print(\"Finale Antwort an den User:\")\n",
    "print(final_answer)\n"
   ],
   "id": "dba5123ff00c321a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "\u001B[2;36m[01/31/26 10:42:23]\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m Created new          \u001B]8;id=224399;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/streamable_http_manager.py\u001B\\\u001B[2mstreamable_http_manager.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=800100;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/streamable_http_manager.py#239\u001B\\\u001B[2m239\u001B[0m\u001B]8;;\u001B\\\n",
      "\u001B[2;36m                    \u001B[0m         transport with       \u001B[2m                              \u001B[0m\n",
      "\u001B[2;36m                    \u001B[0m         session ID:          \u001B[2m                              \u001B[0m\n",
      "\u001B[2;36m                    \u001B[0m         3c2c495253474e2f910a \u001B[2m                              \u001B[0m\n",
      "\u001B[2;36m                    \u001B[0m         4f907c34ba32         \u001B[2m                              \u001B[0m\n",
      "\u001B[2;36m                   \u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m Processing request of type            \u001B]8;id=23499;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/lowlevel/server.py\u001B\\\u001B[2mserver.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=626004;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/lowlevel/server.py#713\u001B\\\u001B[2m713\u001B[0m\u001B]8;;\u001B\\\n",
      "\u001B[2;36m                    \u001B[0m         CallToolRequest                       \u001B[2m             \u001B[0m\n",
      "\u001B[2;36m                   \u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m Processing request of type            \u001B]8;id=623994;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/lowlevel/server.py\u001B\\\u001B[2mserver.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=443354;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/lowlevel/server.py#713\u001B\\\u001B[2m713\u001B[0m\u001B]8;;\u001B\\\n",
      "\u001B[2;36m                    \u001B[0m         ListToolsRequest                      \u001B[2m             \u001B[0m\n",
      "\u001B[2;36m                   \u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m Terminating session:         \u001B]8;id=89572;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/streamable_http.py\u001B\\\u001B[2mstreamable_http.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=858004;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/streamable_http.py#779\u001B\\\u001B[2m779\u001B[0m\u001B]8;;\u001B\\\n",
      "\u001B[2;36m                    \u001B[0m         3c2c495253474e2f910a4f907c34 \u001B[2m                      \u001B[0m\n",
      "\u001B[2;36m                    \u001B[0m         ba32                         \u001B[2m                      \u001B[0m\n",
      "\u001B[2;36m                   \u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m Created new          \u001B]8;id=533042;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/streamable_http_manager.py\u001B\\\u001B[2mstreamable_http_manager.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=819250;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/streamable_http_manager.py#239\u001B\\\u001B[2m239\u001B[0m\u001B]8;;\u001B\\\n",
      "\u001B[2;36m                    \u001B[0m         transport with       \u001B[2m                              \u001B[0m\n",
      "\u001B[2;36m                    \u001B[0m         session ID:          \u001B[2m                              \u001B[0m\n",
      "\u001B[2;36m                    \u001B[0m         2c8df216906e44e7bf67 \u001B[2m                              \u001B[0m\n",
      "\u001B[2;36m                    \u001B[0m         1a3d1faaa74a         \u001B[2m                              \u001B[0m\n",
      "\u001B[2;36m                   \u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m Processing request of type            \u001B]8;id=240058;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/lowlevel/server.py\u001B\\\u001B[2mserver.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=842129;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/lowlevel/server.py#713\u001B\\\u001B[2m713\u001B[0m\u001B]8;;\u001B\\\n",
      "\u001B[2;36m                    \u001B[0m         CallToolRequest                       \u001B[2m             \u001B[0m\n",
      "\u001B[2;36m                   \u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m Processing request of type            \u001B]8;id=884125;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/lowlevel/server.py\u001B\\\u001B[2mserver.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=818909;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/lowlevel/server.py#713\u001B\\\u001B[2m713\u001B[0m\u001B]8;;\u001B\\\n",
      "\u001B[2;36m                    \u001B[0m         ListToolsRequest                      \u001B[2m             \u001B[0m\n",
      "\u001B[2;36m                   \u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m Terminating session:         \u001B]8;id=730276;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/streamable_http.py\u001B\\\u001B[2mstreamable_http.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=867885;file:///home/simon/.virtualenvs/KI-Workshop_2/lib/python3.10/site-packages/mcp/server/streamable_http.py#779\u001B\\\u001B[2m779\u001B[0m\u001B]8;;\u001B\\\n",
      "\u001B[2;36m                    \u001B[0m         2c8df216906e44e7bf671a3d1faa \u001B[2m                      \u001B[0m\n",
      "\u001B[2;36m                    \u001B[0m         a74a                         \u001B[2m                      \u001B[0m\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PHASE 1] planner output:\n",
      "{\n",
      "  \"steps\": [\n",
      "    {\n",
      "      \"description\": \"Multiplikation von 4.0 und 8.0\",\n",
      "      \"tools\": [\n",
      "        {\n",
      "          \"id\": \"mul_1\",\n",
      "          \"name\": \"multiplikation\",\n",
      "          \"arguments\": {\n",
      "            \"a\": 4.0,\n",
      "            \"b\": 8.0\n",
      "          },\n",
      "          \"replace_target\": \"4.0 * 8.0\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Addition von 6.0 und 32.0\",\n",
      "      \"tools\": [\n",
      "        {\n",
      "          \"id\": \"add_1\",\n",
      "          \"name\": \"addition\",\n",
      "          \"arguments\": {\n",
      "            \"a\": 6.0,\n",
      "            \"b\": {\"$ref\": \"mul_1.result\"}\n",
      "          },\n",
      "          \"replace_target\": \"6.0 + 4.0 * 8.0\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"final\": {\"$ref\": \"add_1.result\"}\n",
      "}\n",
      "\n",
      "[DEBUG] trimmed JSON candidate: {\n",
      "  \"steps\": [\n",
      "    {\n",
      "      \"description\": \"Multiplikation von 4.0 und 8.0\",\n",
      "      \"tools\": [\n",
      "        {\n",
      "          \"id\": \"mul_1\",\n",
      "          \"name\": \"multiplikation\",\n",
      "          \"arguments\": {\n",
      "            \"a\": 4.0,\n",
      "            \"b\": 8.0\n",
      "          },\n",
      "          \"replace_target\": \"4.0 * 8.0\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Addition von 6.0 und 32.0\",\n",
      "      \"tools\": [\n",
      "        {\n",
      "          \"id\": \"add_1\",\n",
      "          \"name\": \"addition\",\n",
      "          \"arguments\": {\n",
      "            \"a\": 6.0,\n",
      "            \"b\": {\"$ref\": \"mul_1.result\"}\n",
      "          },\n",
      "          \"replace_target\": \"6.0 + 4.0 * 8.0\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"final\": {\"$ref\": \"add_1.result\"}\n",
      "}\n",
      "[mul_1] multiplikation resolved arguments: {'a': 4.0, 'b': 8.0}\n",
      "[DEBUG] agentic_call_mcp_tool: multiplikation({'a': 4.0, 'b': 8.0})\n",
      "INFO:     127.0.0.1:48196 - \"POST /mcp HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:48206 - \"POST /mcp HTTP/1.1\" 202 Accepted\n",
      "INFO:     127.0.0.1:48210 - \"GET /mcp HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:48226 - \"POST /mcp HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:48238 - \"POST /mcp HTTP/1.1\" 200 OK\n",
      "[DEBUG] Raw tool result type: <class 'mcp.types.CallToolResult'>\n",
      "[DEBUG] Raw tool result repr: CallToolResult(meta=None, content=[TextContent(type='text', text='32.0', annotations=None, meta=None)], structuredContent={'result': 32.0}, isError=False)\n",
      "INFO:     127.0.0.1:48242 - \"DELETE /mcp HTTP/1.1\" 200 OK\n",
      "toolcallresult: {'isError': False, 'data': {'result': 32.0}}\n",
      "[DEBUG] Resolving $ref 'mul_1.result': root='mul_1', initial_type=<class 'dict'>, initial_value={'result': 32.0}\n",
      "[add_1] addition resolved arguments: {'a': 6.0, 'b': 32.0}\n",
      "[DEBUG] agentic_call_mcp_tool: addition({'a': 6.0, 'b': 32.0})\n",
      "INFO:     127.0.0.1:48248 - \"POST /mcp HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:48258 - \"POST /mcp HTTP/1.1\" 202 Accepted\n",
      "INFO:     127.0.0.1:48272 - \"GET /mcp HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:48280 - \"POST /mcp HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:48282 - \"POST /mcp HTTP/1.1\" 200 OK\n",
      "[DEBUG] Raw tool result type: <class 'mcp.types.CallToolResult'>\n",
      "[DEBUG] Raw tool result repr: CallToolResult(meta=None, content=[TextContent(type='text', text='38.0', annotations=None, meta=None)], structuredContent={'result': 38.0}, isError=False)\n",
      "INFO:     127.0.0.1:48294 - \"DELETE /mcp HTTP/1.1\" 200 OK\n",
      "toolcallresult: {'isError': False, 'data': {'result': 38.0}}\n",
      "[DEBUG] Resolving $ref 'add_1.result': root='add_1', initial_type=<class 'dict'>, initial_value={'result': 38.0}\n",
      "Finale Antwort an den User:\n",
      "Die Antwort auf 6.0 + 4.0 * 8.0 = 38.0\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 7
}
